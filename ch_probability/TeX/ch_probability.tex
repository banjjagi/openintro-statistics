\chapter[Probability (special topic)]{확률 (특별 주제)}
\label{probability}

\index{확률|(}

확률이 통계학의 근본을 구성한다. 이미 확률의 여러면에 친숙할 수도 있지만, 확률 개념을 형식한 것은 대부분 사람들에게 새로울 수 있다. 이번장에서 대부분 사람들이 이전에 봤던 과정을 사용해서 확률을 친숙한 용어로 소개하려고 한다.

\section{확률 정의하기 (특별 주제)}
\label{basicsOfProbability}


\begin{example}{``주사위(die, 복수형 dice)''는 \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6} 숫자가 6 면에 세겨진 정육면체다. 주사위를 던졌을 때 1 이 나올 확률은 얼마인가?}\label{probOf1}
만약 주사위가 공정하다면, \resp{1}이 나올 확률은 다른 어떤 숫자가 나올 확률과 같다. 6 개 결과가 나올 수 있기 때문에, 가능성은 6개 중에 1개로, 즉, $1/6$다.
\end{example}

\begin{example}{다음 주사위 던지기에서 \resp{1} 혹은 \resp{2}가 나올 확률은 얼마인가?}\label{probOf1Or2}
\resp{1} 과 \resp{2} 는 6 가지 동일한 가능한 결과 중 둘로, 두 가지 결과 중에 하나를 얻을 가능성은 $2/6 = 1/3$이 된다.
\end{example}

\begin{example}{다음번 주사위를 던져서 \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6} 중에서 하나가 나올 확률은 얼마인가?}\label{probOf123456}
100\%. 결과는 상기 6개 숫자 중에 하나가 되어야만 된다.
\end{example}

\begin{example}{ \resp{2}가 나오지 않을 가능성은 얼마인가?}\label{probNot2}
\resp{2}가 나올 가능성이 $1/6$ 혹은 $16.\bar{6}\%$이기 때문에, \resp{2}가 나오지 않을 가능성은 $100\% - 16.\bar{6}\%=83.\bar{3}\%$ 혹은 $5/6$가 된다.

다른 방식으로, \resp{2}가 나오지 않는 것은 \resp{1}, \resp{3}, \resp{4}, \resp{5}, \resp{6} 중 하나를 얻는 것과 동일하다는 것을 알아챘을 것이다. 즉, 6개 동일한 가능성을 갖는 결과 중에서 5개로 확률이 $5/6$가 된다.
\end{example}

\begin{example}{주사위를 두개 던진다고 가정하자. 첫번째 주사위가 \resp{1}이 나올 가능성 $1/6^{th}$이고, 두번째 주사위도 \resp{1}이 나올 가능성이 $1/6^{th}$이라면, 모두 \resp{1}이 나올 확률은 얼마인가?}\label{probOf2Ones}
만약 첫번째 주사위가 \resp{1}이 나올 확률이 $16.\bar{6}$\%이고, 두번째 주사위도 \resp{1}이 나올 확률이 $16.\bar{6}$\%이라면, 양쪽 주사위 모두 \resp{1}이 나올 확률은 $(1/6)\times (1/6)$ 혹은 $1/36$이 된다.
\end{example}

\subsection{확률}

\index{확률 과정|(}

명백한 임의성(randomness)을 기술하고 이해하는 도구를 구축하는데 확률을 사용한다. 
\term{결과}(outcome)를 생기게 하는 \term{확률 과정}(random process) 관점으로 확률을 종종 표현한다.

\begin{center}
\begin{tabular}{lll}
주사위 굴리기 &$\rightarrow$ & \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6} \\
동전 던지기 &$\rightarrow$ & \resp{H} 혹은 \resp{T} \\
\end{tabular}
\end{center}

주사위 굴리기 혹은 동전 던지기는 확률 과정이며 각각은 결과를 생기게 한다.

\begin{termBox}{\tBoxTitle{확률}
결과의 \term{확률}은 만약 무한번 확률과정을 관측했다면 결과가 일어난 횟수의 비율이다.}
\end{termBox}

확률은 비율로 정의되고, 항상 0~과~1 사이 값을 갖는다 (전부 통틀어).또한 0\% 에서 100\% 사이 퍼센티지로 표현될 수도 있다.

확률은 주사위를 많이 굴려서 실증할 수도 있다. $\hat{p}_n$을 $n$번 주사위를 굴려서 \resp{1}이 나온 결과 비율로 놓자. 주사위 굴리기 횟수가 증가함에 따라, $\hat{p}_n$은 \resp{1}을 굴려 나온 확률에 수렴한다, 즉, $p = 1/6$. 그림~\ref{dieProp}은 주사위를 100,000번 굴린 수렴결과가 도시되어 있다. $\hat{p}_n$이 $p$ 주위 안정화되는 경향성을 \term{대수의 법칙}(Law of Large Numbers)으로 기술된다.


\begin{figure}[bt]
\centering
\includegraphics[width=0.85\textwidth]{ch_probability/figures/dieProp/dieProp}
\caption{모의실험에서 각 단계에 1 인 주사위 던지기 분율. 던지기 횟수가 증가함에 따라 비율은 확률 $1/6 \approx 0.167$에 가까워지는 경향이 있다.}
\label{dieProp}
\end{figure}

\begin{termBox}{\tBoxTitle{대수의 법칙}
더 많은 관측점이 수집됨에 따라, 특정 결과를 갖는 출현 비율, $\hat{p}_n$은 해당 결과 $p$에 수렴한다.}
\end{termBox}

경우에 따라서, 그림~\ref{dieProp}에서 $\hat{p}_n$이 여러번 하듯이, 비율이 확률로부터 방향을 바꾸어 벗어나 대수의 법칙을 부정하는 것처럼 보인다. 하지만, 이러한 편차는 던기지 횟수가 증가함에 따라 점점 작아진다.

위에서 굴려서 \resp{1}이 나온 확률을 $p$로 표기했다. 또한 해당 확률을 다음과 같이 작성할 수도 있다.

\begin{eqnarray*}
P(\text{\resp{1} 굴리기})
\end{eqnarray*}
\marginpar[\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$]{\raggedright\vspace{-13mm}

$P(A)$\vspace{1mm}\\\footnotesize 결과 $A$ \\ 확률} 상기 표기에 편안해지면, 더 축약한다. 예를 들어, 만약 과정이 ``주사위 굴리기''가 명확하면, $P($\resp{1} 굴리기$)$ 을~$P($\resp{1}$)$으로 축약한다.

\begin{exercise} \label{randomProcessExercise}
확률과정은 주사위 굴리기와 동전던지기가 포함된다. (a) 또다른 확률과정을 생각해보라. 
(b) 생각해본 과정의 가능한 모든 결과를 기술하라. 예를 들어, 주사위 굴리기는 확률과정으로 \mbox{\resp{1}, \resp{2}, ..., \resp{6}} 사이 가능한 결과가 나온다.\footnote{네가지 예제가 있다. (i) 누군가 다음달에 아플지 아프지 않을지는 명백하게 \resp{아프다} 와 \resp{아프지 않다}라는 결과가 있는 확률과정이다. (ii) 사람을 임의로 골라서 신장을 측정해서 확률과정을 \emph{생성}할 수 있다. 이 과정 결과는 양수다. (iii) 주식시장이 다음주에 오르고 내리는 것은 \resp{상승}, \resp{하락}, \resp{변동 없음} 가능한 결과를 갖는 확률 과정이다. 다른 방법으로 숫자 결과로 주식시장 변동율을 사용할 수 수도 있다. (iv) 방짝이 오늘밤 설겆이를 했는지는 \resp{설겆이\_\hspace{0.3mm}했음} 과 \resp{설겆이\_\hspace{0.3mm}안했음} 가능한 결과를 갖는 확률과정처럼 보인다.}
\end{exercise}

확률과정으로 생각하는 것이 꼭 확률적일 필요는 없다. 하지만, 너무나 어려워서 정확하게 이해될 수는 없다. Guided Practice~\ref{randomProcessExercise}의 네번째 예제 주석 해답으로 방짝 행동은 확률과정이라고 제시했다. 하지만, 방짝 행동이 완전히 확률적이지는 않을지라도, 방짝의 행동을 확률과정으로 모형화하는 것은 여전히 유용하다.

\begin{tipBox}{\tipBoxTitle{과정을 확률로 모형화}
설사 과정이 엄밀히 확률적이는 않지만, 확률로 모형화하는 것이 도움이 될 수 있다.}
\end{tipBox}

\index{확률 과정|)}

\subsection{서로 겹치지 않거나 상호 배반적인 결과}

\index{서로 겹치지 않는|(}
\index{상호 배반적|(}

만약 두 결과가 함께 발생하지 않는다면, 두 결과는 \term{서로 겹치지 않는}(disjoint) 혹은 \term{상호 배반적}(mutually exclusive)이라고 부른다. 예를 들어, 주사위를 던지면, 결과 \resp{1} 와 \resp{2}는 서로 겹치지 않는다라고 하는데 둘다 함께 일어날 수 없기 때문이다. 다른 한편으로, 결과 \resp{1} 과 ``홀수 굴리기는'' 서로 겹친다 왜냐 하면 주사위 굴린 결과 \resp{1} 이 나오면 두 사건이 동시에 일어난 것이기 때문이다. 용어 \term{서로 겹치지 않는}(disjoint), \term{상호 배반적}(mutually exclusive)은 동치이고 서로 교환가능하다. 

서로 겹치지 않는 결과의 확률을 계산하는 것은 쉽다. 주사위를 굴릴 때, \resp{1} 와 \resp{2} 결과는 서로 겹치지 않아서 이들 중 하나가 일어날 확률은 개별 확률을 더해서 계산한다:

\begin{eqnarray*}
P(\text{\resp{1} 혹은 \resp{2}}) = P(\text{\resp{1}})+P(\text{\resp{2}}) = 1/6 + 1/6 = 1/3
\end{eqnarray*}

주사위를 굴려서 \resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6} 이 나올 확률은 얼마인가? 여기서 다시, 모든 결과는 서로 겹치지 않아서 각각의 확률을 더한다:

\begin{eqnarray*}
&&P(\text{\resp{1} 혹은 \resp{2} 혹은 \resp{3} 혹은 \resp{4} 혹은 \resp{5} 혹은 \resp{6}}) \\
	&&\quad= P(\text{\resp{1}})+P(\text{\resp{2}})+P(\text{\resp{3}})+P(\text{\resp{4}})+P(\text{\resp{5}})+P(\text{\resp{6}}) \\
	&&\quad= 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
\end{eqnarray*}

결과가 서로 겹치지 않을 때, \term{가산 규칙}(Addition Rule)은 이러한 접근법의 정확성을 보증한다.

\begin{termBox}{\tBoxTitle{서로 겹치지 않는 결과의 가산 규칙}
만약 $A_1$ 과 $A_2$가 두 서로 겹치지 않는 결과라면, 이들 결과 중 하나가 발생할 확률은 다음과 같이 주어진다.
\begin{eqnarray*}
P(A_1\text{ 혹은 } A_2) = P(A_1) + P(A_2)
\end{eqnarray*}
만약 서로 겹치지 않는 많은 결과 $A_1$, ..., $A_k$ 이 있다면, 이들 결과 중 하나가 발생할 확률은 다음과 같다.
\begin{eqnarray}
P(A_1) + P(A_2) + \cdots + P(A_k)
\end{eqnarray}
}
\end{termBox}

\begin{exercise}
주사위를 굴려서 \resp{1}, \resp{4}, \resp{5} 이 나올 확률에 관심이 있다. (a) \resp{1}, \resp{4}, \resp{5} 결과가 왜 서로 겹치지 않는지 설명하시요. (b) 가산 규칙을 적용해서 서로 겹치지 않는 결과 $P($\resp{1} or \resp{4} or \resp{5}$)$ 확률을 결정하시요.\footnote{(a) 확률과정은 주사위 굴리기다. 많아야 주사위 결과 중 하나가 나올 수 있다. 이것이 의미하는 것은 주사위 굴린 결과는 서로 겹치지 않는 결과가 된다. (b)~$P($\resp{1} 혹은 \resp{4} 혹은 \resp{5}$) = P($\resp{1}$)+P($\resp{4}$)+P($\resp{5}$) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{3}{6} = \frac{1}{2}$}
\end{exercise}

\index{데이터!email|(}
\begin{exercise}
~\ref{introductionToData} 장 \data{email} 데이터셋에서, \var{number} 변수는 숫자가 없는지 (표식자는 \resp{none}), 단지 작은 하나 혹은 그이상 숫자가 있는지(\resp{small}), 혹은 전자우편에 적어도 하나 큰 숫자(\resp{big})가 나타나는지를 기술한다. 3,921 개 전자우편 중에서 549는 숫자가 없고, 2,827개 전자우편은 단지 작은 하나 혹은 그이상 숫자가 있고, 545는 적어도 하나 큰 숫자가 전자우편에 있다. (a) \resp{none}, \resp{small}, \resp{big} 결과는 서로 겹치지 않는가? (b) 별도로 \resp{small} 와 \resp{big} 값을 갖는 전자우편 비율을 알아내시오. (c) 서로 겹치지 않는 결과에 대해서 가산규칙을 사용해서 데이터셋에서 무작위로 고른 전자우편 내부에 큰던 작던 숫자가를 갖을 확률을 계산하시오.\footnote{
(a) 맞습니다. \var{number} 수준 중 하나로만 각 전자우편이 분류된다. (b) \resp{small}: $\frac{2827}{3921} = 0.721$. \resp{big}: $\frac{545}{3921} = 0.139$. (c) $P($\resp{small} 혹은 \resp{big}$) = P($\resp{small}$) + P($\resp{big}$) = 0.721 + 0.139 = 0.860$.}
\end{exercise}
\index{데이터!email|)}

\index{사건|(}
통계학자는 거의 개별 결과로 작업하지 않고 대신에 결과 \indexthis{\emph{집합}}{집합}(set) or \indexthis{\emph{모임}}{collections}(collections)을 고려한다. $A$가 주사위를 굴렸을 때 \resp{1} 혹은 \resp{2}가 나올 사건을 나타내고, $B$가 주사위 굴렸을 때 \resp{4} 혹은 \resp{6} 이 나올 사건을 나타낸다고 하자. $A$ 를 $\{$\resp{1},~\resp{2}$\}$ 의 결과 집합으로 적고, $B=\{$\resp{4}, \resp{6}$\}$ 와 같이 적는다. 이러한 집합을 일반적으로 \termsub{사건}{사건}(events)으로 불린다. $A$ 와 $B$ 는 공통된 요소가 없기 때문에, 서로 겹치지 않는 사건이다. $A$ 와 $B$ 가 그림~\ref{disjointSets}에 나타나 있다.

\begin{figure}[hhh]
\centering
\includegraphics[width=0.55\textwidth]{ch_probability/figures/disjointSets/disjointSets}
\caption{$A$, $B$, $D$, 세가지 사건은 주사위를 굴려 나온 결과로 구성된다. $A$ 와 $B$는 서로 겹치지 않는데 이유는 어떤 공통된 결과도 없기 때문이다.}
\label{disjointSets}
\end{figure}

가산규칙은 서로 겹치지 않는 결과와 서로 겹치지 않는 사건에 모두 적용된다. 서로 겹치지 않는 사건 $A$ 혹은 $B$ 가 일어날 확률은 개별 확률의 합이다:

\begin{align*}
P(A\text{ 혹은 }B) = P(A) + P(B) = 1/3 + 1/3 = 2/3
\end{align*}

\begin{exercise}
(a) 가산 규칙을 사용해서 사건 $A$의 확률, $P(A)$가 $1/3$임을 입증하시오. (b) 사건 $B$에 대해 동일한 작업을 수행하시오.\footnote{(a) $P(A) = P($\resp{1} 혹은 \resp{2}$) = P($\resp{1}$) + P($\resp{2}$) = \frac{1}{6} + \frac{1}{6} = \frac{2}{6} = \frac{1}{3}$. (b) 마찬가지 방식으로, $P(B) = 1/3$.}
\end{exercise}

\begin{exercise} \label{exerExaminingDisjointSetsABD}
(a) 그림~\ref{disjointSets}을 참조하여, 사건 $D$는 어떤 결과를 나타내는가? (b) 사건 $B$ 와 $D$는 서로 겹치지 않는가? (c) 사건 $A$ 와 $D$는 서로 겹치지 않는가? \footnote{(a)~결과 \resp{2} 와 \resp{3}. (b)~예, 사건 $B$ 와 $D$ 는 서로 겹치지 않는데 이유는 두 사건 모두 어떤 결과도 공유하지 않는다. (c)~사건 $A$ 와 $D$ 는 공통된 결과, \resp{2}를 공유해서, 서로 겹친다.}
\end{exercise}

\begin{exercise}
Guided Practice~\ref{exerExaminingDisjointSetsABD}에서 그림~\ref{disjointSets}으로부터 $B$ 와 $D$가 서로 겹치지 않는다는 것을 확인했다. 사건 $B$ 혹은 $D$ 이 일어날 확률을 계산하시오.\footnote{$B$ 와 $D$ 는 서로 겹치지 않는 사건이기 때문에, 가산 규칙을 사용한다: $P(B$ 혹은 $D) = P(B) + P(D) = \frac{1}{3} + \frac{1}{3} = \frac{2}{3}$.}
\end{exercise}

\index{사건|)}
\index{서로 겹치지 않는|)}
\index{상호 배반적|)}

\subsection{사건이 서로 겹칠 때 확률}

표~\ref{deckOfCards}에 나타낸 \indexthis{52개 정규 카드 한벌}{deck of cards} 맥락에서 서로 겹치는 두 사건을 계산하는 문제를 생각해 보자. 만약 정규 한벌 카드에 친숙하지 않다면, 아래 주석\footnote{카드 52개는 4 \term{묶음}(suite)으로 쪼개진다: $\clubsuit$ (클럽, club), {\color{redcards}$\diamondsuit$} (다이아몬드, diamond), {\color{redcards}$\heartsuit$} (하트, heart), $\spadesuit$ (스페이드, spade). 묶음 각각은 다음 라벨이 붙은 13개 카드로 구성된다: \resp{2}, \resp{3}, ..., \resp{10}, \resp{J} (잭, jack), \resp{Q} (퀸, queen), \resp{K} (킹, king), \resp{A} (에이스, ace). 따라서, 카드 각각은 묶음과 라벨의 유일무이한 조합이 된다, 즉, {\color{redcards}\resp{4$\heartsuit$}}, \resp{J$\clubsuit$}. 잭, 퀸, 킹을 나타내는 카드 12개는 \termsub{\resp{그림카드}}{그림카드}(face card)로 불린다. 일반적으로 {\color{redcards}$\diamondsuit$} 혹은 {\color{redcards}$\heartsuit$} 카드묶음은 {\color{redcards}빨간색}으로 다른 두 묶음 카드는 검은색이다.}을 참조한다.


\begin{table}[h]
\centering
\begin{tabular}{lll lll lll lll l}
\resp{2$\clubsuit$} & \resp{3$\clubsuit$} & \resp{4$\clubsuit$} & \resp{5$\clubsuit$} & \resp{6$\clubsuit$} & \resp{7$\clubsuit$} & \resp{8$\clubsuit$} & \resp{9$\clubsuit$} & \resp{10$\clubsuit$} & \resp{J$\clubsuit$} & \resp{Q$\clubsuit$} & \resp{K$\clubsuit$} & \resp{A$\clubsuit$}  \\
\color{redcards} \resp{2$\diamondsuit$} & \color{redcards}\resp{3$\diamondsuit$} & \color{redcards}\resp{4$\diamondsuit$} & \color{redcards}\resp{5$\diamondsuit$} & \color{redcards}\resp{6$\diamondsuit$} & \color{redcards}\resp{7$\diamondsuit$} & \color{redcards}\resp{8$\diamondsuit$} & \color{redcards}\resp{9$\diamondsuit$} & \color{redcards}\resp{10$\diamondsuit$} & \color{redcards}\resp{J$\diamondsuit$} & \color{redcards}\resp{Q$\diamondsuit$} & \color{redcards}\resp{K$\diamondsuit$} & \color{redcards}\resp{A$\diamondsuit$} \\
\color{redcards}\resp{2$\heartsuit$} & \color{redcards}\resp{3$\heartsuit$} & \color{redcards}\resp{4$\heartsuit$} & \color{redcards}\resp{5$\heartsuit$} & \color{redcards}\resp{6$\heartsuit$} & \color{redcards}\resp{7$\heartsuit$} & \color{redcards}\resp{8$\heartsuit$} & \color{redcards}\resp{9$\heartsuit$} & \color{redcards}\resp{10$\heartsuit$} & \color{redcards}\resp{J$\heartsuit$} & \color{redcards}\resp{Q$\heartsuit$} & \color{redcards}\resp{K$\heartsuit$} & \color{redcards}\resp{A$\heartsuit$} \\
\resp{2$\spadesuit$} & \resp{3$\spadesuit$} & \resp{4$\spadesuit$} & \resp{5$\spadesuit$} & \resp{6$\spadesuit$} & \resp{7$\spadesuit$} & \resp{8$\spadesuit$} & \resp{9$\spadesuit$} & \resp{10$\spadesuit$} & \resp{J$\spadesuit$} & \resp{Q$\spadesuit$} & \resp{K$\spadesuit$} & \resp{A$\spadesuit$}
\end{tabular}
\caption{카드 한벌에 52개 유일무이한 카드를 표현.}
\label{deckOfCards}
\end{table}

\begin{exercise}
(a) 무작위로 선택된 카드가 다이아몬드일 확률은 얼마인가? (b)무작위로 선택된 카드가 그림카드일 확률은 얼마인가?\footnote{(a) 카드가 52개, 다이아몬드가 13개 있다. 만약 카드를 철저히 뒤섞는다면, 카드 각각이 뽑힐 가능성은 동일하다. 그래서 무작위로 선택된 카드가 다이아몬드일 확률은 $P({\color{redcards}\diamondsuit}) = \frac{13}{52} = 0.250$. (b)~마찬가지 방식으로, 그림카드 12개가 있다. 그래서 $P($그림카드$) = \frac{12}{52} = \frac{3}{13} = 0.231$.}
\end{exercise}

둘 혹은 셋 변수, 속성, 확률과정에 대한 결과를 ``포함(in)'' 혹은 ``제외(out)''로 범주화할 때 \term{벤다이어그램}(Venn diagrams)이 유용하다. 그림~\ref{cardsDiamondFaceVenn}에 벤다이어그램은 원을 사용해서 다이아몬드를 나타내고, 또다른 원을 사용해서 그림카드를 나타낸다. 만약 카드가 다이아몬드이고 그림카드라면, 해당 카드는 두 원의 교차지점에 포함된다. 만약 카드가 다이아몬드지만 그림카드가 아니라면, 오른쪽 원이 아닌 왼쪽 원부분이 된다. 다이아몬드인 카드 전체 숫자는 다이아몬드 원의 카드 전체 숫자로 주어진다: $10+3=13$. 또한 확률도 도시되어 있다 (즉, $10/52 = 0.1923$).

\begin{figure}
\centering
\includegraphics[width=0.65\textwidth]{ch_probability/figures/cardsDiamondFaceVenn/cardsDiamondFaceVenn}
\caption{다이아몬드와 그림카드를 위한 벤다이어그램.}
\label{cardsDiamondFaceVenn}
\end{figure}

%\begin{exercise}
%Using Figure~\ref{cardsDiamondFaceVenn}, verify $P($face card$) = 12/52=3/13$.\footnote{The Venn diagram shows face cards split up into ``face card but not {\color{redcards}$\diamondsuit$}'' and ``face card and {\color{redcards}$\diamondsuit$}''. Since these correspond to disjoint events, $P($face card$)$ is found by adding the two corresponding probabilities: $\frac{3}{52} + \frac{9}{52} = \frac{12}{52} = \frac{3}{13}$.}
%\end{exercise}

$A$가 무작위로 선택된 카드가 다이아몬드를 나타내고, $B$는 그림카드 사건을 나타낸다. $P(A$ 혹은 $B)$ 확률을 어떻게 계산하나요? 사건 $A$ 와 $B$ 는 서로 겹친다 -- 카드 {\color{redcards}$J\diamondsuit$}, {\color{redcards}$Q\diamondsuit$}, {\color{redcards}$K\diamondsuit$} 는 양쪽 범주에 포함된다 -- 그래서 서로 겹치지 않는 사건에 대한 가산 법칙을 사용할 수는 없다. 대신에 벤다이어그램을 사용한다. 두 사건에 대한 확률을 더해서 시작한다:

\begin{eqnarray*}
P(A) + P(B) = P({\color{redcards}\diamondsuit}) + P(\text{그림카드}) = 12/52 + 13/52
\label{overCountFaceDiamond}
\end{eqnarray*}
하지만, 양쪽 사건에 해당되는 세 카드가, 확률 각각에 대해서 한번씩, 중복 계수되었다. 이중으로 계수된 것을 수정해야만 된다:
\begin{eqnarray}
P(A\text{ or } B) &=&P(\text{그림카드 혹은 }{\color{redcards}\diamondsuit})  \notag \\
 &=& P(\text{그림카드}) + P({\color{redcards}\diamondsuit}) - P(\text{그림카드 그리고 }{\color{redcards}\diamondsuit}) \label{diamondFace} \\
 &=& 13/52 + 12/52 - 3/52 \notag \\
 &=& 22/52 = 11/26 \notag
\end{eqnarray}

방정식~(\ref{diamondFace})은 \term{일반 가산법칙}(General Addition Rule)의 한 사례다.

\begin{termBox}{\tBoxTitle{일반 가산규칙} 만약 $A$ 와 $B$ 를 서로 겹치든, 겹치지 않든 임의 두 사건이라고 가정하면, 두 사건 중 하나가 일어날 확률은 다음과 같다.
\begin{eqnarray}
P(A\text{ 혹은 }B) = P(A) + P(B) - P(A\text{ 그리고 }B)
\label{generalAdditionRule}
\end{eqnarray}
여기서 $P(A$ 그리고 $B)$ 는 두 사건이 모두 일어날 확률이다.}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{``혹은(or)'' 의 의미는 일체를 포함함.}
통계학에서 ``혹은(or)''을 적을 때, 명시적으로 달리 언급되지 않는다면 ``그리고/혹은 (and/or)'' 의미가 된다. 따라서, $A$ 혹은(or) $B$ 가 일어난다는 의미는 $A$, $B$, 혹은 $A$ 와 $B$ 함께 일어난다는 의미가 된다.}
\end{tipBox}

\begin{exercise}
(a) 만약 $A$ 그리고 $B$ 가 서로 겹치지 않는다면, 왜 이것이 $P(A$ 그리고 $B) = 0$를 암시하는지 기술하시오. (b) (a) 부분을 사용해서, 만약 $A$ 그리고 $B$ 가 서로 겹치지 않는다면 일반가산법칙이 서로 겹치지 않는 사건에 대해서 더 간단한 가산규칙으로 단순화됨을 확증하시오.\footnote{(a) 만약 $A$ 그리고 $B$ 가 서로 겹치지 않는다면, 사건 $A$ 그리고 $B$ 는 결코 동시에 일어날 수 없다. (b) 만약 $A$ 그리고 $B$ 가 서로 겹치지 않는다면, 방정식~(\ref{generalAdditionRule}) 의 마지막 항은 0 이 되고 ((a) 부분 참조), 서로 겹치지 않는 사건에 대한 가산 규칙으로 된다.}
\end{exercise}

\index{데이터!email|(}

\begin{exercise}\label{emailSpamNumberVennExer}
% library(openintro); data(email); table(email[,c("spam", "number")]); table(email[,c("number")]); table(email[,c("spam")])
전자우편 3,921개를 갖는 \data{email} 데이터셋에는 367개 스팸 전자우편이, 전자우편 2,827개는 일부 작은 숫자 하지만 크지 않은 숫자를 갖는 있으며, 전자우편 168개는 두 특성을 모두 지니고 있다. 이런 설정에 대한 벤다이어그램을 생성하시오.\footnote{%
\begin{minipage}[t]{0.65\textwidth}
갯수와 상응하는 {\color{oiB}확률} 이 도시되었다 (즉, $2659/3921 = 0.678$). 왼쪽 원에 나타난 전자우편 숫자는 $2659 + 168 = 2827$에 대응되고, 오른쪽 원에 나타난 숫자는 $168 + 199 = 367$ 이 된다.
\end{minipage}\ %
\begin{minipage}[c]{0.3\textwidth}
\hfill\includegraphics[height=13mm]{ch_probability/figures/emailSpamNumberVenn/emailSpamNumberVenn} \vspace{-13mm}
\end{minipage}
}
\end{exercise}

\begin{exercise}
(a) Guided Practice~\ref{emailSpamNumberVennExer}에 나온 벤다이어그램을 사용해서 \data{email} 데이터셋으로부터 무작위로 뽑힌 전자우편이 스팸이며 적은 숫자(하지만 큰 숫자는 아닌)를 포함할 확률을 알아내시오. (b) 전자우편이 이러한 속성 중 하나를 가질 확률은 얼마인가요?\footnote{
(a)~정답은 두 원의 교차지역으로 나타난다: 0.043. 해답은 원으로 보여진 서로 겹치지 않는 세 확률의 합이다: $0.678 + 0.043 + 0.051 = 0.772$.}
\index{데이터!email|)}
\end{exercise}


\subsection{확률 분포}

\term{확률분포}(probability distribution)는 서로 겹치지 않는 모든 결과와 연관된 확률의 표다. 표~\ref{diceProb}에는 주사위 두개를 굴려나온 결과의 합계에 대한 확률분포가 나와있다.

\begin{table}[h] \small
\centering
\begin{tabular}{l ccc ccc ccc cc}
  \hline
  \ \vspace{-3mm} \\
Dice sum\vspace{0.3mm} & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12  \\
Probability & $\frac{1}{36}$ & $\frac{2}{36}$ & $\frac{3}{36}$ & $\frac{4}{36}$ & $\frac{5}{36}$ & $\frac{6}{36}$ & $\frac{5}{36}$ & $\frac{4}{36}$ & $\frac{3}{36}$ & $\frac{2}{36}$ & $\frac{1}{36}$\vspace{1mm} \\
   \hline
\end{tabular}
\caption{주사위 두개를 굴려나온 결과의 합계에 대한 확률분포.}
\label{diceProb}
\end{table}

\begin{termBox}{\tBoxTitle{확률분포에 대한 규칙}
확률분포는 다음 세가지 규칙을 만족하는 가능한 결과와 대응되는 확률에 대한 목록이다: \vspace{-2mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item 목록에 나온 결과는 서로 겹치지 않아야 된다.
\item 확률 각각은 0 과 1 사이에 놓여야만 한다.
\item 확률은 합이 1 이다. \vspace{1mm}
\end{enumerate}}
\end{termBox}

\begin{exercise}\label{usHouseholdIncomeDistsExercise}
표~\ref{usHouseholdIncomeDists}는 미국 세가지 가구소득에 대한 분포를 제시한다. 단지 하나만 옳다. 어느 분포가 맞을까? 다른 두개 잘못된 분포는 무엇인가?\footnote{분포 (a)는 합이 1이 아니다. 분포 (b)에 두번째 확률은 음수다. 따라서 (c)가 분포 요건을 만족한다. 세가지 분포 중 하나가 실제 미국 가구소득분포이며, 그것은 (c)가 된다.}
\end{exercise}

\begin{table}
\centering
\begin{tabular}{r | rr rr}
  \hline
소득 구간 (\$1000 달러) & 0-25    & 25-50    & 50-100     & 100+    \\
  \hline
(a)\hspace{0.2mm}	 & 0.18 & 0.39 & 0.33 & 0.16 \\
(b)				 & 0.38 & -0.27 & 0.52 & 0.37 \\
(c)\hspace{0.2mm}	 & 0.28 & 0.27 & 0.29 & 0.16 \\
  \hline
\end{tabular}
\caption{미국 가구소득으로 제시된 분포들 (Guided Practice~\ref{usHouseholdIncomeDistsExercise}).}
\label{usHouseholdIncomeDists}
\end{table}

~\ref{introductionToData} 장은  신속한 요약을 위해서 데이터 도식화의 중요성을 강조했다. 확률분포도 막대그래프로 요약될 수 있다. 예를 들어, 미국 가구소득 분포를 막대그래프를 사용해서 그림~\ref{usHouseholdIncomeDistBar}으로 도식화했다.  주사위 두개를 굴려 나온 합계에 대한 확률분포도 표~\ref{diceProb}로 나와있고, 그림~\ref{diceSumDist}으로 도식화했다. %\footnote{It is also possible to construct a distribution plot when income is not artificially binned into four groups. \emph{Continuous} distributions are considered in Section~\ref{contDist}.}

\begin{figure}
\centering
\includegraphics[width=0.68\textwidth]{ch_probability/figures/usHouseholdIncomeDistBar/usHouseholdIncomeDistBar}
\caption{미국 가구소득의 확률분포.}
\label{usHouseholdIncomeDistBar}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.73\textwidth]{ch_probability/figures/diceSumDist/diceSumDist}
\caption{주사위 두개를 던져 나온 합계의 확률분포.}
\label{diceSumDist}
\end{figure}

이러한 막대그래프에서, 막대높이는 결과의 확률을 나타낸다. 만약 결과가 숫자이고 이산형이라면, 주사위를 던져 나온 합계의 경우처럼, 대체로 (시각적으로) 히스토그램을 닮은 막대그래프로 표현하는 것이 편리하다. 각각의 위치에서 막대를 그리는 또다른 예제가 ~\pageref{bookCostDist}쪽 그림~\ref{bookCostDist}에 나와있다.

\subsection{여사건}

주사위를 굴리면 집합 $\{$\resp{1}, \resp{2}, \resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$ 중 값이 하나 나온다. 주사위를 굴릴 때 모든 가능한 결과 집합을 \term{표본공간} ($S$)\marginpar[\raggedright\vspace{-5mm}

$S$\\\footnotesize 표본공간]{\raggedright\vspace{-5mm}

$S$\\\footnotesize 표본공간}\index{S@$S$}이라고 부른다. 
사건이 일어나지 않는 시나리오를 조사하는데 종종 표본 공간을 사용한다.

$D=\{$\resp{2}, \resp{3}$\}$를 주사위를 굴린 결과가 \resp{2} 혹은 \resp{3}인 사건을 나타낸다고 두자. 
그러면 $D$의 \term{여}\marginpar[\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize $A$ 결과의 \\여]{\raggedright\vspace{0.2mm}

$A^c$\\\footnotesize $A$ 결과의 \\여}\index{Ac@$A^c$}는 $D$에 포함되지 않는 표본공간의 모든 결과를 나타내고, $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$로 표시한다. 즉, $D^c$는 $D$에 이미 포함되지 않는 가능한 모든 결과 집합이다. 그림~\ref{complementOfD} 에 $D$, $D^c$, 표본공간 $S$ 사이 관계가 나와있다.

\begin{figure}[hht]
\centering
\includegraphics[width=0.55\textwidth]{ch_probability/figures/complementOfD/complementOfD}
\caption{사건 $D=\{$\resp{2}, \resp{3}$\}$와 여사건 $D^c = \{$\resp{1}, \resp{4}, \resp{5}, \resp{6}$\}$. $S$~는 표본공간을 나타내는데 모든 가능한 사건 집합이다.}
\label{complementOfD}
\end{figure}

\begin{exercise}
(a) 
$P(D^c) = P($ \resp{1}, \resp{4}, \resp{5}, or \resp{6} 굴리기$)$ 을 계산하시요. (b) $P(D) + P(D^c)$ 은 얼마인가?\footnote{(a)~결과는 서로 겹치지 않고, 각각은 확률 $1/6$ 이 되고, 전체 확률은 $4/6=2/3$ 이 된다. (b)~$P(D)=\frac{1}{6} + \frac{1}{6} = 1/3$ 이 된다. $D$ 와 $D^c$ 가 서로 겹치지 않기 때문에, $P(D) + P(D^c) = 1$.}
\end{exercise}

\begin{exercise}
사건 $A=\{$\resp{1}, \resp{2}$\}$ 와 $B=\{$\resp{4}, \resp{6}$\}$ 가 ~\pageref{disjointSets} 쪽 그림~\ref{disjointSets}에 나와있다. (a) $A^c$ 와 $B^c$ 가 무엇을 나타내는지 적으시오. (b) $P(A^c)$ 와 $P(B^c)$ 확률을 계산하시오. (c) $P(A)+P(A^c)$ 와 $P(B)+P(B^c)$ 확률을 계산하시오.\footnote{간략한 해답: (a)~$A^c=\{$\resp{3}, \resp{4}, \resp{5}, \resp{6}$\}$ 와 $B^c=\{$\resp{1}, \resp{2}, \resp{3}, \resp{5}$\}$. (b)~
결과 각각은 서로 겹치지 않음에 주목한다. 개별 결과 확률을 더하면 $P(A^c)=2/3$ 와 $P(B^c)=2/3$ 을 얻게된다. (c)~$A$~와~$A^c$ 은 서로 겹치지 않는다. 그리고, 동일한 것이 $B$~와~$B^c$ 에도 사실이다. 따라서, $P(A) + P(A^c) = 1$ 와 $P(B) + P(B^c) = 1$ 이 된다.}
\end{exercise}

집합의 여사건은 매우 중요한 두가지 속성을 갖도록 만들어진다: 
(i) $A$ 에 존재하지 않는 모든 가능한 결과는 $A^c$ 내부에 있다. (ii) $A$ 와 $A^c$ 는 서로 겹치지 않는다. 속성 (i) 는 다음을 암시한다.

\begin{eqnarray}
P(A\text{ or }A^c) = 1
\label{complementSumTo1}
\end{eqnarray}

즉, 만약 결과가 $A$ 내부에 있지 않는다면, $A^c$ 내부에 나타내야만 된다. 특성 (ii) 을 적용하려면 서로 겹치지 않는 사건에 대한 가산 균칙을 사용한다:

\begin{eqnarray}
P(A\text{ or }A^c) = P(A) + P(A^c)
\label{complementDisjointEquation}
\end{eqnarray}

방정식~(\ref{complementSumTo1}) 와~(\ref{complementDisjointEquation}) 을 결합하면 사건 확률과 여사건 확률 사이에 매우 유용한 관계를 이끌어낸다.

\begin{termBox}{\tBoxTitle{여(Complement)}
$A$ 의 여사건은 $A^c$ 로 표기되고, $A^c$는 $A$ 에 없는 모든 결과를 나타낸다. $A$ 와 $A^c$ 는 수학적으로 다음과 같이 관련된다: \vspace{-2mm}
\begin{eqnarray}\label{complement}
P(A) + P(A^c) = 1, \quad\text{i.e.}\quad P(A) = 1-P(A^c)
\end{eqnarray}\vspace{-6.5mm}}
\end{termBox}

간단한 예로, $A$ 혹은 $A^c$ 을 계산하는 것은 몇 단계로 실행 가능하다. 하지만, 여(complement)를 사용하면 문제 복잡성이 늘어나면 상당한 시간을 절약할 수 있다.

\begin{exercise}

$A$ 가 주사위를 두번 굴려서 합이 \resp{12} 보다 적은 사건을 나타낸다고 하자. (a) 사건 $A^c$ 은 무엇을 나타낼가요? (b) ~\pageref{diceProb} 쪽 표~\ref{diceProb} 에서 $P(A^c)$ 을 알아내세요. (c) $P(A)$ 를 알아내시오.\footnote{(a)~$A$의 여: 합이 \resp{12} 와 같을 때. (b)~$P(A^c) = 1/36$. (c) $P(A^c) = 1/36$, (b) 에서 얻은 여확률과  방정식~(\ref{complement})을 사용: $P($ \resp{12} 보다 적음$) = 1 - P($\resp{12}$) = 1 - 1/36 = 35/36$.}
\end{exercise}

\begin{exercise} 주사위 두개 굴리기와 표~\ref{diceProb}에서 나온 확률을 다시 고려해보자. 다음 확률을 계산하시오: (a) 주사위 합이 6 이 \emph{아닐} 확률. (b) 합이 적어도 \resp{4} 일 확률. 즉, 
$B=\{$\resp{4}, \resp{5}, ..., \resp{12}$\}$ 일 사건이 일어날 확률을 알아낸다. (c) 합이 \resp{10} 보다 크지 않다. 즉, $D=\{$\resp{2}, \resp{3}, ..., \resp{10}$\}$ 일 사건이 일어날 확률을 알아낸다.\footnote{(a)~먼저 $P($\resp{6}$)=5/36$ 을 알아내고 나서, 여(complement)를 사용: $P($not \resp{6}$) = 1 - P($\resp{6}$) = 31/36$.

(b)~먼저 여(complement)를 알아내는데, 훨씬 적은 노력이 든다: $P($\resp{2} 혹은 \resp{3}$)=1/36+2/36=1/12$. 그리고 나서 $P(B) = 1-P(B^c) = 1-1/12 = 11/12$ 을 계산한다.

(c)~이전처럼, 여(complement)를 찾아내는 것이 $P(D)$ 을 알아내는 현명한 방법이다. 먼저 $P(D^c) = P($\resp{11} 혹은 \resp{12}$)=2/36 + 1/36=1/12$ 을 알아내고 나서, $P(D) = 1 - P(D^c) = 11/12$ 을 계산한다.}
\end{exercise}


\subsection{독립(Independence)}
\label{probabilityIndependence}

변수와 관측점이 독립이듯이, 확률과정도 또한 독립일 수 있다. 만약 한 확률과정 결과 정보를 아는 것이 다른 확률과정 결과에 대한 어떠한 정보도 제공하지 않는다면, 두 확률과정은 \term{독립}(independent)이다. 예를 들어, 동전 던지기와 주사위 던지기는 독립된 두 확률과정이다 -- 동전 앞면을 안다는 것이 주사위 굴리기 결과를 확인하는데 도움이 되지 못한다. 다른 한편으로, 주식 가격은 함께 올라가고 내려가서, 독립적이지 않다.

예제~\ref{probOf2Ones}에 독립된 두 확률과정 기본 예제가 나와있다: 주사위 두개 굴리기. 두 주사위 모두 \resp{1}이 나올 확률을 알고자 한다. 주사위 하나는 빨간색이고, 다른 하나는 하얀색으로 가정하자. 만약 빨간 주사위 결과가 \resp{1}이 나오면, 하얀색 주사위 결과에 대해서는 어떤 정보도 제공하지 못한다. 먼저,  예제~\ref{probOf2Ones} (페이지~\pageref{probOf2Ones})에 나온 동일한 문제와 마주한다. 다음 추론엔진을 사용해서 확률을 계산했다: 빨간 주사위가 \resp{1}나올 경우의 수는 $1/6^{th}$이고, 하얀색 주사위가 \resp{1}나올 경우의 수는 $1/6^{th}$이다. 그림~\ref{indepForRollingTwo1s}에 주사위 두개를 굴리는 사례를 도식화했다. 주사위는 서로 독립이기 때문에, 최종 결과값을 도출하는데 상응하는 결과 확률을 곱한다: $(1/6)\times(1/6)=1/36$. 이 사례를 많은 독립 확률과정으로 일반화할 수 있다.  

\begin{figure}[hht]
\centering
\includegraphics[width=0.6\textwidth]{ch_probability/figures/indepForRollingTwo1s/indepForRollingTwo1s}
\caption{첫번째 주사위 굴린 결과가 \resp{1}인 것은 $1/6^{th}$. 두번째 주사위 굴리기도 \resp{1}인 것도 $1/6^{th}$이다.}
\label{indepForRollingTwo1s}
\end{figure}

\begin{example}{기존 주사위와 다른 파란 주사위가 있다면 어떨까? 주사위 세개를 굴려서 모두 \resp{1}이 나올 확률은 얼마인가? }\label{threeDice}
예제~\ref{probOf2Ones}와 동일한 로직을 적용한다. 하얀색과 빨간색 주사위가 모두 \resp{1}일 경우 확률이 $1/36^{th}$이면, 파란색 주사위도 \resp{1}이 나올 확률도 $1/6^{th}$이다. 그래서 곱한다:

{\begin{align*}
P(하얀색=\text{\small\resp{1} and } 빨간색=\text{\small\resp{1} and } 파란색=\text{\small\resp{1}})
	&= P(하얀색=\text{\small\resp{1}})\times P(빨간색=\text{\small\resp{1}})\times P(파란색=\text{\small\resp{1}}) \\
	&= (1/6)\times (1/6)\times (1/6)
	= 1/216
\end{align*}} \vspace{-7mm}
\end{example}

예제~\ref{threeDice}는 소위 독립 확률과정에 대한 곱셈정리(Multiplication Rule)를 보여주고 있다.

\begin{termBox}{\tBoxTitle{독립 확률과정에 대한 \term{곱셈정리}(Multiplication Rule)}
만약 $A$ 와 $B$가 서로 다른 두 독립 확률과정이라면, $A$ 와 $B$가 모두 발생할 확률은 각각의 확률 곱으로 계산된다: \vspace{-1.5mm}
\begin{eqnarray}\label{eqForIndependentEvents}
P(A \text{ and }B) = P(A) \times  P(B)
\end{eqnarray}

유사하게, 만약 $k$ 개 독립 확률과정에서 나온 $k$개 사건 $A_1$, ..., $A_k$가 있다면, 모든 사건이 일어날 확률은 다음과 같다. \vspace{-1.5mm}
\begin{eqnarray*}
P(A_1) \times  P(A_2)\times  \cdots \times  P(A_k)
\end{eqnarray*}\vspace{-6mm}}
\end{termBox}

\begin{exercise} \label{ex2Handedness}
사람의 약 9\%가 왼손잡이다. 미국 인구집단에서 무작위로 사람을 두명 뽑았다고 가정하자. 표본크기 2는 전체 모집단과 연관해서 매우 작기 때문에, 뽑힌 두 사람이 서로 독립이라고 가정해도 합리적이다. (a)~두 사람 모두 왼손잡이일 확률은 얼마인가? (b)~두 사람 모두 오른손잡이일 확률은 얼마인가?\footnote{(a) 첫번째 사람이 왼손잡이일 확률은 $0.09$이고, 두번째 사람도 동일하다. 독립 확률과정에 대한 곱셈정리를 적용해서 두사람이 모두 왼손잡이일 확률을 계산한다: $0.09\times 0.09 = 0.0081$

(b) 양손잡이인 인구비율이 거의 0으로 가정하는 것도 합리적으로, 오른손잡이 확률은 $P($right-handed$)=1-0.09=0.91$이 된다. (a)와 동일한 추론엔진을 사용해서 두사람이 모두 오른손잡이일 확률은 $0.91\times 0.91 = 0.8281$이 된다.}
\end{exercise}

\textC{\newpage}

\begin{exercise} \label{ex5Handedness}
무작위로 5명을 뽑았다고 가정하자.\footnote{
(a)~오른손잡이(right-handed)와 왼손잡이(left-handed)를 각각 \resp{RH}와 \resp{LH} 줄임말로 표현한다. 각각은 서로 독립이기 때문에, 독립 확률과정에 대한 곱셈정리를 적용한다: 
\begin{align*}
P(\text{5명 모두 \resp{RH}})
&= P(\text{첫번째 = \resp{RH}, 두번째 = \resp{RH}, ..., 다섯번째 = \resp{RH}}) \\
&= P(\text{첫번째 = \resp{RH}})\times P(\text{두번째 = \resp{RH}})\times  \dots \times P(\text{다섯번째 = \resp{RH}}) \\
&= 0.91\times 0.91\times 0.91\times 0.91\times 0.91 = 0.624
\end{align*}

(b)~(a)와 동일한 추론엔진을 사용해서, $0.09\times 0.09\times 0.09\times 0.09\times 0.09 = 0.0000059 이 된다.$

(c)~이 질문에 답하기 위해서 보수, $P($all five are \resp{RH}$)$, 를 사용한다:
\begin{align*}
P(\text{모두 \resp{RH}가 아님})
	= 1 - P(\text{모두 \resp{RH}})
	= 1 - 0.624 = 0.376
\end{align*}} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] 모두 오른손잡이일 확률은 얼마인가?
\item[(b)] 모두 왼손잡이일 확률은 얼마인가?
\item[(c)] 모든 사람이 오른손잡이가 아닐 확률은 얼마인가?
\end{enumerate}
\end{exercise}

변수 \var{handedness}(잘쓰는 손), \var{gender}(성별)이 독립이라고 가정하자. 즉, 누군가의 \var{gender}(성별)을 안다는 것이 \var{handedness}(잘쓰는 손)에 대한 어떤 정보도 제공하지 못하고 반대의 경우도 그렇다. 그러면, 곱셈정리를 사용해서, 무작위로 고른 사람이 오른손잡이고, 동시에 여성일 확률을 계산할 수 있다.\footnote{미국 인구에서 실제 \resp{female}(여성) 비율이 약 50\%다. 그래서 여성 표본확률로 0.5를 사용한다. 하지만, 이 확률은 국가마다 다르다.}:
\begin{eqnarray*}
P(\text{오른손잡이 그리고 여성}) &=& P(\text{오른손잡이) \times  P(\text{여성}) \\
&=& 0.91 \times  0.50 = 0.455
\end{eqnarray*}


\begin{exercise}
무작위로 세명을 뽑았다.\footnote{간략한 정답은 다음과 같다. (a)~This can be written in probability notation as $P($무작위로 뽑힌 사람이 남성이고 오른손잡이$)같은 확률표기법으로 적는다=0.455$. (b) 0.207. (c) 0.045. (d) 0.0093.} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] 첫번째 사람이 남성이고 오른손잡이일 확률은 얼마인가?
\item[(b)] 첫 두사람이 남성이고 오른손잡이일 확률은 얼마인가?
\item[(c)] 세번째 사람이 여성이고 왼손잡이일 확률은 얼마인가?
\item[(d)] 첫 두사람이 남성이고 오른손잡이고, 세번째 사람이 여성이고 왼손잡이일 확률은 얼마인가?
\end{enumerate}
\end{exercise}

Sometimes we wonder if one outcome provides useful information about another outcome. The question we are asking is, are the occurrences of the two events independent? We say that two events $A$ and $B$ are independent if they satisfy Equation~\eqref{eqForIndependentEvents}.

\begin{example}{If we shuffle up a deck of cards and draw one, is the event that the card is a heart independent of the event that the card is an ace?}
The probability the card is a heart is $1/4$ and the probability that it is an ace is $1/13$. The probability the card is the ace of hearts is $1/52$. We check whether Equation~\ref{eqForIndependentEvents} is satisfied:
\begin{align*}
P({\color{redcards}\heartsuit})\times P(\text{ace}) = \frac{1}{4}\times \frac{1}{13} = \frac{1}{52} 
					= P({\color{redcards}\heartsuit}\text{ and ace})
\end{align*}
Because the equation holds, the event that the card is a heart and the event that the card is an ace are independent events.
\end{example}

%_________________
\section{Conditional probability (special topic)}
\label{conditionalProbabilitySection}

\index{data!family\_college|(}

The \data{family\_\hspace{0.3mm}college} data set contains a sample of 792 cases with two variables, \var{teen} and \var{parents}, and is summarized in Table~\ref{contTableOfParStCollege}.\footnote{A simulated data set based on real population summaries at \oiRedirect{textbook-student_parent_college_2001}{nces.ed.gov/pubs2001/2001126.pdf}.} The \var{teen} variable is either \resp{college} or \resp{not}, where the \var{college} label means the teen went to college immediately after high school. The~\var{parents} variable takes the value \resp{degree} if at least one parent of the teenager competed a college degree.

\begin{table}[ht]
\centering
\begin{tabular}{ll rr r rr}
  && \multicolumn{2}{c}{\var{parents}} & \hspace{1cm} &  \\
  \cline{3-4}
	&& \resp{degree} & \resp{not} & Total  \\
  \cline{2-5}
	& \resp{college}     & 231 & 214 & 445 \\
\raisebox{1.5ex}[0pt]{\var{teen}}	& \resp{not} \hspace{0.5cm} & 49 & 298 & 347   \\
  \cline{2-5}
	& Total & 280 & 512 & 792 \\
\end{tabular}
\caption{Contingency table summarizing the \data{family\_\hspace{0.3mm}college} data set.}
\label{contTableOfParStCollege}
\end{table}
%set.seed(5); n <- 792
%parents <- sample(c("college", "not"), n, replace = TRUE, prob = c(0.35, 0.65))
%p <- ifelse(parents == "college", 0.82, 0.45)
%teen <- ifelse(runif(n) < p, "college", "not")
%table(teen, parents)
%table(teen, parents) / n

\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{ch_probability/figures/familyCollegeVenn/familyCollegeVenn}
\caption{A Venn diagram using boxes for the \data{family\_\hspace{0.3mm}college} data set.}
\label{familyCollegeVenn}
\end{figure}

\begin{example}{If at least one parent of a teenager completed a college degree, what is the chance the teenager attended college right after high school?}
We can estimate this probability using the data. Of the 280 cases in this data set where \var{parents} takes value \resp{degree}, 231 represent cases where the \var{teen} variable takes value \resp{college}:
\begin{eqnarray*}
P(\text{\var{teen} \resp{college} given \var{parents} \resp{degree}}) = \frac{231}{280} = 0.825
\end{eqnarray*}
\end{example}

\begin{example}{A teenager is randomly selected from the sample and she did not attend college right after high school. What is the probability that at least one of her parents has a college degree?}\label{collegeProbOfParentsGivenStudentNot}
If the teenager did not attend, then she is one of the 347 teens in the second row. Of~these 347 teens, 49 had at least one parent who got a college degree:
\begin{eqnarray*}
P(\text{\var{parents} \resp{degree} given \var{teen} \resp{not}}) = \frac{49}{347} = 0.141
\end{eqnarray*}
\end{example}

\subsection{Marginal and joint probabilities}
\label{marginalAndJointProbabilities}

\index{marginal probability|(}
\index{joint probability|(}

Table~\ref{contTableOfParStCollege} includes row and column totals for each variable separately in the \data{family\_\hspace{0.3mm}college} data set. These totals represent \termsub{marginal probabilities}{marginal probability} for the sample, which are the probabilities based on a single variable without regard to any other variables. For instance, a probability based solely on the \var{teen} variable is a marginal probability:
\begin{align*}
P(\text{\var{teen} \resp{college}}) = \frac{445}{792} = 0.56
\end{align*}
A probability of outcomes for two or more variables or processes is called a \termsub{joint \mbox{probability}}{joint probability}:
\begin{align*}
P(\text{\var{teen} \resp{college} and \var{parents} \resp{not}}) = \frac{214}{792} = 0.27
\end{align*}
It is common to substitute a comma for ``and'' in a joint probability, although either is acceptable. That is,
\begin{center}
$P(\text{\var{teen} \resp{college}, \var{parents} \resp{not}})$ \\[2mm]
means the same thing as \\[2mm]
$P(\text{\var{teen} \resp{college} and \var{parents} \resp{not}})$
\end{center}

\begin{termBox}{\tBoxTitle{Marginal and joint probabilities}
If a probability is based on a single variable, it is a \emph{\hiddenterm{marginal probability}}. The probability of outcomes for two or more variables or processes is called a \emph{\hiddenterm{joint probability}}.}
\end{termBox}

We use \term{table proportions} to summarize joint probabilities for the \data{family\_\hspace{0.3mm}college} sample. These proportions are computed by dividing each count in Table~\ref{contTableOfParStCollege} by the table's total, 792, to obtain the proportions in Table~\ref{familyCollegeProbTable}. The joint probability distribution of the \var{parents} and \var{teen} variables is shown in Table~\ref{familyCollegeDistribution}.

\begin{table}[h]
\centering
\begin{tabular}{l rr r}
  \hline
& \var{parents}: \resp{degree} & \var{parents}: \resp{not} & Total  \\
  \hline
\var{teen}: \resp{college}     & 0.29 & 0.27 & 0.56 \\
\var{teen}: \resp{not} \hspace{0.5cm} & 0.06 & 0.38 & 0.44  \\
   \hline
Total & 0.35 & 0.65 & 1.00 \\
\hline
\end{tabular}
\caption{Probability table summarizing whether at least one parent had a college degree and the teenager attended college.}
\label{familyCollegeProbTable}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l c}
  \hline
Joint outcome & Probability \\
  \hline
\var{parents} \resp{degree} and \var{teen} \resp{college} & 0.29 \\
\var{parents} \resp{degree} and \var{teen} \resp{not} & 0.06 \\
\var{parents} \resp{not} and \var{teen} \resp{college} & 0.27 \\
\var{parents} \resp{not} and \var{teen} \resp{not} & 0.38 \\
   \hline
Total & 1.00 \\
\hline
\end{tabular}
\caption{Joint probability distribution for the \data{family\_\hspace{0.3mm}college} data set.}
\label{familyCollegeDistribution}
\end{table}

\begin{exercise}
Verify Table~\ref{familyCollegeDistribution} represents a probability distribution: events are disjoint, all probabilities are non-negative, and the probabilities sum to~1.\footnote{Each of the four outcome combination are disjoint, all probabilities are indeed non-negative, and the sum of the probabilities is $0.29 + 0.06 + 0.27 + 0.38 = 1.00$.}
\end{exercise}

We can compute marginal probabilities using joint probabilities in simple cases. For example, the probability a random teenager from the study went to college is found by summing the outcomes where \var{teen} takes value \resp{college}:\index{marginal probability|)}\index{joint probability|)}
\begin{align*}
P(\text{\underline{\color{black}\var{teen} \resp{college}}})
&=  P(\text{\var{parents} \resp{degree} and \underline{\color{black}\var{teen} \resp{college}}}) \\
& \quad \quad + P(\text{\var{parents} \resp{not} and \underline{\color{black}\var{teen} \resp{college}}}) \\
&= 0.28 + 0.27 \\
&= 0.56
\end{align*}


\subsection{Defining conditional probability}

\index{conditional probability|(}

There is some connection between education level of parents and of the teenager: a college degree by a parent is associated with college attendance of the teenager. In this section, we discuss how to use information about associations between two variables to improve probability estimation.

The probability that a random teenager from the study attended college is 0.56. Could we update this probability if we knew that one of the teen's parents has a college degree? Absolutely. To do so, we limit our view to only those 280 cases where a parent has a college degree and look at the fraction where the teenager attended college:
\begin{eqnarray*}
P(\text{\var{teen} \resp{college} given \var{parents} \resp{degree}}) = \frac{231}{280} = 0.825
\end{eqnarray*}
We call this a \term{conditional probability} because we computed the probability under a condition: a parent has a college degree. There are two parts to a conditional probability, the \term{outcome of interest} and the \term{condition}. It is useful to think of the condition as information we know to be true, and this information usually can be described as a known outcome or~event.

We separate the text inside our probability notation into the outcome of interest and the condition:
\begin{eqnarray}
&& P(\text{\var{teen} \resp{college} given \var{parents} \resp{degree}}) \notag \\
&& = P(\text{\var{teen} \resp{college}}\ |\ \text{\var{parents} \resp{degree}}) = \frac{231}{280} = 0.825
\label{probStudentUsedIfParentsUsedInFormalNotation}
\end{eqnarray}
\marginpar[\raggedright\vspace{-10mm}

$P(A | B)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$\\given $B$]{\raggedright\vspace{-10mm}

$P(A | B)$\vspace{1mm}\\\footnotesize Probability of\\outcome $A$\\given $B$}The vertical bar ``$|$'' is read as \emph{given}.

In Equation~\eqref{probStudentUsedIfParentsUsedInFormalNotation}, we computed the probability a teen attended college based on the condition that at least one parent has a college degree as a fraction:\vspaceB{-1mm}
\begin{eqnarray}
&& P(\text{\var{teen} \resp{college}}\ |\ \text{\var{parents} \resp{degree}}) \notag \\
&&\quad = \frac{\text{\# cases where \var{teen} \resp{college} and \var{parents} \resp{degree}}}{\text{\# cases where \var{parents} \resp{degree}}} \label{ratioOfBothToRatioOfConditionalForParentsAndStudent} \\
&&\quad = \frac{231}{280} = 0.825 \notag
\end{eqnarray}
We considered only those cases that met the condition, \var{parents} \resp{degree}, and then we computed the ratio of those cases that satisfied our outcome of interest, the teenager attended college.

Frequently, marginal and joint probabilities are provided instead of count data. For example, disease rates are commonly listed in percentages rather than in a count format. We would like to be able to compute conditional probabilities even when no counts are available, and we use Equation~(\ref{ratioOfBothToRatioOfConditionalForParentsAndStudent}) as an example demonstrating this technique.

We considered only those cases that satisfied the condition, \var{parents} \resp{degree}. Of these cases, the conditional probability was the fraction who represented the outcome of interest, \var{teen} \resp{college}. Suppose we were provided only the information in Table~\ref{familyCollegeProbTable}, i.e. only probability data. Then if we took a sample of 1000 people, we would anticipate about 35\% or $0.35\times 1000 = 350$ would meet the information criterion (\var{parents} \resp{degree}). Similarly, we would expect about 29\% or $0.29\times 1000 = 290$ to meet both the information criteria and represent our outcome of interest. Then the conditional probability can be computed as
\begin{align}
&P(\text{\var{teen} \resp{college}}\ |\ \text{\var{parents} \resp{degree}}) \notag \\
	&= \frac{\text{\# (\var{teen} \resp{college} and \var{parents} \resp{degree})}}{\text{\# (\var{parents} \resp{degree})}} \notag \\
	&= \frac{290}{350}
		= \frac{0.29}{0.35}
		= 0.829\quad\text{(different from 0.825 due to rounding error)}
\label{stUserPUsedHypSampSize}
\end{align}
In Equation~(\ref{stUserPUsedHypSampSize}), we examine exactly the fraction of two probabilities, 0.29 and 0.35, which we can write as
\begin{align*}
P(\text{\var{teen} \resp{college} and \var{parents} \resp{degree}})
	\quad\text{and}\quad
	P(\text{\var{parents} \resp{degree}}).
\end{align*}
The fraction of these probabilities is an example of the general formula for conditional probability.

\begin{termBox}{\tBoxTitle{Conditional probability}
The conditional probability of the outcome of interest $A$ given condition $B$ is computed as the following:
\begin{align}
P(A | B) = \frac{P(A\text{ and }B)}{P(B)}
\label{condProbEq}
\end{align}}
\end{termBox}

\begin{exercise}\label{familyCollegeProbOfParentsEqualNotGivenTeen}
(a) Write out the following statement in conditional probability notation: ``\emph{The probability a random case where neither parent has a college degree if it is known that the teenager didn't attend college right after high school}''. Notice that the condition is now based on the {teenager}, not the {parent}. \\[1mm]
(b)~Determine the probability from part (a). Table~\vref{familyCollegeProbTable} may be helpful.\footnote{(a) $P(\text{\var{parents} \resp{not}}\ |\ \text{\var{teen} \resp{not}})$. (b)~Equation~(\ref{condProbEq}) for conditional probability indicates we should first find $P(\text{\var{parents} \resp{not} and \var{teen} \resp{not}}) = 0.38$ and $P(\text{\var{teen} \resp{not}}) = 0.44$. Then the ratio represents the conditional probability: $0.38 / 0.44 = 0.864$.}
\end{exercise}

\textC{\pagebreak}

\begin{exercise}\label{whyCondProbSumTo1}
(a)~Determine the probability that one of the parents has a college degree if it is known the teenager did not attend college. \\[1mm]
(b)~Using the answers from part~(a) and Guided Practice~\ref{familyCollegeProbOfParentsEqualNotGivenTeen}(b), compute\\[1mm]
\textC{{\color{white}.\hspace{5mm}}}$P(\text{\var{parents} \resp{degree}}\ |\ \text{\var{teen} \resp{not}})
\ + \ P(\text{\var{parents} \resp{not}}\ |\ \text{\var{teen} \resp{not}})$\\[1mm]
(c)~Provide an intuitive argument to explain why the sum in (b) is~1.\footnote{(a)~This probability is $\frac{P(\text{\var{parents} \resp{degree}, \var{teen} \resp{not}})}{P(\text{\var{teen} \resp{not}})} = \frac{0.06}{0.44} = 0.136$. (b)~The total equals~1. (c)~Under the condition the teenager didn't attend college, the parents must either have a college degree or not. The complement still works for conditional probabilities, provided the probabilities are conditioned on the same information.}
\end{exercise}

\begin{exercise}
The data indicate there is an association between parents having a college degree and their teenager attending college. Does this mean the parents' college degree(s) \emph{caused} the teenager to go to college?\footnote{No. While there is an association, the data are observational. Two potential confounding variables include \var{income} and \var{region}. Can you think of others?}
\index{conditional probability|)}
\index{data!family\_college|)}
\end{exercise}


\subsection{Smallpox in Boston, 1721}

\index{data!smallpox|(}

The \data{smallpox} data set provides a sample of 6,224 individuals from the year 1721 who were exposed to smallpox in Boston.\footnote{Fenner F. 1988. \emph{Smallpox and Its Eradication (History of International Public Health, No. 6)}. Geneva: World Health Organization. ISBN 92-4-156110-6.} Doctors at the time believed that inoculation, which involves exposing a person to the disease in a controlled form, could reduce the likelihood of death.

Each case represents one person with two variables: \var{inoculated} and \var{result}. The variable \var{inoculated} takes two levels: \resp{yes} or \resp{no}, indicating whether the person was inoculated or not. The variable \var{result} has outcomes \resp{lived} or \resp{died}. These data are summarized in Tables~\ref{smallpoxContingencyTable} and~\ref{smallpoxProbabilityTable}.

\begin{table}[h]
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
\cline{2-5}
		& \resp{lived}     & 238 & 5136 & 5374 \\
\raisebox{1.5ex}[0pt]{\var{result}} &  \resp{died} \hspace{0.5cm} & 6 & 844 & 850  \\
\cline{2-5}
	& Total & 244 & 5980 & 6224 \\
\end{tabular}
\caption{Contingency table for the \data{smallpox} data set.}
\label{smallpoxContingencyTable}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{ll rr r}
& & \multicolumn{2}{c}{inoculated} & \\
\cline{3-4}
& & \resp{yes} & \resp{no} & Total  \\
   \cline{2-5}
 & \resp{lived}     & 0.0382 & 0.8252 & 0.8634 \\
\raisebox{1.5ex}[0pt]{\var{result}} & \resp{died} \hspace{0.5cm} & 0.0010 & 0.1356  & 0.1366  \\
   \cline{2-5}
& Total & 0.0392 & 0.9608 & 1.0000 \\
\end{tabular}
\caption{Table proportions for the \data{smallpox} data, computed by dividing each count by the table total, 6224.\textC{\vspace{-2mm}}}
\label{smallpoxProbabilityTable}
\end{table}

%\textC{\newpage}

\begin{exercise} \label{probDiedIfNotInoculated}
Write out, in formal notation, the probability a randomly selected person who was not inoculated died from smallpox, and find this \mbox{probability.}\footnote{$P($\var{result} = \resp{died} $|$ \var{inoculated} = \resp{no}$) = \frac{P(\text{\var{result} = \resp{died} and \var{inoculated} = \resp{no}})}{P(\text{\var{inoculated} = \resp{no}})} = \frac{0.1356}{0.9608} = 0.1411$.}
\end{exercise}

\begin{exercise}
Determine the probability that an inoculated person died from smallpox. How does this result compare with the result of Guided Practice~\ref{probDiedIfNotInoculated}?\footnote{$P($\var{result} = \resp{died} $|$ \var{inoculated} = \resp{yes}$) = \frac{P(\text{\var{result} = \resp{died} and \var{inoculated} = \resp{yes}})}{P(\text{\var{inoculated} = \resp{yes}})} = \frac{0.0010}{0.0392} = 0.0255$. The death rate for individuals who were inoculated is only about 1~in~40 while the death rate is about 1~in~7 for those who were not inoculated.}
\end{exercise}

\begin{exercise}\label{SmallpoxInoculationObsExpExercise}
The people of Boston self-selected whether or not to be inoculated. (a) Is this study observational or was this an experiment? (b) Can we infer any causal connection using these data? (c) What are some potential confounding variables that might influence whether someone \resp{lived} or \resp{died} and also affect whether that person was inoculated?\footnote{Brief answers: (a)~Observational. (b)~No, we cannot infer causation from this observational study. (c)~Accessibility to the latest and best medical care. There are other valid answers for part~(c).}
\end{exercise}

\subsection{General multiplication rule}

Section~\ref{probabilityIndependence} introduced the Multiplication Rule for independent processes. Here we provide the \term{General Multiplication Rule} for events that might not be independent.

\begin{termBox}{\tBoxTitle{General Multiplication Rule}
If $A$ and $B$ represent two outcomes or events, then \vspace{-1.5mm}
\begin{eqnarray*}
P(A\text{ and }B) = P(A | B)\times P(B)
\end{eqnarray*} \vspace{-6.5mm} \par
It is useful to think of $A$ as the outcome of interest and $B$ as the condition.}
\end{termBox}
This General Multiplication Rule is simply a rearrangement of the definition for conditional probability in Equation~(\ref{condProbEq}) on page~\pageref{condProbEq}.

\begin{example}{Consider the \data{smallpox} data set. Suppose we are given only two pieces of information: 96.08\% of residents were not inoculated, and 85.88\% of the residents who were not inoculated ended up surviving. How could we compute the probability that a resident was not inoculated and lived?}
We will compute our answer using the General Multiplication Rule and then verify it using Table~\ref{smallpoxProbabilityTable}. We want to determine
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} and \var{inoculated} = \resp{no}})
\end{eqnarray*}
and we are given that
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} }|\text{ \var{inoculated} = \resp{no}})=0.8588 \\
P(\text{\var{inoculated} = \resp{no}})=0.9608
\end{eqnarray*}
Among the 96.08\% of people who were not inoculated, 85.88\% survived:
\begin{eqnarray*}
P(\text{\var{result} = \resp{lived} and \var{inoculated} = \resp{no}}) = 0.8588\times 0.9608 = 0.8251
\end{eqnarray*}
This is equivalent to the General Multiplication Rule. We can confirm this probability in Table~\ref{smallpoxProbabilityTable} at the intersection of \resp{no} and \resp{lived} (with a small rounding error).
\end{example}

\begin{exercise}
Use $P($\var{inoculated} = \resp{yes}$) = 0.0392$ and $P($\var{result} = \resp{lived} $|$ \var{inoculated} = \resp{yes}$) = 0.9754$ to determine the probability that a person was both inoculated and lived.\footnote{The answer is 0.0382, which can be verified using Table~\ref{smallpoxProbabilityTable}.}
\end{exercise}

\begin{exercise}
If 97.45\% of the people who were inoculated lived, what proportion of inoculated people must have died?\footnote{There were only two possible outcomes: \resp{lived} or \resp{died}. This means that 100\% - 97.45\% = 2.55\% of the people who were inoculated died.}
\end{exercise}

\begin{termBox}{\tBoxTitle{Sum of conditional probabilities}
Let $A_1$, ..., $A_k$ represent all the disjoint outcomes for a variable or process. Then if $B$ is an event, possibly for another variable or process, we have: \vspace{-1mm}
\begin{eqnarray*}
P(A_1|B)+\cdots+P(A_k|B) = 1
\end{eqnarray*}\vspace{-5.5mm} \par
The rule for complements also holds when an event and its complement are conditioned on the same information: \vspace{-1.5mm}
\begin{eqnarray*}
P(A | B) = 1 - P(A^c | B)
\end{eqnarray*}}
\end{termBox}

\begin{exercise}
Based on the probabilities computed above, does it appear that inoculation is effective at reducing the risk of death from smallpox?\footnote{The samples are large relative to the difference in death rates for the ``inoculated'' and ``not inoculated'' groups, so it seems there is an association between \var{inoculated} and \var{outcome}. However, as noted in the solution to Guided Practice~\ref{SmallpoxInoculationObsExpExercise}, this is an observational study and we cannot be sure if there is a causal connection. (Further research has shown that inoculation is effective at reducing death rates.)}
\end{exercise}

\subsection{Independence considerations in conditional probability}

If two events are independent, then knowing the outcome of one should provide no information about the other. We can show this is mathematically true using conditional probabilities.

\begin{exercise} \label{condProbOfRollingA1AfterOne1}
Let $X$ and $Y$ represent the outcomes of rolling two dice.\footnote{Brief solutions: (a) $1/6$. (b) $1/36$. (c)~$\frac{P(Y = \text{ \resp{1} and }X=\text{ \resp{1}})}{P(X=\text{ \resp{1}})} = \frac{1/36}{1/6} = 1/6$. (d)~The probability is the same as in part~(c): $P(Y=1)=1/6$. The probability that $Y=1$ was unchanged by knowledge about $X$, which makes sense as $X$ and $Y$ are independent.}
\begin{enumerate}[(a)]
\item What is the probability that the first die, $X$, is \resp{1}?
\item What is the probability that both $X$ and $Y$ are \resp{1}?
\item Use the formula for conditional probability to compute $P(Y =$ \resp{1}$\ |\ X = $ \resp{1}$)$.
\item What is $P(Y=1)$? Is this different from the answer from part (c)? Explain.
\end{enumerate}
\end{exercise}

\textC{\newpage}

We can show in Guided Practice~\ref{condProbOfRollingA1AfterOne1}(c) that the conditioning information has no influence by using the Multiplication Rule for independence processes:
\begin{eqnarray*}
P(Y=\text{\resp{1}}\ |\ X=\text{\resp{1}})
	&=& \frac{P(Y=\text{\resp{1} and }X=\text{\resp{1}})}{P(X=\text{\resp{1}})} \\
	&=& \frac{P(Y=\text{\resp{1}})\times \color{oiGB}P(X=\text{\resp{1}})}{\color{oiGB}P(X=\text{\resp{1}})} \\
	&=& P(Y=\text{\resp{1}}) \\
\end{eqnarray*}

\begin{exercise}
Ron is watching a roulette table in a casino and notices that the last five outcomes were \resp{black}. He figures that the chances of getting \resp{black} six times in a row is very small (about $1/64$) and puts his paycheck on red. What is wrong with his reasoning?\footnote{He has forgotten that the next roulette spin is independent of the previous spins. Casinos do employ this practice; they post the last several outcomes of many betting games to trick unsuspecting gamblers into believing the odds are in their favor. This is called the \term{gambler's fallacy}.}
\end{exercise}


\subsection{Tree diagrams}

\index{data!smallpox|)}
\index{tree diagram|(}

\termsub{Tree diagrams}{tree diagram} are a tool to organize outcomes and probabilities around the structure of the data. They are most useful when two or more processes occur in a sequence and each process is conditioned on its predecessors.

The \data{smallpox} data fit this description. We see the population as split by \var{inoculation}: \resp{yes} and \resp{no}. Following this split, survival rates were observed for each group. This structure is reflected in the \term{tree diagram} shown in Figure~\ref{smallpoxTreeDiagram}. The first branch for \var{inoculation} is said to be the \term{primary} branch while the other branches are \term{secondary}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.93\textwidth]{ch_probability/figures/smallpoxTreeDiagram/smallpoxTreeDiagram}
\caption{A tree diagram of the \data{smallpox} data set.}
\label{smallpoxTreeDiagram}
\end{figure}

Tree diagrams are annotated with marginal and conditional probabilities, as shown in Figure~\ref{smallpoxTreeDiagram}. This tree diagram splits the smallpox data by \var{inoculation} into the \resp{yes} and \resp{no} groups with respective marginal probabilities 0.0392 and 0.9608. The secondary branches are conditioned on the first, so we assign conditional probabilities to these branches. For example, the top branch in Figure~\ref{smallpoxTreeDiagram} is the probability that \var{result} = \resp{lived} conditioned on the information that \var{inoculated} = \resp{yes}. We may (and usually do) construct joint probabilities at the end of each branch in our tree by multiplying the numbers we come across as we move from left to right. These joint probabilities are computed using the General Multiplication Rule:
\begin{eqnarray*}
&& P(\text{\var{inoculated} = \resp{yes} and \var{result} = \resp{lived}}) \\
	&&\quad = P(\text{\var{inoculated} = \resp{yes}})\times P(\text{\var{result} = \resp{lived}}|\text{\var{inoculated} = \resp{yes}}) \\
	&&\quad = 0.0392\times 0.9754=0.0382
\end{eqnarray*}

\begin{example}{Consider the midterm and final for a statistics class. Suppose 13\% of students earned an \resp{A} on the midterm. Of those students who earned an \resp{A} on the midterm, 47\% received an \resp{A} on the final, and 11\% of the students who earned lower than an \resp{A} on the midterm received an \resp{A} on the final. You randomly pick up a final exam and notice the student received an \resp{A}. What is the probability that this student earned an \resp{A} on the midterm?} \label{exerciseForTreeDiagramOfStudentGettingAOnMidtermGivenThatSheGotAOnFinal}
The end-goal is to find $P(\text{\var{midterm} = \resp{A}} | \text{\var{final} = \resp{A}})$. To calculate this conditional probability, we need the following probabilities:
\begin{eqnarray*}
P(\text{\var{midterm} = \resp{A} and \var{final} = \resp{A}}) \qquad\text{and}\qquad
P(\text{\var{final} = \resp{A}})
\end{eqnarray*}
However, this information is not provided, and it is not obvious how to calculate these probabilities. Since we aren't sure how to proceed, it is useful to organize the information into a tree diagram, as shown in Figure~\ref{testTree}. When constructing a tree diagram, variables provided with marginal probabilities are often used to create the tree's primary branches; in this case, the marginal probabilities are provided for midterm grades. The final grades, which correspond to the conditional probabilities provided, will be shown on the secondary branches.

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{ch_probability/figures/testTree/testTree}
\caption{A tree diagram describing the \var{midterm} and \var{final} variables.}
\label{testTree}
\end{figure}

With the tree diagram constructed, we may compute the required probabilities:
\begin{eqnarray*}
&&P(\text{\var{midterm} = \resp{A} and \var{final} = \resp{A}}) = 0.0611 \\
&&P(\text{\underline{\color{black}\var{final} = \resp{A}}})  \\
&& \quad= P(\text{\var{midterm} = \resp{other} and \underline{\color{black}\var{final} = \resp{A}}}) + P(\text{\var{midterm} = \resp{A} and \underline{\color{black}\var{final} = \resp{A}}}) \\
&& \quad= 0.0957 + 0.0611  = 0.1568
\end{eqnarray*}
The marginal probability, $P($\var{final} = \resp{A}$)$, was calculated by adding up all the joint probabilities on the right side of the tree that correspond to \var{final} = \resp{A}. We may now finally take the ratio of the two probabilities:
\begin{eqnarray*}
P(\text{\var{midterm} = \resp{A}} | \text{\var{final} = \resp{A}}) &=& \frac{P(\text{\var{midterm} = \resp{A} and \var{final} = \resp{A}})}{P(\text{\var{final} = \resp{A}})} \\
&=& \frac{0.0611}{0.1568} = 0.3897
\end{eqnarray*}
The probability the student also earned an A on the midterm is about 0.39.
\end{example}

\begin{exercise}
After an introductory statistics course, 78\% of students can successfully construct tree diagrams. Of those who can construct tree diagrams, 97\% passed, while only 57\% of those students who could not construct tree diagrams passed. (a)~Organize this information into a tree diagram. (b)~What is the probability that a randomly selected student passed? (c)~Compute the probability a student is able to construct a tree diagram if it is known that she passed.\footnote{\begin{minipage}[t]{0.47\linewidth}
(a) The tree diagram is shown to the right.
(b)~Identify which two joint probabilities represent students who passed, and add them: $P($passed$) = 0.7566+0.1254= 0.8820$. (c)~$P($construct tree diagram $|$ passed$) = \frac{0.7566}{0.8820} = 0.8578$. \vspace{15mm} \\\ 
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[width=\textwidth]{ch_probability/figures/treeDiagramAndPass/treeDiagramAndPass} \vspace{-25mm}
\end{minipage}}
\end{exercise}


\subsection{Bayes' Theorem}
\label{bayesTheoremSubsection}

\index{Bayes' Theorem|(}

In many instances, we are given a conditional probability of the form
\begin{align*}
P(\text{statement about variable 1 } | \text{ statement about variable 2})
\end{align*}
but we would really like to know the inverted conditional probability:
\begin{align*}
P(\text{statement about variable 2 } | \text{ statement about variable 1})
\end{align*}
Tree diagrams can be used to find the second conditional probability when given the first. However, sometimes it is not possible to draw the scenario in a tree diagram. In these cases, we can apply a very useful and general formula: Bayes' Theorem.

We first take a critical look at an example of inverting conditional probabilities where we still apply a tree diagram.

\begin{example}{In Canada, about 0.35\% of women over 40 will develop breast cancer in any given year. A common screening test for cancer is the mammogram, but this test is not perfect. In about 11\% of patients with breast cancer, the test gives a \term{false negative}: it indicates a woman does not have breast cancer when she does have breast cancer. Similarly, the test gives a \term{false positive} in 7\% of patients who do not have breast cancer: it indicates these patients have breast cancer when they actually do not.\footnote{The probabilities reported here were obtained using studies reported at \oiRedirect{textbook-breastCancerDotOrg_20090831b}{www.breastcancer.org} and \oiRedirect{textbook-ncbi_nih_breast_cancer}{www.ncbi.nlm.nih.gov/pmc/articles/PMC1173421}.} If we tested a random woman over 40 for breast cancer using a mammogram and the test came back positive -- that is, the test suggested the patient has cancer -- what is the probability that the patient actually has breast cancer?} 

\label{probabilityOfBreastCancerGivenPositiveTestExample}

\begin{figure}[h]
\centering
\includegraphics[width=0.93\textwidth]{ch_probability/figures/BreastCancerTreeDiagram/BreastCancerTreeDiagram}
\caption{Tree diagram for Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample}, computing the probability a random patient who tests positive on a mammogram actually has breast cancer.}
\label{BreastCancerTreeDiagram}
\end{figure}

Notice that we are given sufficient information to quickly compute the probability of testing positive if a woman has breast cancer ($1.00-0.11=0.89$). However, we seek the inverted probability of cancer given a positive test result. (Watch out for the non-intuitive medical language: a~\emph{positive} test result suggests the possible presence of cancer in a mammogram screening.) This inverted probability may be broken into two pieces:
\begin{align*}
P(\text{has BC } | \text{ mammogram$^+$}) = \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})}
\end{align*}
where ``has BC'' is an abbreviation for the patient actually having breast cancer and ``mammogram$^+$'' means the mammogram screening was positive. A tree diagram is useful for identifying each probability and is shown in Figure~\ref{BreastCancerTreeDiagram}. The probability the patient has breast cancer and the mammogram is positive is
\begin{align*}
P(\text{has BC and mammogram$^+$}) &= P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC}) \\
	&= 0.89\times 0.0035 = 0.00312
\end{align*}
The probability of a positive test result is the sum of the two corresponding scenarios:
\begin{align*}
P(\text{\underline{\color{black}mammogram$^+$}}) &= P(\text{\underline{\color{black}mammogram$^+$} and has BC}) + P(\text{\underline{\color{black}mammogram$^+$} and no BC}) \\
	&= P(\text{has BC})P(\text{mammogram$^+$ } | \text{ has BC}) \\
	&\qquad\qquad	+ P(\text{no BC})P(\text{mammogram$^+$ } | \text{ no BC}) \\
	&= 0.0035\times 0.89 + 0.9965\times 0.07 = 0.07288
\end{align*}
Then if the mammogram screening is positive for a patient, the probability the patient has breast cancer is
\begin{align*}
P(\text{has BC } | \text{ mammogram$^+$})
	&= \frac{P(\text{has BC and mammogram$^+$})}{P(\text{mammogram$^+$})}\\
	&= \frac{0.00312}{0.07288} \approx 0.0428
\end{align*}
That is, even if a patient has a positive mammogram screening, there is still only a~4\%~chance that she has breast cancer.
\end{example}

Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample} highlights why doctors often run more tests regardless of a first positive test result. When a medical condition is rare, a single positive test isn't generally definitive.

Consider again the last equation of Example~\ref{probabilityOfBreastCancerGivenPositiveTestExample}.
Using the tree diagram, we can see that the numerator (the top of the fraction) is equal to the following product:
\begin{align*}
P(\text{has BC and mammogram$^+$}) = P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})
\end{align*}
The denominator -- the probability the screening was positive -- is equal to the sum of probabilities for each positive screening scenario:
\begin{align*}
P(\text{\underline{\color{black}mammogram$^+$}})
	&= P(\text{\underline{\color{black}mammogram$^+$} and no BC})
		+ P(\text{\underline{\color{black}mammogram$^+$} and has BC})
\end{align*}
In the example, each of the probabilities on the right side was broken down into a product of a conditional probability and marginal probability using the tree diagram.
\begin{align*}
P(\text{mammogram$^+$})
	&= P(\text{mammogram$^+$ and no BC}) + P(\text{mammogram$^+$ and has BC}) \\
	&= P(\text{mammogram$^+$ } | \text{ no BC})P(\text{no BC}) \\
			   &\qquad\qquad + P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})
\end{align*}
We can see an application of Bayes' Theorem by substituting the resulting probability expressions into the numerator and denominator of the original conditional probability.
\begin{align*}
& P(\text{has BC } | \text{ mammogram$^+$})  \\
& \qquad= \frac{P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})}
	{P(\text{mammogram$^+$ } | \text{ no BC})P(\text{no BC}) + P(\text{mammogram$^+$ } | \text{ has BC})P(\text{has BC})}
\end{align*}

\begin{termBox}{\tBoxTitle{Bayes' Theorem: inverting probabilities}
Consider the following conditional probability for variable 1 and variable 2:\vspace{-1.5mm}
\begin{align*}
P(\text{outcome $A_1$ of variable 1 } | \text{ outcome $B$ of variable 2})
\end{align*}
Bayes' Theorem states that this conditional probability can be identified as the following fraction:\vspace{-1.5mm}
\begin{align}
\frac{P(B | A_1) P(A_1)}
	{P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + \cdots + P(B | A_k) P(A_k)}
	\label{equationOfBayesTheorem}
\end{align}
where $A_2$, $A_3$, ..., and $A_k$ represent all other possible outcomes of the first variable.}\index{Bayes' Theorem|textbf}
\end{termBox}

Bayes' Theorem is just a generalization of what we have done using tree diagrams. The numerator identifies the probability of getting both $A_1$ and $B$. The denominator is the marginal probability of getting $B$. This bottom component of the fraction appears long and complicated since we have to add up probabilities from all of the different ways to get $B$. We always completed this step when using tree diagrams. However, we usually did it in a separate step so it didn't seem as complex.

To apply Bayes' Theorem correctly, there are two preparatory steps:
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(1)] First identify the marginal probabilities of each possible outcome of the first variable: $P(A_1)$, $P(A_2)$, ..., $P(A_k)$.
\item[(2)] Then identify the probability of the outcome $B$, conditioned on each possible scenario for the first variable: $P(B | A_1)$, $P(B | A_2)$, ..., $P(B | A_k)$.
\end{enumerate}
Once each of these probabilities are identified, they can be applied directly within the formula.

\begin{tipBox}{\tipBoxTitle{Only use Bayes' Theorem when tree diagrams are difficult}
Drawing a tree diagram makes it easier to understand how two variables are connected. Use Bayes' Theorem only when there are so many scenarios that drawing a tree diagram would be complex.}
\end{tipBox}

\textC{\newpage}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent}
Jose visits campus every Thursday evening. However, some days the parking garage is full, often due to college events. There are academic events on 35\% of evenings, sporting events on 20\% of evenings, and no events on 45\% of evenings. When there is an academic event, the garage fills up about 25\% of the time, and it fills up 70\% of evenings with sporting events. On evenings when there are no events, it only fills up about 5\% of the time. If Jose comes to campus and finds the garage full, what is the probability that there is a sporting event? Use a tree diagram to solve this problem.\footnote{\begin{minipage}[t]{0.47\linewidth}
The tree diagram, with three primary branches, is shown to the right. Next, we identify two probabilities from the tree diagram. (1) The probability that there is a sporting event and the garage is full: 0.14. (2) The probability the garage is full: $0.0875 + 0.14 + 0.0225 = 0.25$. Then the solution is the ratio of these probabilities: $\frac{0.14}{0.25} = 0.56$. If the garage is full, there is a 56\% probability that there is a sporting event. \vspace{0.1mm} \\\ 
\end{minipage}
\begin{minipage}[c]{0.5\linewidth}
\includegraphics[width=\textwidth]{ch_probability/figures/treeDiagramGarage/treeDiagramGarage}\vspace{-30mm}
\end{minipage}}
\end{exercise}

\begin{example}{Here we solve the same problem presented in Guided Practice~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent}, except this time we use Bayes' Theorem.}
The outcome of interest is whether there is a sporting event (call this $A_1$), and the condition is that the lot is full ($B$). Let $A_2$ represent an academic event and $A_3$ represent there being no event on campus. Then the given probabilities can be written as
\begin{align*}
&P(A_1) = 0.2 &&P(A_2) = 0.35 &&P(A_3) = 0.45 \\
&P(B | A_1) = 0.7 &&P(B | A_2) = 0.25 &&P(B | A_3) = 0.05
\end{align*}
Bayes' Theorem can be used to compute the probability of a sporting event ($A_1$) under the condition that the parking lot is full ($B$):
\begin{align*}
P(A_1 | B) &= \frac{P(B | A_1) P(A_1)}{P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + P(B | A_3) P(A_3)} \\
		&= \frac{(0.7)(0.2)}{(0.7)(0.2) + (0.25)(0.35) + (0.05)(0.45)} \\
		&= 0.56 
\end{align*}
Based on the information that the garage is full, there is a 56\% probability that a sporting event is being held on campus that evening.
\end{example}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsAnAcademicEvent}
Use the information in the previous exercise and example to verify the probability that there is an academic event conditioned on the parking lot being full is 0.35.\footnote{Short answer:
\begin{align*}
P(A_2 | B) &= \frac{P(B | A_2) P(A_2)}{P(B | A_1) P(A_1) + P(B | A_2) P(A_2) + P(B | A_3) P(A_3)} \\
		&= \frac{(0.25)(0.35)}{(0.7)(0.2) + (0.25)(0.35) + (0.05)(0.45)} \\
		&= 0.35
\end{align*}}
\end{exercise}

\begin{exercise} \label{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsNoEvent}
In Guided Practice~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsASportingEvent} and~\ref{exerciseForParkingLotOnCampusBeingFullAndWhetherOrNotThereIsAnAcademicEvent}, you found that if the parking lot is full, the probability there is a sporting event is 0.56 and the probability there is an academic event is 0.35. Using this information, compute $P($no event $|$ the lot is full$)$.\footnote{Each probability is conditioned on the same information that the garage is full, so the complement may be used: $1.00 - 0.56 - 0.35 = 0.09$.}
\end{exercise}

The last several exercises offered a way to update our belief about whether there is a sporting event, academic event, or no event going on at the school based on the information that the parking lot was full. This strategy of \emph{updating beliefs} using Bayes' Theorem is actually the foundation of an entire section of statistics called \term{Bayesian statistics}. While Bayesian statistics is very important and useful, we will not have time to cover much more of it in this book.

\index{Bayes' Theorem|)}
\index{tree diagram|)}
\index{conditional probability|)}
\index{probability|)}



%_________________
\section{Sampling from a small population (special topic)}
\label{smallPop}

\begin{example}{Professors sometimes select a student at random to answer a question. If each student has an equal chance of being selected and there are 15 people in your class, what is the chance that she will pick you for the next question?}
If there are 15 people to ask and none are skipping class, then the probability is $1/15$, or about $0.067$.
\end{example}

\begin{example}{If the professor asks 3 questions, what is the probability that you will not be selected? Assume that she will not pick the same person twice in a given lecture.}\label{3woRep}
For the first question, she will pick someone else with probability $14/15$. When she asks the second question, she only has 14 people who have not yet been asked. Thus, if you were not picked on the first question, the probability you are again not picked is $13/14$. Similarly, the probability you are again not picked on the third question is $12/13$, and the probability of not being picked for any of the three questions is
\begin{eqnarray*}
&&P(\text{not picked in 3 questions}) \\
&&\quad = P(\text{\var{Q1}} = \text{\resp{not\_\hspace{0.3mm}picked}, }\text{\var{Q2}} = \text{\resp{not\_\hspace{0.3mm}picked}, }\text{\var{Q3}} = \text{\resp{not\_\hspace{0.3mm}picked}.}) \\
&&\quad = \frac{14}{15}\times\frac{13}{14}\times\frac{12}{13} = \frac{12}{15} = 0.80
\end{eqnarray*}
\end{example}

\begin{exercise}
What rule permitted us to multiply the probabilities in Example~\ref{3woRep}?\footnote{The three probabilities we computed were actually one marginal probability, $P($\var{Q1}$ = $\resp{not\_\hspace{0.3mm}picked}$)$, and two conditional probabilities:
\begin{eqnarray*}
&&P(\text{\var{Q2}} =  \text{\resp{not\_\hspace{0.3mm}picked} }|\text{ \var{Q1}} = \text{\resp{not\_\hspace{0.3mm}picked}}) \\
&&P(\text{\var{Q3}} =  \text{\resp{not\_\hspace{0.3mm}picked} }|\text{ \var{Q1}} = \text{\resp{not\_\hspace{0.3mm}picked}, }\text{\var{Q2}} = \text{\resp{not\_\hspace{0.3mm}picked}})
\end{eqnarray*}
Using the General Multiplication Rule, the product of these three probabilities is the probability of not being picked in 3 questions.}
\end{exercise}

\textC{\newpage}

\begin{example}{Suppose the professor randomly picks without regard to who she already selected, i.e. students can be picked more than once. What is the probability that you will not be picked for any of the three questions?}\label{3wRep}
Each pick is independent, and the probability of not being picked for any individual question is $14/15$. Thus, we can use the Multiplication Rule for independent processes.
\begin{eqnarray*}
&&P(\text{not picked in 3 questions}) \\
&&\quad = P(\text{\var{Q1}} = \text{\resp{not\_\hspace{0.3mm}picked}, }\text{\var{Q2}} = \text{\resp{not\_\hspace{0.3mm}picked}, }\text{\var{Q3}} = \text{\resp{not\_\hspace{0.3mm}picked}.}) \\
&&\quad = \frac{14}{15}\times\frac{14}{15}\times\frac{14}{15} = 0.813
\end{eqnarray*}
You have a slightly higher chance of not being picked compared to when she picked a new person for each question. However, you now may be picked more than once.
\end{example}

\begin{exercise}
Under the setup of Example~\ref{3wRep}, what is the probability of being picked to answer all three questions?\footnote{$P($being picked to answer all three questions$) = \left(\frac{1}{15}\right)^3 = 0.00030$.}
\end{exercise}

If we sample from a small population \term{without replacement}, we no longer have independence between our observations. In Example~\ref{3woRep}, the probability of not being picked for the second question was conditioned on the event that you were not picked for the first question. In Example~\ref{3wRep}, the professor sampled her students \term{with replacement}: she repeatedly sampled the entire class without regard to who she already picked. 

\begin{exercise} \label{raffleOf30TicketsWWOReplacement}
Your department is holding a raffle. They sell 30 tickets and offer seven prizes. (a) They place the tickets in a hat and draw one for each prize. The tickets are sampled without replacement, i.e. the selected tickets are not placed back in the hat. What is the probability of winning a prize if you buy one ticket? (b)~What if the tickets are sampled with replacement?\footnote{(a) First determine the probability of not winning. The tickets are sampled without replacement, which means the probability you do not win on the first draw is $29/30$, $28/29$ for the second, ..., and $23/24$ for the seventh. The probability you win no prize is the product of these separate probabilities: $23/30$. That is, the probability of winning a prize is $1 - 23/30 = 7/30 = 0.233$. (b)~When the tickets are sampled with replacement, there are seven independent draws. Again we first find the probability of not winning a prize: $(29/30)^7 = 0.789$. Thus, the probability of winning (at least) one prize when drawing with replacement is 0.211.}
\end{exercise}

\begin{exercise} \label{followUpToRaffleOf30TicketsWWOReplacement}
Compare your answers in Guided Practice~\ref{raffleOf30TicketsWWOReplacement}. How much influence does the sampling method have on your chances of winning a prize?\footnote{There is about a 10\% larger chance of winning a prize when using sampling without replacement. However, at most one prize may be won under this sampling procedure.}
\end{exercise}

Had we repeated Guided Practice~\ref{raffleOf30TicketsWWOReplacement} with 300 tickets instead of 30, we would have found something interesting: the results would be nearly identical. The probability would be 0.0233 without replacement and 0.0231 with replacement. When the sample size is only a small fraction of the population (under 10\%), observations are nearly independent even when sampling without replacement.



\textC{\newpage}



%_________________
\section{Random variables (special topic)}
\label{randomVariablesSection}

\index{random variable|(}

\begin{example}{Two books are assigned for a statistics class: a textbook and its corresponding study guide. The university bookstore determined 20\% of enrolled students do not buy either book, 55\% buy the textbook only, and 25\% buy both books, and these percentages are relatively constant from one term to another. If~there are 100 students enrolled, how many books should the bookstore expect to sell to this class?}\label{bookStoreSales}
Around 20 students will not buy either book (0 books total), about 55 will buy one book (55 books total), and approximately 25 will buy two books (totaling 50 books for these 25 students). The bookstore should expect to sell about 105 books for this class.
\end{example}

\begin{exercise}
Would you be surprised if the bookstore sold slightly more or less than 105 books?\footnote{If they sell a little more or a little less, this should not be a surprise. Hopefully Chapter~\ref{introductionToData} helped make clear that there is natural variability in observed data. For example, if we would flip a coin 100 times, it will not usually come up heads exactly half the time, but it will probably be close.}
\end{exercise}

\begin{example}{The textbook costs \$137 and the study guide \$33. How much revenue should the bookstore expect from this class of 100 students?}\label{bookStoreRev}
About 55 students will just buy a textbook, providing revenue of
\begin{eqnarray*}
\$137 \times  55 = \$7,535
\end{eqnarray*}
The roughly 25 students who buy both the textbook and the study guide would pay a total of
\begin{eqnarray*}
(\$137 + \$33) \times  25 = \$170 \times  25 = \$4,250
\end{eqnarray*}
Thus, the bookstore should expect to generate about $\$7,535 + \$4,250 = \$11,785$ from these 100 students for this one class. However, there might be some \emph{sampling variability} so the actual amount may differ by a little bit.
\end{example}

\begin{figure}[h]
\centering
\includegraphics[width=0.65\textwidth]{ch_probability/figures/bookCostDist/bookCostDist}
\caption{Probability distribution for the bookstore's revenue from a single student. The distribution balances on a triangle representing the average revenue per student.}
\label{bookCostDist}
\end{figure}

\begin{example}{What is the average revenue per student for this course?}\label{revFromStudent}
The expected total revenue is \$11,785, and there are 100 students. Therefore the expected revenue per student is $\$11,785/100 =  \$117.85$.
\end{example}

\subsection{Expectation}

\index{expectation|(}

We call a variable or process with a numerical outcome a \term{random variable}, and we usually represent this random variable with a capital letter such as $X$, $Y$, or $Z$. The amount of money a single student will spend on her statistics books is a random variable, and we represent it by $X$.

\begin{termBox}{\tBoxTitle{Random variable}
A random process or variable with a numerical outcome.}
\end{termBox}

The possible outcomes of $X$ are labeled with a corresponding lower case letter $x$ and subscripts. For example, we write $x_1=\$0$, $x_2=\$137$, and $x_3=\$170$, which occur with probabilities $0.20$, $0.55$, and $0.25$. The distribution of $X$ is summarized in Figure~\ref{bookCostDist} and Table~\ref{statSpendDist}.

\begin{table}[h]
\centering
\begin{tabular}{l ccc r}
\hline
$i$	  & 1 & 2 & 3  & Total\\
\hline
$x_i$ & \$0 & \$137 & \$170 & --\\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 & 1.00 \\
\hline
\end{tabular}
\caption{The probability distribution for the random variable $X$, representing the bookstore's revenue from a single student.}
\label{statSpendDist}
\end{table}

We computed the average outcome of $X$ as \$117.85 in Example~\ref{revFromStudent}. We call this average the \term{expected value} of $X$, denoted by $E(X)$\index{EX@$E(X)$}\marginpar[\raggedright\vspace{-3mm}

$E(X)$\vspace{1mm}\\\footnotesize Expected\\value of $X$]{\raggedright\vspace{-3mm}

$E(X)$\vspace{1mm}\\\footnotesize Expected\\value of $X$}. The expected value of a random variable is computed by adding each outcome weighted by its probability:
\begin{align*}
E(X) &= 0 \times  P(X=0) + 137 \times  P(X=137) + 170 \times  P(X=170) \\
	&= 0 \times  0.20 + 137 \times  0.55 + 170 \times  0.25 = 117.85
\end{align*}

\begin{termBox}{\tBoxTitle{Expected value of a Discrete Random Variable}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$, the expected value of $X$ is the sum of each outcome multiplied by its corresponding probability:
\begin{align}
E(X) 	&= x_1\times P(X=x_1) + \cdots + x_k\times P(X=x_k) \notag \\
	&= \sum_{i=1}^{k}x_iP(X=x_i)
\end{align}
The Greek letter $\mu$\index{Greek!mu ($\mu$)} may be used in place of the notation $E(X)$.}
\end{termBox}

The expected value for a random variable represents the average outcome. For example, $E(X)=117.85$ represents the average amount the bookstore expects to make from a single student, which we could also write as $\mu=117.85$.

It is also possible to compute the expected value of a continuous random variable (see Section~\ref{contDist}). However, it requires a little calculus and we save it for a later class.\footnote{$\mu = \int xf(x)dx$ where $f(x)$ represents a function for the density curve.}

In physics, the expectation holds the same meaning as the center of gravity. The distribution can be represented by a series of weights at each outcome, and the mean represents the balancing point. This is represented in Figures~\ref{bookCostDist} and~\ref{bookWts}. The idea of a center of gravity also expands to continuous probability distributions. Figure~\ref{contBalance} shows a continuous probability distribution balanced atop a wedge placed at the mean.

\begin{figure}
\centering
\includegraphics[width=0.72\textwidth]{ch_probability/figures/bookWts/bookWts}
\caption{A weight system representing the probability distribution for $X$. The string holds the distribution at the mean to keep the system balanced.}
\label{bookWts}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.68\textwidth]{ch_probability/figures/contBalance/contBalance}
\caption{A continuous distribution can also be balanced at its mean.}
\label{contBalance}
\end{figure}

\index{expectation|)}


\subsection{Variability in random variables}

Suppose you ran the university bookstore. Besides how much revenue you expect to generate, you might also want to know the volatility (variability) in your revenue. 

The \indexthis{variance}{variance} and \indexthis{standard deviation}{standard deviation} can be used to describe the variability of a random variable. Section~\ref{variability}
introduced a method for finding the variance and standard deviation for a data set. We first computed deviations from the mean ($x_i - \mu$), squared those deviations, and took an average to get the variance. In the case of a random variable, we again compute squared deviations. However, we take their sum weighted by their corresponding probabilities, just like we did for the expectation. This weighted sum of squared deviations equals the variance, and we calculate the standard deviation by taking the square root of the variance, just as we did in Section~\ref{variability}.

\begin{termBox}{\tBoxTitle{General variance formula}
If $X$ takes outcomes $x_1$, ..., $x_k$ with probabilities $P(X=x_1)$, ..., $P(X=x_k)$ and expected value $\mu=E(X)$, then the variance of $X$, denoted by $Var(X)$ or the symbol $\sigma^2$, is
\begin{align}
\sigma^2 &= (x_1-\mu)^2\times P(X=x_1) + \cdots \notag \\
	& \qquad\quad\cdots+ (x_k-\mu)^2\times P(X=x_k) \notag \\
	&= \sum_{j=1}^{k} (x_j - \mu)^2 P(X=x_j)
\end{align}
The standard deviation of $X$, labeled $\sigma$\index{Greek!sigma ($\sigma$)}, is the square root of the variance.}
\end{termBox}
\marginpar[\raggedright\vspace{-47mm}

$Var(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$]{\raggedright\vspace{-47mm}

$Var(X)$\vspace{1mm}\\\footnotesize Variance\\of $X$}

\begin{example}{Compute the expected value, variance, and standard deviation of $X$, the revenue of a single statistics student for the bookstore.}
It is useful to construct a table that holds computations for each outcome separately, then add up the results.
\begin{center}
\begin{tabular}{l rrr r}
\hline
$i$ & 1 & 2 & 3 & Total \\
\hline
$x_i$ & \$0 & \$137 & \$170 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i \times  P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
\hline
\end{tabular}
\end{center}
Thus, the expected value is $\mu=117.85$, which we computed earlier. The variance can be constructed by extending this table:
\begin{center}
\begin{tabular}{l rrr r}
\hline
$i$ & 1 & 2 & 3 & Total \\
\hline
$x_i$ & \$0 & \$137 & \$170 &  \\
$P(X=x_i)$ & 0.20 & 0.55 & 0.25 &  \\
$x_i \times  P(X=x_i)$ & 0 & 75.35 & 42.50 & 117.85 \\
$x_i - \mu$ & -117.85 & 19.15 & 52.15 &  \\
$(x_i-\mu)^2$ & 13888.62 &  366.72 & 2719.62 &  \\
$(x_i-\mu)^2\times P(X=x_i)$ & 2777.7 & 201.7 & 679.9 & 3659.3 \\
\hline
\end{tabular}
\end{center}
The variance of $X$ is $\sigma^2 = 3659.3$, which means the standard deviation is $\sigma = \sqrt{3659.3} = \$60.49$.
\end{example}

\begin{exercise}
The bookstore also offers a chemistry textbook for \$159 and a book supplement for \$41. From past experience, they know about 25\% of chemistry students just buy the textbook while 60\% buy both the textbook and supplement.\footnote{(a) 100\% - 25\% - 60\% = 15\% of students do not buy any books for the class. Part~(b) is represented by the first two lines in the table below. The expectation for part~(c) is given as the total on the line $y_i\times P(Y=y_i)$. The result of part~(d) is the square-root of the variance listed on in the total on the last line: $\sigma = \sqrt{Var(Y)} = \$69.28$.
\begin{center}
\begin{tabular}{rrrrr}
  \hline
$i$ (scenario) & 1 (\resp{noBook}) & 2 (\resp{textbook}) & 3 (\resp{both}) & Total \\
  \hline
$y_i$ & 0.00 & 159.00 & 200.00 &  \\
$P(Y=y_i)$ & 0.15 & 0.25 & 0.60 & \\
$y_i\times P(Y=y_i)$ & 0.00 & 39.75 & 120.00 & $E(Y) = 159.75$\\
$y_i-E(Y)$ & -159.75 & -0.75 & 40.25 & \\
$(y_i-E(Y))^2$ & 25520.06 & 0.56 & 1620.06 & \\
$(y_i-E(Y))^2\times P(Y)$ & 3828.0 & 0.1 & 972.0 & $Var(Y) \approx 4800$ \\
   \hline
\end{tabular}
\end{center}}
\begin{enumerate}
\item[(a)] What proportion of students don't buy either book? Assume no students buy the supplement without the textbook.
\item[(b)] Let $Y$ represent the revenue from a single student. Write out the probability distribution of $Y$, i.e. a table for each outcome and its associated probability.
\item[(c)] Compute the expected revenue from a single chemistry student. 
\item[(d)] Find the standard deviation to describe the variability associated with the revenue from a single student.
\end{enumerate}
\end{exercise}

\subsection{Linear combinations of random variables}

So far, we have thought of each variable as being a complete story in and of itself. Sometimes it is more appropriate to use a combination of variables. For instance, the amount of time a person spends commuting to work each week can be broken down into several daily commutes. Similarly, the total gain or loss in a stock portfolio is the sum of the gains and losses in its components.

\begin{example}{John travels to work five days a week. We will use $X_1$ to represent his travel time on Monday, $X_2$ to represent his travel time on Tuesday, and so on. Write an equation using $X_1$, ..., $X_5$ that represents his travel time for the week, denoted by $W$.}
His total weekly travel time is the sum of the five daily values:
$$ W = X_1 + X_2 + X_3 + X_4 + X_5 $$
Breaking the weekly travel time $W$ into pieces provides a framework for understanding each source of randomness and is useful for modeling $W$.
\end{example}

\begin{example}{It takes John an average of 18 minutes each day to commute to work. What would you expect his average commute time to be for the week?}
We were told that the average (i.e. expected value) of the commute time is 18 minutes per day: $E(X_i) = 18$. To get the expected time for the sum of the five days, we can add up the expected time for each individual day:
\begin{align*}
E(W) &= E(X_1 + X_2 + X_3 + X_4 + X_5) \\
	&= E(X_1) + E(X_2) + E(X_3) + E(X_4) + E(X_5) \\
	&= 18 + 18 + 18 + 18 + 18 = 90\text{ minutes}
\end{align*}
The expectation of the total time is equal to the sum of the expected individual times. More generally, the expectation of a sum of random variables is always the sum of the expectation for each random variable.
\end{example}

\begin{exercise} \label{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction}
Elena is selling a TV at a cash auction and also intends to buy a toaster oven in the auction. If $X$ represents the profit for selling the TV and $Y$ represents the cost of the toaster oven, write an equation that represents the net change in Elena's cash.\footnote{She will make $X$ dollars on the TV but spend $Y$ dollars on the toaster oven: $X-Y$.}
\end{exercise}

\begin{exercise}
Based on past auctions, Elena figures she should expect to make about \$175 on the TV and pay about \$23 for the toaster oven. In total, how much should she expect to make or spend?\footnote{$E(X-Y) = E(X) - E(Y) = 175 - 23 = \$152$. She should expect to make about \$152.}
\end{exercise}

\begin{exercise} \label{explainWhyThereIsUncertaintyInTheSum}
Would you be surprised if John's weekly commute wasn't exactly 90 minutes or if Elena didn't make exactly \$152? Explain.\footnote{No, since there is probably some variability. For example, the traffic will vary from one day to next, and auction prices will vary depending on the quality of the merchandise and the interest of the attendees.}
\end{exercise}

Two important concepts concerning combinations of random variables have so far been introduced. First, a final value can sometimes be described as the sum of its parts in an equation. Second, intuition suggests that putting the individual average values into this equation gives the average value we would expect in total. This second point needs clarification -- it is guaranteed to be true in what are called \emph{linear combinations of random variables}.

A \term{linear combination} of two random variables $X$ and $Y$ is a fancy phrase to describe a combination
$$ aX + bY$$
where $a$ and $b$ are some fixed and known numbers. For John's commute time, there were five random variables -- one for each work day -- and each random variable could be written as having a fixed coefficient of 1:
$$ 1X_1 + 1 X_2 + 1 X_3 + 1 X_4 + 1 X_5 $$
For Elena's net gain or loss, the $X$ random variable had a coefficient of +1 and the $Y$ random variable had a coefficient of -1.

When considering the average of a linear combination of random variables, it is safe to plug in the mean of each random variable and then compute the final result. For a few examples of nonlinear combinations of random variables -- cases where we cannot simply plug in the means -- see the footnote.\footnote{If $X$ and $Y$ are random variables, consider the following combinations: $X^{1+Y}$, $X\times Y$, $X/Y$. In such cases, plugging in the average value for each random variable and computing the result will not generally lead to an accurate average value for the end result.}

\begin{termBox}{\tBoxTitle{Linear combinations of random variables and the average result}
If $X$ and $Y$ are random variables, then a linear combination of the random variables is given by
\begin{align}\label{linComboOfRandomVariablesXAndY}
aX + bY
\end{align}
where $a$ and $b$ are some fixed numbers. To compute the average value of a linear combination of random variables, plug in the average of each individual random variable and compute the result:
\begin{align*}
a\times E(X) + b\times E(Y)
\end{align*}
Recall that the expected value is the same as the mean, e.g. $E(X) = \mu_X$.}
\end{termBox}

\begin{example}{Leonard has invested \$6000 in Google Inc. (stock ticker: GOOG) and \$2000 in Exxon Mobil Corp. (XOM). If $X$ represents the change in Google's stock next month and $Y$ represents the change in Exxon Mobil stock next month, write an equation that describes how much money will be made or lost in Leonard's stocks for the month.}
For simplicity, we will suppose $X$ and $Y$ are not in percents but are in decimal form (e.g. if Google's stock increases 1\%, then $X=0.01$; or if it loses 1\%, then $X=-0.01$). Then we can write an equation for Leonard's gain as
\begin{align*}
\$6000\times X + \$2000\times Y
\end{align*}
If we plug in the change in the stock value for $X$ and $Y$, this equation gives the change in value of Leonard's stock portfolio for the month. A positive value represents a gain, and a negative value represents a loss.
\end{example}

\begin{exercise}\label{expectedChangeInLeonardsStockPortfolio}
Suppose Google and Exxon Mobil stocks have recently been rising 2.1\% and 0.4\% per month, respectively. Compute the expected change in Leonard's stock portfolio for next month.\footnote{$E(\$6000\times X + \$2000\times Y) = \$6000\times 0.021 + \$2000\times 0.004 = \$134$.}
% library(stockPortfolio); gr <- getReturns(c("GOOG", "XOM"), start="2006-01-01"); gr
\end{exercise}

\begin{exercise}
You should have found that Leonard expects a positive gain in Guided Practice~\ref{expectedChangeInLeonardsStockPortfolio}. However, would you be surprised if he actually had a loss this month?\footnote{No. While stocks tend to rise over time, they are often volatile in the short term.}
\end{exercise}

\subsection{Variability in linear combinations of random variables}

Quantifying the average outcome from a linear combination of random variables is helpful, but it is also important to have some sense of the uncertainty associated with the total outcome of that combination of random variables. The expected net gain or loss of Leonard's stock portfolio was considered in Guided Practice~\ref{expectedChangeInLeonardsStockPortfolio}. However, there was no quantitative discussion of the volatility of this portfolio. For instance, while the average monthly gain might be about \$134 according to the data, that gain is not guaranteed. Figure~\ref{changeInLeonardsStockPortfolioFor36Months} shows the monthly changes in a portfolio like Leonard's during the 36 months from 2009 to 2011. The gains and losses vary widely, and quantifying these fluctuations is important when investing in stocks.

\begin{figure}[ht]
\centering
\includegraphics[width=0.65\textwidth]{ch_probability/figures/changeInLeonardsStockPortfolioFor36Months/changeInLeonardsStockPortfolioFor36Months}
\caption{The change in a portfolio like Leonard's for the 36 months from 2009 to 2011, where \$6000 is in Google's stock and \$2000 is in Exxon Mobil's.}
\label{changeInLeonardsStockPortfolioFor36Months}
\end{figure}

Just as we have done in many previous cases, we use the variance and standard deviation to describe the uncertainty associated with Leonard's monthly returns. To do so, the variances of each stock's monthly return will be useful, and these are shown in Table~\ref{sumStatOfGOOGXOM}. The stocks' returns are nearly independent.

\begin{table}
\centering
\begin{tabular}{lrrr}
\hline
	& Mean ($\bar{x}$) & Standard deviation ($s$) & Variance ($s^2$) \\
\hline
GOOG & 0.0210	& 0.0846					&	0.0072	\\
XOM & 0.0038		& 0.0519					&	0.0027	\\
\hline
\end{tabular}
\caption{The mean, standard deviation, and variance of the GOOG and XOM stocks. These statistics were estimated from historical stock data, so notation used for sample statistics has been used.}
\label{sumStatOfGOOGXOM}
\end{table}

Here we use an equation from probability theory to describe the uncertainty of Leonard's monthly returns; we leave the proof of this method to a dedicated probability course. The variance of a linear combination of random variables can be computed by plugging in the variances of the individual random variables and squaring the coefficients of the random variables:
\begin{align*}
Var(aX + bY) = a^2\times Var(X) + b^2\times Var(Y)
\end{align*}
It is important to note that this equality assumes the random variables are independent; if independence doesn't hold, then more advanced methods are necessary. This equation can be used to compute the variance of Leonard's monthly return:
\begin{align*}
Var(6000\times X + 2000\times Y)
	&= 6000^2\times Var(X) + 2000^2\times Var(Y) \\
	&= 36,000,000\times 0.0072 + 4,000,000\times 0.0027 \\
	&= 270,000
\end{align*}
The standard deviation is computed as the square root of the variance: $\sqrt{270,000} = \$520$. While an average monthly return of \$134 on an \$8000 investment is nothing to scoff at, the monthly returns are so volatile that Leonard should not expect this income to be very stable.

\begin{termBox}{\tBoxTitle{Variability of linear combinations of random variables}
The variance of a linear combination of random variables may be computed by squaring the constants, substituting in the variances for the random variables, and computing the result:
\begin{align*}
Var(aX + bY) = a^2\times Var(X) + b^2\times Var(Y)
\end{align*}
This equation is valid as long as the random variables are independent of each other. The standard deviation of the linear combination may be found by taking the square root of the variance.}
\end{termBox}

\begin{example}{Suppose John's daily commute has a standard deviation of 4 minutes. What is the uncertainty in his total commute time for the week?} \label{sdOfJohnsCommuteWeeklyTime}
The expression for John's commute time was
\begin{align*}
X_1 + X_2 + X_3 + X_4 + X_5
\end{align*}
Each coefficient is 1, and the variance of each day's time is $4^2=16$. Thus, the variance of the total weekly commute time is
\begin{align*}
&\text{variance }= 1^2 \times  16 + 1^2 \times  16 + 1^2 \times  16 + 1^2 \times  16 + 1^2 \times  16 = 5\times 16 = 80 \\
&\text{standard deviation } = \sqrt{\text{variance}} = \sqrt{80} = 8.94
\end{align*}
The standard deviation for John's weekly work commute time is about 9 minutes.
\end{example}

\begin{exercise}
The computation in Example~\ref{sdOfJohnsCommuteWeeklyTime} relied on an important assumption: the commute time for each day is independent of the time on other days of that week. Do you think this is valid? Explain.\footnote{One concern is whether traffic patterns tend to have a weekly cycle (e.g. Fridays may be worse than other days). If that is the case, and John drives, then the assumption is probably not reasonable. However, if John walks to work, then his commute is probably not affected by any weekly traffic cycle.}
\end{exercise}

\begin{exercise}\label{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuctionVariability}
Consider Elena's two auctions from Guided Practice~\ref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction} on page~\pageref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuction}. Suppose these auctions are approximately independent and the variability in auction prices associated with the TV and toaster oven can be described using standard deviations of \$25 and \$8. Compute the standard deviation of Elena's net gain.\footnote{The equation for Elena can be written as
\begin{align*}
(1)\times X + (-1)\times Y
\end{align*}
The variances of $X$ and $Y$ are 625 and 64. We square the coefficients and plug in the variances:
\begin{align*}
(1)^2\times Var(X) + (-1)^2\times Var(Y) = 1\times 625 + 1\times 64 = 689
\end{align*}
The variance of the linear combination is 689, and the standard deviation is the square root of 689: about \$26.25.}
\end{exercise}

Consider again Guided Practice~\ref{elenaIsSellingATVAndBuyingAToasterOvenAtAnAuctionVariability}. The negative coefficient for $Y$ in the linear combination was eliminated when we squared the coefficients. This generally holds true: negatives in a linear combination will have no impact on the variability computed for a linear combination, but they do impact the expected value computations.

\index{random variable|)}

%_________________
\section{Continuous distributions (special topic)}
\label{contDist}

\index{data!FCID|(}
\index{hollow histogram|(}
\begin{example}{Figure~\ref{fdicHistograms} shows a few different hollow histograms of the variable \var{height} for 3 million US adults from the mid-90's.\footnote{This sample can be considered a simple random sample from the US population. It relies on the USDA Food Commodity Intake Database.} How does changing the number of bins allow you to make different interpretations of the data?}\label{usHeights}
Adding more bins provides greater detail. This sample is extremely large, which is why much smaller bins still work well. Usually we do not use so many bins with smaller sample sizes since small counts per bin mean the bin heights are very volatile.
\end{example}

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{ch_probability/figures/fdicHistograms/fdicHistograms}
\caption{Four hollow histograms of US adults heights with varying bin widths.}
\label{fdicHistograms}
\end{figure}

\begin{example}{What proportion of the sample is between \resp{180} cm and \resp{185} cm tall (about 5'11" to 6'1")?}\label{contDistProb}
We can add up the heights of the bins in the range \resp{180} cm and \resp{185} and divide by the sample size. For instance, this can be done with the two shaded bins shown in Figure~\ref{usHeightsHist180185}. The two bins in this region have counts of 195,307 and 156,239 people, resulting in the following estimate of the probability:
\begin{eqnarray*}
\frac{195307+156239}{\text{3,000,000}} = 0.1172
\end{eqnarray*}
This fraction is the same as the proportion of the histogram's area that falls in the range \resp{180} to \resp{185} cm.
\end{example}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{ch_probability/figures/usHeightsHist180185/usHeightsHist180185}
\caption{A histogram with bin sizes of 2.5 cm. The shaded region represents individuals with heights between \resp{180} and \resp{185} cm. }
\label{usHeightsHist180185}
\end{figure}

\subsection{From histograms to continuous distributions}

Examine the transition from a boxy hollow histogram in the top-left of Figure~\ref{fdicHistograms} to the much smoother plot in the lower-right. In this last plot, the bins are so slim that the hollow histogram is starting to resemble a smooth curve. This suggests the population height as a \emph{continuous} numerical variable might best be explained by a curve that represents the outline of extremely slim bins.

This smooth curve represents a \term{probability density function} (also called a \term{density} or \term{distribution}), and such a curve is shown in Figure~\ref{fdicHeightContDist} overlaid on a histogram of the sample. A density has a special property: the total area under the density's curve is 1. 

\begin{figure}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{ch_probability/figures/fdicHeightContDist/fdicHeightContDist}
\caption{The continuous probability distribution of heights for US adults.}
\label{fdicHeightContDist}
\end{figure}

\index{hollow histogram|)}

\subsection{Probabilities from continuous distributions}

We computed the proportion of individuals with heights \resp{180} to \resp{185} cm in Example~\ref{contDistProb} as a fraction:
\begin{eqnarray*}
\frac{\text{number of people between \resp{180} and \resp{185}}}{\text{total sample size}}
\end{eqnarray*}
We found the number of people with heights between \resp{180} and \resp{185} cm by determining the fraction of the histogram's area in this region. Similarly, we can use the area in the shaded region under the curve to find a probability (with the help of a computer):
\begin{eqnarray*}
P(\text{\var{height} between \resp{180} and \resp{185}})
	= \text{area between \resp{180} and \resp{185}}
	= 0.1157
\end{eqnarray*}
The probability that a randomly selected person is between \resp{180} and \resp{185} cm is 0.1157. This is very close to the estimate from Example~\ref{contDistProb}: 0.1172. 

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ch_probability/figures/fdicHeightContDistFilled/fdicHeightContDistFilled}
\caption{Density for heights in the US adult population with the area between 180 and 185 cm shaded. Compare this plot with Figure~\ref{usHeightsHist180185}.}
\label{fdicHeightContDistFilled}
\end{figure}

\begin{exercise}
Three US adults are randomly selected. The probability a single adult is between \resp{180} and \resp{185} cm is 0.1157.\footnote{Brief answers: (a) $0.1157 \times 0.1157 \times 0.1157 = 0.0015$. (b) $(1-0.1157)^3 = 0.692$} \vspace{-1.5mm}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item[(a)] What is the probability that all three are between \resp{180} and \resp{185} cm tall?
\item[(b)] What is the probability that none are between \resp{180} and \resp{185} cm?
\end{enumerate}
\end{exercise}

\begin{example}{What is the probability that a randomly selected person is \textbf{exactly} \resp{180}~cm? Assume you can measure perfectly.}
\label{probabilityOfExactly180cm}
This probability is zero. A person might be close to \resp{180} cm, but not exactly \resp{180} cm tall. This also makes sense with the definition of probability as area; there is no area captured between \resp{180}~cm and \resp{180}~cm.
\end{example}

\begin{exercise}
Suppose a person's height is rounded to the nearest centimeter. Is there a chance that a random person's \textbf{measured} height will be \resp{180} cm?\footnote{This has positive probability. Anyone between \resp{179.5} cm and \resp{180.5} cm will have a \emph{measured} height of \resp{180} cm. This is probably a more realistic scenario to encounter in practice versus Example~\ref{probabilityOfExactly180cm}.}
\end{exercise}

\index{data!FCID|)}



