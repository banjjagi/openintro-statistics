\chapter{추정의 토대}
\label{foundationsForInference}

통계적 추론은 주로 모수 추정에 대한 품질 해석과 관련된다. 예를 들어, 전통적인 추정 질문은 다음과 같다. ``추정한 평균 $\bar{x}$가 모집단 평균 $\mu$에 근처에 있다는 것을 얼마나 확신할 수 있는가?''  방정식과 구체적인 사항이 설정에 따라 변할 수 있지만, 추정에 대한 토대는 모든 통계학에서 동일하다. 모집단 평균 $\mu$에 관한 추정을 논의하면서 \ref{variabilityInEstimates}-\ref{cltSection}~절에 걸쳐 공통 주제를 소개하고 \ref{aFrameworkForInference}~절에서 다른 모수와 시나리오를 위한 장을 마련한다. 이번 장 이해가 책 나머지 부분을 성공적으로 만들고, 사실 나머지 통계학도 훨씬 더 친숙하게 된다.

\index{데이터!yrbss|(}

다음 몇 절에 걸쳐서 \data{yrbss}라는 데이터를 살펴볼 것이다. 이 데이터는 2013년부터 청소년 위험 행동 감시 시스템(Youth Risk Behavior Surveillance System, YRBSS)에서 나온 고등학생 13,533명을 나타낸다. \footnote{\oiRedirect{textbook-yrbss}{www.cdc.gov/healthyyouth/data/yrbs/data.htm}} 데이터셋 일부가 표~\ref{yrbssDF}에 나와 있고, 변수도 표~\ref{yrbssVariables}에 기술되어 있다.

\begin{table}[h]
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & 연령 & 성별 & 학년 & 신장 & 몸무게 & 헬멧착용 & 운동일수 & 헬스일수 \\ 
  \hline
1 &  14 & female & 9 &  &  & never &   4 &   0 \\ 
  2 &  14 & female & 9 &  &  & never &   2 &   0 \\ 
  3 &  15 & female & 9 & 1.73 & 84.37 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  13582 &  17 & female & 12 & 1.60 & 77.11 & sometimes &   5 &  \\ 
  13583 &  17 & female & 12 & 1.57 & 52.16 & did not ride &   5 &  \\ 
  \hline
\end{tabular}
\caption{\data{yrbss} 데이터셋에서 나온 다섯 사례. 관측점 일부는 공백인데 결측값 때문이다. 예를 들어, 학생 1과 2의 신장과 몸무게가 결측값이다. \textC{\vspace{-2mm}}}
\label{yrbssDF}
\end{table}
% yrbss 데이터셋 가져오기
% library(devtools); install_github("OpenIntroOrg/openintro-r-package", subdir = "openintro")
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss, 4), tail(yrbss, 2))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])

\begin{table}[h]
\centering\small
\begin{tabular}{l p{110mm}}
\hline
{\bf age(연령)} & {\bf 학생 연령.} \\
\hline
\var{gender}(성별) & {학생 성별.} \\
\var{grade}(학급) & 고등학교 학년(미국 기준) \\
\var{height}(신장) & 신장 (단위:미터), 1 미터는 3.28 피트. \\
\var{weight}(몸무게) & 몸무게 (단위: 킬로그램), 1 킬로그램은 2.2 파운드. \\
\var{helmet}(헬멧) & 지난 12개월동안 오토바이를 탈 때 헬멧을 착용한 횟수. \\
\var{active}(운동일수) & 지난 7일 동안 60분 이상 신체적으로 활발한 일수. \\
\var{lifting}(헬스일수) & 지난 7일 동안 근력 운동(예, )을 한 일수. \\
\hline
\end{tabular}
\caption{ \data{yrbss} 데이터셋에 대한 변수와 변수 설명.}
\label{yrbssVariables}
\end{table}

\index{데이터!yrbss\_samp|(}

2013년 YRBSS에 참여한 고등학생 모집단을 생각해보자. 이 모집단에서 단순표본추출을 했는데 표~\ref{yrbssSampDF}에 나와 있다.

We're going to consider the population of high school students who participated in the 2013 YRBSS. We took a simple random sample of this population, which is represented in 표~\ref{yrbssSampDF}.\footnote{각 변수에 대해서 약 10\% 고등학생이 질문에 응답을 하지 않기로 했다. 다중회귀(\ref{multipleAndLogisticRegression}~장 참조)를 사용해서 무응답을 예측했다. 단순함을 위해서 이렇게 예측한 값이 정확하게 사실이라고 가정한다.} \data{yrbss\_samp} 데이터셋으로 언급되는 이 표본을 사용해서 YRBSS 참여자 모집단에 관한 결론을 도출한다. 넓은 의미에서 통계적 추론을 실습이 된다. \data{yrbss\_samp} 데이터셋에서 나온 변수 \var{height}, \var{weight}, \var{active}, \var{lifting}을 요약하는 히스토그램 두개가 그림~\ref{yrbssSampHistograms}에 나와 있다.

\begin{table}
\centering
\begin{tabular}{rrllrrlrr}
  \hline
ID & 연령 & 성별 & 학년 & 신장 & 몸무게 & 헬멧착용 & 운동일수 & 헬스일수 \\ 
  \hline
5653 &  16 & female & 11 & 1.50 & 52.62 & never &   0 &   0 \\ 
  9437 &  17 & male & 11 & 1.78 & 74.84 & rarely &   7 &   5 \\ 
  2021 &  17 & male & 11 & 1.75 & 106.60 & never &   7 &   0 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
  2325 &  14 & male & 9 & 1.70 & 55.79 & never &   1 &   0 \\ 
   \hline
\end{tabular}
\caption{Four observations for the \data{yrbss\_samp} data set, which represents a simple random sample of 100 high schoolers from the 2013 YRBSS.}
\label{yrbssSampDF}
% library(openintro); library(xtable); data(yrbss); xtable(rbind(head(yrbss.samp, 3), tail(yrbss.samp, 1))[, c("age", "gender", "grade", "height", "weight", "helmet_12m", "physically_active_7d", "strength_training_7d")])
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); xtable(yrbss.amp[c(1,2,3,100),])
\end{table}

% WARNING: This figure is referenced in Section 4.2
\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{ch_inference_foundations/figures/yrbssSampHistograms/yrbssSampHistograms} 
\caption{ YRBSS 표본 데이터에 대한 \var{height}, \var{weight}, \var{activity}, \var{lifting} 변수 히스토그램. 신장 \var{height} 변수는 근사적으로 좌우대칭, 몸무게 \var{weight} 변수는 중간정도 우측으로 기울어졌고, 운동일수 \var{activity} 변수는 (기울어짐이 명확하지 않은) 이봉 혹은 다봉, 헬스일수 \var{lifting} 변수는 강하게 우측으로 기울어졌다.\index{기움!예제: 중간}\index{skew!example: 강함}}
\label{yrbssSampHistograms}
\end{figure}


%__________________
\section[Variability in estimates]{추정값에 내재하는 변동성 \sectionvideohref{youtube-DNIauUrRIEM&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{variabilityInEstimates}

\index{점추정|(}

표본을 사용해서 YRBSS 데이터셋에 다음 고등학생 특징을 추정하고자 한다.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[(1)] YRBSS 데이터셋 고등학생 평균 신장은 얼마인가?
\item[(2)] YRBSS 데이터셋 고등학생 평균 몸무게는 얼마인가?
\item[(3)] 평균적으로, YRBSS 데이터셋 고등학생은 주당 몇일 운동하나요?
\item[(4)] 평균적으로, YRBSS 데이터셋 고등학생은 주당 몇일 근력 운동을 하나요?
\end{itemize}

이번 장에서 평균에 집중을 하지만, 변동성에 대한 질문은 실무에서 중요하다. 예를 등러, 만약 학생들이 매우 활동적이거나 거의 비활동적이라면 (분포형태는 이봉), 모든 고등학생을 다소 활동적으로 만드는 것보다 다른 전략을 사용해서 학생들 중 일부 건강을 증진시킬 수도 있다.

% We will use $x_1, ..., x_{100}$ to represent the heights for each high schooler in our sample, and $y_1, ..., y_{100}$ will represent their weights.

\subsection{점추정}
\label{pointEstimates}

\index{점추정값!단일 평균|(}

표본에 기반하여 \term{모집단 평균}(population mean)을 추정하자. 이 작업을 수행하는 가장 직관적인 방식은 단순히 \term{표본 평균}(sample mean)을 얻으면 된다. 즉, 모든 YRBSS 고등학생 평균 신장을 추정하는데 표본에 대한 평균 신장을 계산한다:

\begin{eqnarray*}
\bar{x}_{height} = \frac{1.50 + 1.78 + \dots + 1.70}{100} = 1.697
\end{eqnarray*}
%library(openintro); data(yrbss.samp); mean(yrbss.samp$height); yrbss.samp$height

표본 평균 $\bar{x} = 1.697$ 미터 (5피트, 6.8인치)를 모평균에 대한 \term{점추정}(point estimate)이라고 부른다: 만약 모평균을 추정하는 값을 하나만 고른다면, 이 추정값이 최상의 추측이 된다. 100 명에 대한 신규 표본을 얻어 평균을 다시 계산한다고 가정하자: data{yrbss\_samp} 데이터셋을 사용해서 얻는 답과 정확하게 동일하지는 않은 것이다. 일반적으로 추정값은 표본에 따라 변하고, 이러한 \term{표본 변동}(sampling variation)은 추정값이 모수에는 가깝지만 정확하게 동일하지 않는 점을 시사한다.

변수 \var{weight} (단위 킬로그램) 표본 평균을 조사해서 YRBSS 응답자 평균 몸무게와 주당 평균 운동일수를 추정할 수 있다:

\begin{align*}
\bar{x}_{신장} &= \frac{52.6 + 74.8 + \dots + 55.8}{100} = 68.89
&\bar{x}_{운동일수} &= \frac{0 + 7 + \dots + 1}{100} = 3.75
\end{align*}
%library(openintro); data(yrbss.samp); d <- yrbss.samp$weight; mean(d); d
%library(openintro); data(yrbss.samp); d <- yrbss.samp$physically_active_7d; mean(d); d
평균 체중은 68.89 킬로그램으로 약 151.6 파운드가 나온다.

다른 \term{모수}(population parameters)에 대한 점추정을 산출하면 어떨까? 예를 들어, 모집단 중위수 혹은 모집단 표준편차? 다시 한번, 표본 통계량에 기반하여 모수를 추정한다. 추정 결과는 표~\ref{ptEstimatesYrbssActive}에 나와 있다. 예를 들어, 표본 표준편차를 사용해서 \var{active} 운동일수에 대한 모집단 표준편차는 2.56~일이 나온다.

\begin{table}[h]
\centering
\begin{tabular}{ l rr}
\hline
운동일수 \var{active}	& 추정값 & 모수  \\
\hline
평균		& 3.75 & 3.90 \\
중위수		& 4.00 & 4.00 \\
표준편차		& 2.556 & 2.564 \\
\hline
\end{tabular}
\caption{ 운동일수 \var{active} 변수에 대한 점추정과 모수값. 모수는 전체 YRBSS 응답자 평균, 중위수, 표준편차를 계산해서 산출했다.}
\label{ptEstimatesYrbssActive}
\end{table}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); d <- yrbss.samp$physically_active_7d; mean(d); median(d); sd(d); d <- yrbss$physically_active_7d; mean(d, na.rm = TRUE); median(d, na.rm = TRUE); sd(d, na.rm = TRUE)


\begin{exercise} \label{peOfDiffActiveBetweenGender}
남학생과 여학생에 대한 운동일수에 대한 차이를 추정한다고 가정하자. 만약 $\bar{x}_{men} = 4.3$ 와 $\bar{x}_{women} = 3.2$이라면, 모집단 차이에 대한 좋은 성질을 갖는 점추정값은 무엇일까?
\footnote{두 표본 평균에 대한 차이를 들 수 있다: $4.3 - 3.2 = 1.1$. YRBSS 데이터셋에서 남학생이 평균적으로 여학생보다 약 1.1일 운동일수가 많다.}
\end{exercise}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); (x <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, mean)); diff(x)

\begin{exercise}
만약 참여자 신장에 대한 모집단 사분위간 범위(InterQuanrtile Range, IQR)에 대한 점추정값을 찾는다면, 표본을 사용해서 어떻게 추정값을 만들 수 있을까?\footnote{YRBSS 전체 학생 신장에 대한 점추정값을 얻으려면, 표본 사분위간 범위를 얻어서 구할 수 있다.}

\index{점추정값!단일 평균|)}

\end{exercise}

\subsection{점추정값이 정말 정확하지는 않다.}

추정값은 대체로 참값과 정확하게 같지는 않지만, 데이터가 많아짐에 따라 추정값이 더 나아진다. \data{yrbss\_samp} 데이터셋에서 이동 평균을 도식화해서 이점을 확인할 수 있다. 여기서 각 평균은 데이터 이전 수열로부터 평균보다 계산에서 관측점을 하나더 사용한다. 예를 들어, 수열 두번째 평균은 첫 두 관측점 평균이 되고, 수열 세번째 평균은 첫 세 관측점 평균이다. \data{yrbss\_samp} 데이터셋에서 \var{active} 변수에 대한 이동 평균이 그림~\ref{yrbssActiveRunningMean}에 나와있다. 더 많은 데이터가 이용가능해짐에 따라 모집단 평균 3.90 일에 근접해간다. 

\begin{figure}[h]
   \centering
   \includegraphics[width=0.72\textwidth]{ch_inference_foundations/figures/yrbssActiveRunningMean/yrbssActiveRunningMean}
   \caption{표본에 개별 관측점을 더한 뒤에 계산된 평균. 표본평균이 데이터가 많아짐에 따라 모집단 평균에 근접하는 경향이 있다.}
   \label{yrbssActiveRunningMean}
\end{figure}

표본 점추정값은 모수에 근접만 한다. 표본 점추정값은 표본마다 다르다. 만약 YRBSS 고등학생에 대한 또다른 단순임의표본을 추출하면, 운동일수에 대한 표본평균이 약간 다른 것을 알 수 있다. 추정값이 표본마다 얼마의 변동성을 갖는지 정량화하는 것이 유용하다. 만약 변동성이 작다면 (즉, 표본 평균이 표본마다 그다지 변동하지 않는다면), 이 추정값은 아마도 매우 정확성이 높을 것이다. 만약 표본마다 추정값이 변동한다면, 추정값이 매우 좋을 것으로 기대하지는 말아야 된다.

\subsection{평균에 대한 표준 오차}
\label{seOfTheMean}

\data{yrbss\_samp} 데이터셋에 나온 임의 표본으로부터 YRBSS 고등학생 평균 운동일수가 3.75~일로 추측했다. 또다른 고등학생 100명을 임의추출해서 평균(3.22~일)을 구했다고 가정하자. 또다른 평균(3.67~일)을 구했다고 가정하자. 또다른 평균(4.10~days) 등등. 만약 아주 많이 반복한다면 -- 모든 YRBSS 고등학생 정보를 가질 때만 할 수 있다 -- 표본크기 100을 갖는 표본평균에 대한 \term{표집분포}(sampling distribution)를 만들 수 있다. 그림~\ref{yrbssActive1000SampDist}에 나와있다.

\begin{figure}[h]
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations/figures/yrbssActive1000SampDist/yrbssActive1000SampDist}
   \caption{주당 운동횟수에 대한 표본평균 1000개에 대한 히스토그램. 표본크기는 $n=100$이다.}
   \label{yrbssActive1000SampDist}
\end{figure}

\begin{termBox}{\tBoxTitle{표집 분포}
표집분포(sampling distribution)는 특정 모집단에 대한 고정된 크기 표본에 기반한 점추정값 분포를 나타낸다. 특정 점추정값이 그런 분포에서 나온 것으로 간주하는 것이 유용하다. 표집분포에 대한 개념을 이해하는 것이 통계적 추론을 이해하는 핵심이다.}
\end{termBox}

그림~\ref{yrbssActive1000SampDist}에 나온 표집분포는 단봉이고 근사적으로 대칭이다. 또한 모집단 평균에 정확하게 중심을 잡고 있다: $\mu=3.90$. 직관적으로, 이해가 된다. 표본평균은 모집단 평균 ``주변''에 위치하는 경향이 있어야 한다.

표본평균이 모집단 평균 주변에서 변동성이 있음을 알 수 있는데, 표본평균 분포에 대한 표준편차를 사용해서 정량화할 수 있다: $\sigma_{\bar{x}} = 0.26$. 표본평균에 대한 표준편차는 특정 추정값이 실제 모집단 평균(3.90~일)으로부터 얼마나 떨어져있는지 알려준다. 점추정값에 대한 \term{오차}(error)도 기술할 수 있다. 이런 연유로 일반적으로 이 표준편차를 추정값에 대한 \term{표준오차}(standard error, SE) 라고 부른다.\index{SE}\marginpar[\raggedright\vspace{-4mm}

$SE$\\\footnotesize 표준오차,\\standard error]{\raggedright\vspace{-4mm}

$SE$\\\footnotesize standard\\error} of the estimate.

\begin{termBox}{\tBoxTitle{추정값에 대한 표준오차}
추정값과 연관된 표준편차를 \emph{표준오차}(standard error)라고 부른다. 표준오차는 추정값과 연관된 대표적인 오차 혹은 불확실성을 기술한다.}
\end{termBox}

점추정값 $\bar{x}$에 대한 경우를 고려해볼 때, 문제가 하나있다: 단일 표본에서 표준오차를 추정할 수 있는 명확한 방법이 없다. 하지만, 이 문제를 다룰 수 있는 도움이 되는 도구를 통계이론이 제공한다.


\textC{\newpage}

\begin{exercise}
(a) 모수를 추정할 때 작은 표본 혹은 큰 표본을 사용하는게 좋을까요? 왜 그런가요? (b) (a)에서 나온 추론을 사용해서, 큰 표본에 근거한 점추정값보다 작은 표본에 근거한 점추정값이 더 작은 혹은 더 큰 표준오차를 가질 것으로 예상합니까?
\footnote{(a) 임의표본 두개를 생각해보자: 하나는 표본크기 10, 다른 하나는 표본크기 1000. 작은 표본에서 개별 관측점이 추정값에 영향력이 큰 반면 큰 표본에서는 개별 관측점은 서로 평균을 내게된다. 더 큰 표본크기가 좀더 정확한 추정값을 제공하는 경향이 있다. (b) 만약 추정값이 더 좋다면, 아마도 추정값이 더 적은 오차를 갖는다는 의미를 띈다. (a)에 근거해서, 더 큰 표본크기는 더 작은 표준오차에 상응된다고 직관적으로 볼 수 있다.}
\end{exercise}

표본 100명 고등학생 중에서, 표본평균에 대한 표준오차는 모집단 표준편차를 표본크기에 제곱근을 씌워서 나눈 것이 된다:

\begin{eqnarray*}
SE_{\bar{x}} = \sigma_{\bar{x}} = \frac{\sigma_{x}}{\sqrt{n}} = \frac{2.6}{\sqrt{100}} = 0.26
\end{eqnarray*}

여기서 $\sigma_{x}$는 개별 관측점에 대한 표준편차다. 이것은 우연의 일치가 아니다. 수학적으로 \ref{randomVariablesSection}~절에 나온 확률 도구를 사용해서 관측점이 독립일 때 상기 방정식이 맞다는 것을 보일 수 있다.

\begin{termBox}{\tBoxTitle{표본 평균에 대한 표준오차(SE) 계산}
표준편차 $\sigma$를 갖는 모집단에서 나온 $n$ 독립 관측점이 주어졌을 때, 표본평균에 대한 표준오차는 다음과 같다.\vspace{-1mm}

\begin{eqnarray}
SE = \frac{\sigma}{\sqrt{n}}
\label{seOfXBar}
\end{eqnarray}\vspace{-3mm}%

표본 관측점의 독립성을 보장할 수 있는 신뢰성있는 방법은 10\% 미만 모집단으로 구성된 단순 임의표본을 추출하는 것이다.
\index{표준오차 (SE)!단일 평균}
}
\end{termBox}

방정식~(\ref{seOfXBar})에 한가지 미묘한 이슈가 있다: 모집단 표준편차는 일반적으로 알려져 있지 않다. 이미 이런 문제를 어떻게 해결할 수 있는지 추측했을 것이다: 표본으로부터 표준편차 점추정값을 사용할 수 있다. 표본크기가 적어도 30이고 모집단 분포가 강하게 기울어져 있지 않을 때 이 추정값은 충분히 좋은 경향이 있다. 따라서, 모집단 표준편차 $\sigma$ 대신에 표본 표준편차 $s$만 종종 사용한다.
%\footnote{Some books suggest 30 is sufficient; we take a slightly more conservative approach.}
% x <- c(); for(i in 1:10000){ x[i] <- mean(exp(rnorm(30, sd=0.5))) }; hist(x); (quantile(x, c(0.025, 0.975)) - mean(x)) / sd(x)
% M <- mean(x); x <- rep(NA, 1000); for(i in 1:1000){ temp <- exp(rnorm(30, sd=0.5)); x[i] <- (max(temp) - M) / sd(temp) }; hist(x); quantile(x, 0.95)
% for(i in 1:1000){ temp <- exp(rnorm(30, sd=0.5)); hist(temp, breaks=seq(0, 12, 0.5), main=round((max(temp) - mean(temp))/sd(temp), 1)); Sys.sleep(0.5) }
표본크기가 30보다 작을 때, 표준오차에 부가된 불확실성을 고려할 방법을 사용할 필요가 있다. 만약 기울어짐 조건이 충족되지 않는다면, 부가된 
기울어짐을 보상하는데 더 많은 표본이 필요하다. 이런 주제는 \ref{cltSection}~절에서 좀더 논의될 것이다.

\begin{exercise}
고등학생 100 명 표본에서 고등학생 신장 표준편차는 $s_{신장} = 0.088$ 미터다. 이 경우에, 데이터가 모집단에서 10\% 미만으로 구성된 단순 임의추출 표본임을 검사해서 관측점이 독립임을 확인할 수 있다. (a) 표본평균 $\bar{x}_{신장} = 1.70$ 미터에 대한 표준오차는 얼마인가? (b) 누군가 모든 YRBSS 응답자 평균신장이 사실 1.69 미터라고 말하는 것을 듣는다면 놀라울 것이 있을까?
\footnote{
(a) 표본 표준편차를 갖는 방정식~(\ref{seOfXBar})을 사용해서 표준오차를 계산한다: $SE_{\bar{y}} = 0.088 / \sqrt{100} = 0.0088$ 미터. (b) 전혀 놀랍지 않다. 표본이 1.69 미터에서 약 1 표준오차 떨어져 있다. 다른 말로, 표본이 상대적으로 가까이 있다면 1.69 미터가 믿기 어려워 보이지는 않는다. (표준오차를 사용해서 얼마나 가까이 있는지 식별한다.)
}
\end{exercise}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); d <- yrbss.samp$height; mean(d); sd(d); mean(yrbss$height, na.rm=TRUE); sd(yrbss$height, na.rm=TRUE)

\textC{\pagebreak}

\begin{exercise}
(a) 관측점 100개 혹은 400개를 갖는 표본 중에서 더 신뢰가는 것은 어느 것인가? (b) 표본크기가 더 커질 때 추정값이 더 나아짐을 수학적으로 보이고자 한다. 만약 개별 관측점 표준편차가 10 이라면, 표본크기가 100일 때 표준오차 추정값은 얼마인가? 관측점이 400개일 때는 얼마인가? (c) (b)에 나온 정답이 수학적으로 (a)에 나온 직관을 정당화하는 방법을 설명하시오.
\footnote{
(a) 부가되는 관측점은 일반적으로 모집단에 대한 이해를 돕는다. 그래서 관측점 400개를 갖는 점추정값에 더 신뢰가 간다. (b) 표본크기가 100일 때 표준오차는 $SE_{100} = 10/\sqrt{100} = 1$로 주어진다. 400개에 대해서는 $SE_{400} = 10/\sqrt{400} = 0.5$로 주어진다. 더 큰 표본이 더 작은 표준오차를 갖는다. (c) 관측점 400개를 갖는 표본 표준오차가 관측점 100개를 갖는 표본 표준오차보다 더 낮다. 표준오차가 대표적인 오차를 기술한다. 더 큰 표본에 대해서 표준오차가 낮기 때문에, 수학적으로 더 큰 표본에서 나온 추정값이 더 좋은 경향을 보여준다 -- 하지만 더 큰 모든 표본이 특정 작은 표본보다 더 좋은 추정값을 제공한다는 보장은 하지 않는다.}
\end{exercise}

\subsection{점추정값에 대한 기본 성질}

이번 절에서 목적 세가지를 달성했다. 먼저, 표본에서 나온 점추정값을 사용해서 모수를 추정할 수 있음을 밝혀냈다. 또한 이러한 점추정값이 정확하지 않다는 것도 밝혀냈다: 점추정값은 표본마다 변한다. 마지막으로, 수학적으로 수식~\eqref{seOfXBar}에 나온 표준오차를 사용해서 표본평균 불확실성을 정량화했다. 설사 다른 추정값(예를 들어, 중위수, 표준편차, 혹은 다른 통계량)에 대해서도 표준오차를 정량화할 수 있지만, 후속 장이나 다른 교육과정에서 다룰 예정이다.

\index{점추정|)}


%__________________
\section[Confidence intervals]{신뢰구간 \sectionvideohref{youtube-FUaXoKdCre4&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{confidenceIntervals}

\index{신뢰구간|(}
점추정값은 모수에 대한 타당할 것 같은 단일 값을 제공한다.하지만, 점추정값이 완벽적은 드물다; 일반적으로 추정값에 오차가 있다. 모수에 대한 점추정값만 제공하는 대신에, 당연한 다음 단계는 모수에 대한 \emph{값의 범위}(range of values)를 제공하는 것이다.

% In this section and in Section~\ref{hypothesisTesting}, we will emphasize the special case where the point estimate is a sample mean and the parameter is the population mean. In Section~\ref{aFrameworkForInference}, we generalize these methods for a variety of point estimates and population parameters that we will encounter in Chapter~\ref{inferenceForNumericalData} and beyond.

\subsection{모수를 정확히 포착하기}

모수에 대한 타당성 있는 값의 범위를 \term{신뢰구간}(confidence interval)이라고 부른다.

점추정만 사용하면 탁한 호수에서 작살로 낚시하는 것과 같다. 신뢰구간을 사용하면 그물로 물고기를 잡는 것과 같다. 작살을 물고기를 본 곳을 향해 던지지만, 아마도 맞추지 못할 것이다. 반대로, 만약 해당 지역에 그물을 던지면, 물고기를 잡을 가능성이 높다.

만약 점추정값을 보고하면, 아마도 정확한 모수를 맞추지 못할 것이다. 반대로, 타당성 있는 값의 범위-- 신뢰 구간 --를 보고하면, 적중시켜서 모수를 정확히 잡아낼 것이다.

\begin{exercise}
만약 모수를 정확히 잡아내는데 매우 확신을 갖는다면, 더 넓은 신뢰구간 혹은 더 좁은 신뢰구간을 사용해야 할까요?
\footnote{물고기를 잡는데 좀더 확실히 하려면, 더 넓은 그물을 사용한다. 마찬가지로, 모수를 잡아내는데 좀더 확신을 가지려면 더 넓은 신뢰구간을 사용한다.}
\end{exercise}


\textC{\pagebreak}


\subsection{근사 95\% 신뢰구간}

점추정값은 모수에 대한 가장 타당성 있는 값이다. 그래서, 점추정값 주위에 신뢰구간을 설치하는 것이 일리가 있다. 점추정값과 연관된 불확실성 측도인 표준오차가 신뢰구간을 얼마나 크게 만들지에 대한 지침이 된다.

표준오차는 추정값과 연관된 표준편차를 나타낸다. 대략 추정치의 95\% 정도가 모수 2 표준오차 내에 위치한다. 만약 점추정값에서 2 표준오차만큼 뻗으면, 모수를 잡아내는데 대략 95\% \term{신뢰}(confident)할 수 있다:

\begin{eqnarray}
\text{점추정값}\ \pm\ 2\times SE
\label{95PercentConfidenceIntervalFormula}
\end{eqnarray}

하지만, ``95\% 신뢰''가 무슨 의미인가? 표본을 많이 뽑아서, 수식~(\ref{95PercentConfidenceIntervalFormula})을 사용해서 각 표본에 대한 신뢰구간을 생성한다고 가정하자. 그렇다면 이런 신뢰구간 중에 약 95\%가 실제 평균 $\mu$를 포함할 것이다. 표본 25개를 가지고 이 과정을 그림~\ref{95PercentConfidenceInterval}에 나타냈다. 여기서 YRBSS 고등학생의 주당 운동일수 모평균 $\mu=3.90$~일을 포함하는 신뢰구간이 24개 그리고 한 신뢰구간은 모평균을 포함하고 있지 않다.

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.78\textwidth]{ch_inference_foundations/figures/95PercentConfidenceInterval/95PercentConfidenceInterval}
   \caption{크기 $n=100$에 대한 표본 25개를 \data{yrbss} 데이터셋에서 뽑았다. 각 표본에 대해서 신뢰구간을 생성해서 고등학생의 주당 평균 운동일수를 잡아내려고 했다. 신뢰구간 25개 중에서 하나만 실제 평균 $\mu = 3.90$~일을 잡아내지 못했다.}
   \label{95PercentConfidenceInterval}
\end{figure}

\begin{exercise}
그림~\ref{95PercentConfidenceInterval}에서 구간 하나가 3.90 분을 포함하고 있지 않다. 평균이 3.90 분이 될 수 없다는 의미가 되는가?\footnote{일부 관측점은 평균에서 2 표준편차보다 떨어져 생겨날 수 있듯이, 일부 점추정값이 모수에서 2 표준편차보다 떨어져 나올 수 있다. 신뢰구간은 단지 모수에 대한 타당한 값의 범위만 제공할 뿐이다. 데이터에 근거해서 다른 값들이 타당하지 않은 것으로 말할 수도 있지만, 불가능하다는 것을 의미하지는 않는다.}
\end{exercise}

대략 관측점 95\%가 평균으로부터 2 표준편차 내에 존재한다는 법칙은 근사적으로만 참이다. 하지만, 정규분포에 대해서는 매우 잘 맞는다. 곧 알게되듯이, 표본크기가 충분히 클 때, 평균은 정규분포되는 경향이 있다.

\begin{example}{
\data{yrbss\_samp}로부터 주당 평균 운동 일수는 3.75~일이다. 표본 표준편차를 사용해서 추정된 표준오차는 $SE=\frac{2.6}{\sqrt{100}} = 0.26$ 일이다. (모집단 표준편차(SD)가 대부분 알려져있지 않아서, 표본 표준편차(SD)를 사용한다.) 모든 YRBSS 학생에 대한 주당 평균 운동일수에 대한 근사 95\% 신뢰구간을 구하시오.}
수식~(\ref{95PercentConfidenceIntervalFormula})을 적용한다:
\begin{eqnarray*}
3.75\ \pm\ 2 \times  0.26 \quad \rightarrow \quad (3.23, 4.27)
\end{eqnarray*}
상기 데이터에 기반해서, 모든 YRBSS 학생에 대한 주당 평균 운동일수가 3.23일보다 크지만, 4.27일보다 적은 것을 대략 95\% 신뢰한다. 추정한 구간은 점추정 $\bar{x}_{active}$으로부터 2 표준오차만큼 확장된다.
\end{example}
% library(openintro); library(xtable); d <- yrbss.samp; mean(d$physically_active_7d); sd(d$physically_active_7d); sd(yrbss$physically_active_7d, na.rm=TRUE)

\begin{exercise} \label{95CIExerciseForAgeOfYrbssSamp1}
평균 YRBSS 학생 신장이 $\bar{x}_{height} = 1.697$ 미터, 표준오차는 0.0088 미터(표본 표준편차 0.088미터를 사용해서 추정)임을 표본 데이터가 제시하고 있다. 모든 YRBSS 학생의 평균신장에 대한 근사 95\% 신뢰구간은 어떻게 되는가?
\footnote{수식~(\ref{95PercentConfidenceIntervalFormula})을 적용한다: $1.697 \ \pm \ 2\times 0.0088 \rightarrow (1.6794, 1.7146)$. 산출된 구간을 다음과 같이 해석한다: 모든 YRBSS 학생 평균신장은 1.6794 미터에서 1.7146 미터 사이 (5.51~-- 5.62~피트)임에 대해서 대략 95\% 신뢰를 갖는다.}
\end{exercise}
% library(openintro); d <- yrbss.samp; mean(d$height); sd(d$height)

\subsection{평균에 대한 표집 분포}
\ref{seOfTheMean}~절에서 $\bar{x}$, 표본크기 100을 갖는 주당 평균 운동일수에 대한 표집분포를 소개했다. 이전 그림~\ref{yrbssActive1000SampDist}에서 이 분포에 대해서 면밀히 살펴봤다. 이제 표본 100,000번 뽑아 각각에 대해 평균을 산출하고, 표집분포에 대해 정확한 묘사를 위해 히스토그램에 도식화한다. 이 히스토그램이 그림~\ref{yrbssActiveBigSampDist} 왼쪽 패널에 나와있다.

\begin{figure}[hht]
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations/figures/yrbssActiveBigSampDist/yrbssActiveBigSampDist}
   \caption{100,000개 다른 임의 표본에 대한 표본 평균 히스토그램이 좌측 패널에 나와 있다. 표본 평균에 대한 정규확률도는 우측 패널에 나와 있다.}
   \label{yrbssActiveBigSampDist}
\end{figure}

해당 분포가 친숙해보이는가? 바라건게 그렇다! 표본평균 분포는 정규분포와 매우 닮았다 (\ref{normalDist}~절 참조). 표본평균에 대한 정규확률도가 그림~\ref{yrbssActiveBigSampDist} 오른쪽 패널에 나와 있다. 모든 점들이 직선에 매우 가까이 찍혀있다. 표본평균 분포가 거의 정규분포라는 결론을 내릴 수 있다. 중심극한정리(Central Limit Theorem)가 이러한 결과를 설명한다.

\begin{termBox}{\tBoxTitle{중심극한정리, 알기 쉬운 기술}
표본이 적어도 독립적인 30개 관측점이고 데이터가 심하게 기울어지 않았다면, 표본평균에 대한 분포는 정규모형으로 잘 근사된다.\index{중심극한정리}}
\end{termBox}

지금은 중심극한정리 알기 쉬운 버젼을 적용하고 \ref{cltSection}~절에서 좀더 깊이 논의한다.

수식~(\ref{95PercentConfidenceIntervalFormula})에서 2 표준오차를 사용한 선택은 대략 95\% 정도 관측점이 평균에서 2 표준편차 내에 위치한다는 일반적인 지침에 근거한다. 정규모형 아래에서 2 대신에 1.96을 사용해서 좀더 정확성을 높일 수 있다.

\begin{eqnarray}
\text{점추정값}\ \pm\ 1.96\times SE
\label{95PercentCIWhenUsingNormalModel}
\end{eqnarray}
만약 $\bar{x}$같은 점추정값이 정규모형과 표준오차 $SE$와 연관되다면, 좀더 정확한 95\% 신뢰구간을 사용한다.

\subsection{신뢰수준 변경하기}
\label{changingTheConfidenceLevelSection}

\index{신뢰구간!신뢰수준|(}

신뢰수준을 95\%보다 다소 높은 신뢰구간을 고려한다고 가정하자; 아마도 신뢰수준을 99\%로 두고자 한다. 물고기를 잡는 비유를 다시 들자: 만약 물고기를 잡는데 좀더 확실히 하려면, 더 넓은 그물을 사용해야만 된다. 99\% 신뢰수준을 생성하려면, 95\% 구간도 넓여야만 된다. 다른 한편으로, 만약 90\% 처럼 더 낮은 신뢰구간을 갖으려면, 최초 95\% 신뢰구간을 좀더 얇게 만들어야 한다. 

95\% 신뢰구간 구조는 새로운 신뢰수준을 갖는 구간을 만드는 방법에 대한 안내지침이 된다. 다음에 거의 정규분포에서 나온 점추정에 대한 일반적인 95\% 신뢰구간이 나와 있다:
\begin{eqnarray}
\text{점추정값}\ \pm\ 1.96\times SE
\end{eqnarray}
신뢰구간에 세가지 구성요소가 있다: 점추정, ``1.96'', 표준오차. 추정값이 대략 95\% 정도 모수에 대해 1.96 표준편차 안에 위치하기 때문에 $1.96\times SE$ 선택은 데이터의 95\%를 잡아낸다는 것에 기반한다. 1.96 선택이 95\% 신뢰수준에 대응된다.

\begin{exercise} \label{leadInForMakingA99PercentCIExercise}
만약 $X$가 정규분포에서 나온 확률변수라면, $X$가 평균의 2.58 표준편차 안에 존재할 가능성은 얼마나 될까?
\footnote{Z-점수가 -2.58보다 크고 2.58보다 작을 것을 묻는 것과 동일하다. (도식화한 것은, 그림~\ref{choosingZForCI}을 참고한다.) 이 확률값을 알아내려면, 정규확률표에서 -2.58과 2.58을 찾아낸다. 그렇게 하면, 미관측된 확률변수 $X$가 $\mu$의 2.58 표준편차 안에 존재할 확률이 $0.9951-0.0049 \approx 0.99$이 나온다.}
\end{exercise}

99\% 신뢰구간을 생성하려면, 95\% 신뢰구간 공식에 나온 1.96을 $2.58$로 변경한다. 정규확률변수가 평균의 2.58 표준편차 안에 있을 확률이 99\%가 된다는 점이 Guided Practice~\ref{leadInForMakingA99PercentCIExercise}에서 부각되었다. 이런 접근법은 -- 신뢰수준을 계산하는데 정규모형에 Z-점수를 사용 -- $\bar{x}$가 평균 $\mu$와 표준편차 $SE_{\bar{x}}$ 모수를 갖는 정규분포와 연관될 때 적합하다. 따라서, 99\% 신뢰구간에 대한 공식은 다음과 같다:

\begin{eqnarray}
\bar{x}\ \pm\ 2.58\times SE_{\bar{x}}
\label{99PercCIForMean}
\end{eqnarray}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{ch_inference_foundations/figures/choosingZForCI/choosingZForCI}
\caption{-$z^{\star}$ 와 $z^{\star}$ 사이 면적은 $|z^{\star}|$가 커집에 따라 증가한다. 만약 신뢰수준이 99\%라면, $z^{\star}$를 선택해서 정규곡선 99\%는 -$z^{\star}$ 와 $z^{\star}$ 사이가 되는데, 아래 꼬리는 0.5\%와 위쪽 꼬리는 0.5\%에 대응된다: $z^{\star}=2.58$.}
\label{choosingZForCI}
\index{신뢰구간!신뢰수준|)}
\end{figure}

정규근사가 신뢰구간 정도에 결정적이다. 언제 정규모형이 안전하게 적용될 수 있는지에 대해 \ref{cltSection}~절에서 좀더 자세한 논의가 있다. 정규모형이 잘 적합되지 않을 때, 표집분포 특징을 좀더 잘 잡아낼 수 있는 대안 분포를 사용한다.

\begin{termBox}{\tBoxTitle{$\bar{x}$가 거의 정규분포를 따라고 $SE$ 정확성에 대한 조건\label{terBoxOfCondForXBarBeingNearlyNormalAndSEBeingAccurate}}
$\bar{x}$에 대한 표집분포가 거의 정규분포를 따르고, SE 추정값이 충분히 정확함을 보증할 수 있는 중요한 조건:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item 표본 관측점이 독립이다.
\item 표본크기가 크다: 경험치로 $n\geq30$.
\item 모집단 분포가 심하게 기울어져 있지 않다. 이 조건을 평가하기는 쉽지 않다. 그래서 본인이 알아서 최선의 판단을 한다.
\end{itemize}
추가적으로, 표본크기가 더 커짐에 따라, 표본 치우침에 대해 좀더 관대해진다.}
\end{termBox}

\begin{tipBox}{\tipBoxTitle[]{표본 관측점이 독립임을 검증하는 방법}
만약 관측점이 단순임의표본에서 나왔고, 모집단에 대해 10\% 보다 적게 구성된다면, 이런 관측점은 독립이다.
만약 실험 대상이 처리집단에 임의 할당되었다면 독립으로 간주된다.
만약 표본이 확률과정으로 보이는 곳에서 나왔다면, 예를 들어 특정 제조공정에 사용되는 렌치나 스패너의 수명, 독립성 검사가 더 어렵다. 이런 경우에는 본인이 알아서 최선의 판단을 한다.}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle[]{강한 치우침에 대한 검사는 명백한 이상점에 대한 검사다}
눈에 띄는 이상점이 존재할 때, 표본은 적어도 관측점이 100개 포함되어야 하고, 일부 경우에는 더 많이 있어야 된다.
통계학에서 첫번재 수업여서, 치우침 평가에 완벽한 판단이 서지 못할 것이다. 괜찮다. 만약 곤경에 처했다면, 통계학자에게 자문을 구하거나, 스튜던트 붓스트랩 (studentized bootstrap, bootstrap-t) 방법을 검토한다.}
\end{tipBox}

% WARNING !!!!
% EOCE 4.9 (as of 2nd Edition) references the results of this exercise
\begin{exercise} \label{find99CIForYrbssAgeExercise}
\data{yrbss\_samp}을 사용해서 모든 YRBSS 학생에 대한 주당 평균 운동일수에 대한 99\% 신뢰구간을 생성하시오. 점추정값은 $\bar{x}_{active} = 3.75$, 표준오차는 $SE_{\bar{x}} = 0.26$이다. 
\footnote{
관측점은 독립이다 (단순임의표본, 모집단 $<10\%$ 미만), 표본크기는 적어도 30 ($n = 100$), 분포가 분명한 치우침이 없다 (\pageref{yrbssSampHistograms}~페이지, 그림~\ref{yrbssSampHistograms}); 정규근사와 SE 추정값에는 타당성이 있다. 99\% 신뢰구간공식을 적용한다: $\bar{x}_{active}\ \pm\ 2.58 \times  SE_{\bar{x}} \rightarrow (3.08, 4.42)$. 모든 YRBSS 학생에 대한 주당 평균 운동일수가 3.08일과 4.42일 사이을 99\% 신뢰한다.}
\end{exercise}
%library(openintro); data(yrbss.samp); d <- yrbss.samp; mean(d$age); sd(d$age)/sqrt(100)

\begin{termBox}{\tBoxTitle{임의 신뢰수준에 대한 신뢰구간}
만약 점추정값이 표준오차 $SE$를 갖는 정규모형을 따른다면, 모수에 대한 신뢰구간은 다음과 같다.
\begin{eqnarray*}
\text{점추정값}\ \pm\ z^{\star} SE
\end{eqnarray*}
$z^{\star}$는 선정된 신뢰수준에 대응된다.}
\end{termBox}

신뢰수준에 기반해서 $z^{\star}$를 식별하는 방법에 대한 도식이 그림~\ref{choosingZForCI}에 나와있다. $z^{\star}$를 선택하면, 정규모형에서 -$z^{\star}$ 와 $z^{\star}$ 사이 면적이 신뢰수준에 대응된다.

\begin{termBox}{\tBoxTitle{오차범위 (Margin of error)}
\label{marginOfErrorTermBox}신뢰구간에서 $z^{\star}\times SE$을 \term{오차범위}(margin of error)라고 부른다.}
\end{termBox}

\textC{\newpage}

\begin{exercise} \label{find90CIForYrbssAgeExercise}
Guided Practice~\ref{find99CIForYrbssAgeExercise}에서 나온 데이터를 사용해서 모든 YRBSS 학생에 대한 주당 평균 운동횟수에 대한 90\% 신뢰구간을 생성하라.\footnote{
먼저 $z^{\star}$을 찾아 분포의 90\%가 표준정규모형($N(\mu=0, \sigma=1)$)에서 -$z^{\star}$ 와 $z^{\star}$ 사이에 있음을 확인한다. 아래쪽 꼬리 5\%를 찾아 정규확률표에서 -$z^{\star}$를 찾고, 위쪽 꼬리에서 또다른 5\%를 찾아 $z^{\star}=1.65$를 찾아낸다. 그리고 나면, 90\% 신뢰구간은 $\bar{x}_{active}\ \pm\ 1.65\times SE_{\bar{x}} \to (3.32, 4.18)$으로 계산된다. (이미 정규성과 표준오차에 대한 조건은 검증했다.) 즉, 주당 평균 운동일수는 3.32일과 4.18일 사이가 됨에 90\% 신뢰가 있다.}
\end{exercise}

\subsection{신뢰구간 해석}
\label{interpretingCIs}

\index{신뢰구간!해석|(}
신뢰구간을 기술하는데 사용되는 다소 어색한 언어를 목격했을 것이다. 올바른 해석은 다음과 같다:

\begin{quote}
모수가 ... 사이에 있음을 XX\% 신뢰한다.
\end{quote}

\emph{올바르지 못한} 표현은 특정 확률로 모수를 잡아내는 것으로 신뢰구간을 기술하는 것이다. 이러한 것이 흔한 오류다: 확률로 신뢰구간을 간주하는 것이 유용하지만, 신뢰수준은 단지 모수가 구간에 얼마의 타당성을 갖고 존재할지만 정량화한다.

신뢰구간에 대한 또다른 고려할 점이 신뢰구간이 \emph{단지 모수만 잡아내려고 한다}는 점이다. 신뢰구간은 각 개별 관측점, 관측점 비율, 혹은 점추정값을 잡아내는데는 어떠한 점도 말하고 있지 않는다. 신뢰구간은 단지 모수를 잡아내려고만 한다.

\index{신뢰구간!해석|)}
\index{신뢰구간|)}


%__________________
\section[Hypothesis testing]{가설검정 \sectionvideohref{youtube-NVbPE1_Cbx8&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{hypothesisTesting}

\index{가설검정|(}

현재 학생들이 역기들기 혹은 다른 근력 강화 훈련을 과거 학생들과 비교하여 더 혹은 덜 할까? 2013년 YRBSS 설문조사에 나온 표본 100명과 2011년 YRBSS 설문에서 나온 학생 데이터를 비교한다.

또한 수면행동도 고려한다. 최근 조사에 따르면 대학생은 평균 대략 7시간 수면을 취한다. \footnote{\oiRedirect{textbook-theloquitur_1161}{\emph{설문조사를 통해서 대학생이 수면이 부족하다고 한다}. theloquitur.com/?p=1161}} 하지만, 지방대학 연구원은 지방학생이 평균 7시간보다 더 많은 수면을 취하는지 관심이 있다. 이 연구주제는 \ref{pValue}~절에서 조사한다.

\subsection{가설검정 얼개}

2011년 조사된 YRBSS 학생은 주당 평균 3.09 일 역기를 들었다 (혹은 다른 근력 강화 훈련을 실시했다). \data{yrbss\_samp} 데이터셋이 2013년 조사된 YRBSS 학생이 2011년 조사된 YRBSS 학생보다 더 혹은 덜 역기를 들었는지 혹은 또 다른 가능성으로 변동이 없는지에 대한 강력한 증거가 되는지 판단하고자 한다. 세가지 선택옵션을 두가지 경쟁하는 \termsub{가설}{가설}(hypotheses)로 단순화한다:

\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] YRBSS 학생이 역기를 든 주당 평균 일수는 2011년과 2013년 동일하다.
\item[$H_A$:] YRBSS 학생이 역기를 든 주당 평균 일수는 2011년 보다 2013년에 대해 \emph{다르다}.
\end{itemize}

$H_0$\marginpar[\raggedright\vspace{6mm}

$H_0$\\\footnotesize 귀무가설\vspace{3mm}\\\normalsize $H_A$\\\footnotesize 대립가설]{\raggedright\vspace{6mm}

$H_0$\\\footnotesize 귀무가설\vspace{3mm}\\\normalsize $H_A$\\\footnotesize 대립\\ 가설} 귀무가설(null hypothesis) $H_A$는 대립가설(alternative hypothesis).

\begin{termBox}{\tBoxTitle{귀무가설과 대립가설}
{\small \term{귀무가설($H_0$)}은 흔히 회의적인 시각 혹은 검정해야 되는 주장을 나타낸다. \term{대립가설 ($H_A$)}은 고려중인 대립 주장을 나타내고, 흔히 가능한 모수 값의 범위로 나타낸다.}}
\end{termBox}

귀무가설은 흔히 회의적인 자세 혹은 차이 없음 관점을 나타낸다. 대립가설은 흔히 변화가 있다는 가능성처럼 새로운 관점을 나타낸다.

\begin{tipBox}{\tipBoxTitle{가설 검정 얼개(hypothesis testing framework)}
대립가설($H_A$)을 옹호하는 증거가 강력해서 $H_A$ 지지에 대해 ($H_0$)를 거절하지 않는다면, 회의론자는 귀무가설($H_0$)를 기각하지 않을 것이다.}
\end{tipBox}

가설검정얼개(hypothesis testing framework)는 매우 일반적인 도구로, 재고도 않고 흔히 사용된다. 만약 어떤 사람이 다소 믿을 수 없는 주장을 한다면, 처음에는 회의적인 시각을 갖는다. 하지만, 주장을 뒷받침하는 충분한 증거가 있다면, 회의적인 시각을 파기하고 대립가설을 지지하고 귀무가설을 기각한다. 가설검정 전형적인 특징을 미국 사법체제에서도 발견할 수 있다.

\begin{exercise} \label{hypTestCourtExample}
미국 법원은 피고에 대한 두가지 가능한 주장을 고려한다: 유죄이거나 무죄다. 가설검정얼개로 주장을 준비한다면, 귀무가설과 대립가설은 각각 어느 것이 될까요? 
\footnote{배심원은 증거가 너무나 설득력 있어서 (강해서) 피고가 유죄라는 것에 관해서 합리적 의혹(reasonable doubt)이 없는지를 고려한다; 그런 경우에, 배심원은 결백(귀무가설)을 기각하고, 피고가 유죄(대립가설)라고 판결한다.}
\end{exercise}

배심원은 피고가 유죄인지를 설득력있게 보이는지 알아내기 위해서 증거를 면밀히 조사한다. 설사 배심원이 합리적 의혹을 넘어 유죄를 확신하지 못할지라도, 이것이 바로 피고가 무죄로 믿는 것을 의미하지는 않는다. 가설검정의 경우도 마찬가지다: \emph{설사 귀무가설을 기각하는데 실패할지라도, 귀무가설을 사실로 받아들이는 것은 일반적으로 아니다.} 대립가설에 대한 강력한 증거를 발견하지 못하는 것이 귀무가설을 받아들인다는 것과 동일시 되지는 않는다.

YRSS 예제에서, 귀무가설은 2011년과 2013년에 역기를 드는 주당 평균 일수에 차이가 없음을 나타낸다. 대립가설은 뭔가 새로운 혹은 좀더 흥미로운 면을 나타낸다: 증가하든 하락하든 차이가 있다. 이러한 가설을 2013년에 대해서 역기들기 평균일수로 $\mu_{13}$ 수학적 표기법으로 나타낸다:

\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] $\mu_{13} = 3.09$
\item[$H_A$:] $\mu_{13} \neq 3.09$
\end{itemize}
숫자 3.09는 2011년 YRBSS 학생자료로부터 나온 역기들기에 대한 주당 평균 일수다. 수학적 표기법을 사용해서, 통계적 도구를 사용해서 가설을 좀더 쉽게 평가할 수 있다. 만약 귀무가설이 사실이라면, 모수값을 나타내기 때문에 3.09를 \term{영값}(null value)으로 부른다.

\subsection{신뢰구간을 사용한 가설 검정}
\label{utilizingOurCI}

\data{yrbss\_samp} 데이터셋을 사용해서 가설검정을 평가한다. 학생이 역기를 든 주당평균 횟수 2013년 점추정값($\bar{x}_{13} = 2.78$~일)을 비교해서 착수해 나간다. 이추정값은 2013년 YRBSS 조사에서 나온 학생이 2011년 YRBSS 조사 학생보다 역기를 덜 든다고 제시하고 있다. 하지만, 변화가 있었다는 강력한 증거를 제시하는지 평가하기 위해서, $\bar{x}_{13}$와 연관된 불확실성을 고려해봐야 한다.

\ref{variabilityInEstimates}~절에서 표본마다 변동성이 존재하고, 표본평균이 정확하게 모수와 일치할 것 같지 않다는 것도 학습했다; $\bar{x}_{13}$이 정확하게 $\mu_{13}$와 동일하다고 예상하면 않된다. $\bar{x}_{13} = 2.78$이 주어졌지만, 2013년 YRBSS 설문조사에서 나온 모든 학생 평균이 2011년 YRBSS 조사에서 나온 평균과 동일하다고 가정하는 것은 여전히 유효하다. $\bar{x}_{13}$와 3.09 사이 차이는 \emph{표집변동}(sampling variation) 때문일 수 있다. 즉, 임의 표본을 수집할 때 점추정과 연관된 변동성.

\ref{confidenceIntervals}~절에서 신뢰구간이 모평균에 대한 타당한 값의 범위를 찾는 방법으로 소개되었다.

\begin{example}{In the sample of 100 students from the 2013 YRBSS survey, the average number of days per week that students lifted weights was 2.78~days with a standard deviation of 2.56 days (coincidentally the same as days active). Compute a 95\% confidence interval for the average for all students from the 2013 YRBSS survey. You can assume the conditions for the normal model are met.}
The general formula for the confidence interval based on the normal distribution is
\begin{align*}
\bar{x} \pm z^{\star} SE_{\bar{x}}
\end{align*}
We are given $\bar{x}_{13} = 2.78$, we use $z^{\star} = 1.96$ for a 95\% confidence level, and we can compute the standard error using the standard deviation divided by the square root of the sample size:
\begin{align*}
SE_{\bar{x}} = \frac{s_{13}}{\sqrt{n}} = \frac{2.56}{\sqrt{100}} = 0.256
\end{align*}
Entering the sample mean, $z^{\star}$, and the standard error into the confidence interval formula results in (2.27, 3.29). We are 95\% confident that the average number of days per week that all students from the 2013 YRBSS lifted weights was between 2.27~and 3.29~days.
\end{example}

Because the average of all students from the 2013 YRBSS survey is 3.09, which falls within the range of plausible values from the confidence interval, we cannot say the null hypothesis is implausible. That is, we fail to reject the null hypothesis, $H_0$.

\begin{tipBox}{\tipBoxTitle{Double negatives can sometimes be used in statistics}
In many statistical explanations, we use double negatives. For instance, we might say that the null hypothesis is \emph{not implausible} or we \emph{failed to reject} the null hypothesis. Double negatives are used to communicate that while we are not rejecting a position, we are also not saying it is correct.}
\end{tipBox}

\begin{exercise} \label{htForHousingExpenseForCommunityCollege650}
Colleges frequently provide estimates of student expenses such as housing. A consultant hired by a community college claimed that the average student housing expense was \$650 per month. What are the null and alternative hypotheses to test whether this claim is accurate?\footnote{$H_0$: The average cost is \$650 per month, $\mu = \$650$.

\hspace{3.4mm}$H_A$: The average cost is different than \$650 per month, $\mu \neq \$650$.}
\end{exercise}

\begin{exercise} \label{normalDistCondForHousingExpenseForCommunityCollege650}
The community college decides to collect data to evaluate the \$650 per month claim. They take a random sample of 175 students at their school and obtain the data represented in Figure~\ref{communityCollegeClaimedHousingExpenseDistribution}. Can we apply the normal model to the sample mean?\footnote{Applying the normal model requires that certain conditions are met. Because the data are a simple random sample and the sample (presumably) represents no more than 10\% of all students at the college, the observations are independent. The sample size is also sufficiently large ($n = 175$) and the data exhibit strong skew. While the data are strongly skewed, the sample is sufficiently large that this is acceptable, and the normal model may be applied to the sample mean.}

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{ch_inference_foundations/figures/communityCollegeClaimedHousingExpenseDistribution/communityCollegeClaimedHousingExpenseDistribution}
\caption{Sample distribution of student housing expense. These data are strongly skewed\index{skew!example: strong}, which we can see by the long right tail with a few notable outliers.}
\label{communityCollegeClaimedHousingExpenseDistribution}
\end{figure}
\end{exercise}

\begin{tipBox}{\tipBoxTitle[]{Evaluating the skew condition is challenging}
Don't despair if checking the skew condition is difficult or confusing. You aren't alone -- nearly all students get frustrated when checking skew. Properly assessing skew takes practice, and you won't be a pro, even at the end of this book. \\[2mm]
But this doesn't mean you should give up. Checking skew and the other conditions is extremely important for a responsible data analysis. However, rest assured that evaluating skew isn't something you need to be a master of by the end of the book, though by that time you should be able to properly assess clear cut cases.}
\end{tipBox}

\textC{\newpage}

\begin{example}{The sample mean for student housing is \$616.91 and the sample standard deviation is \$128.65. Construct a 95\% confidence interval for the population mean and evaluate the hypotheses of Guided Practice~\ref{htForHousingExpenseForCommunityCollege650}.}
The standard error associated with the mean may be estimated using the sample standard deviation divided by the square root of the sample size. Recall that $n = 175$ students were sampled.
\begin{align*}
SE = \frac{s}{\sqrt{n}} = \frac{128.65}{\sqrt{175}} = 9.73
\end{align*}
You showed in Guided Practice~\ref{normalDistCondForHousingExpenseForCommunityCollege650} that the normal model may be applied to the sample mean. This ensures a 95\% confidence interval may be accurately constructed:
$$\bar{x}\ \pm\ z^{\star} SE \quad\to\quad 616.91\ \pm\ 1.96 \times 9.73 \quad \to \quad (597.84, 635.98) $$
Because the null value \$650 is not in the confidence interval, a true mean of \$650 is implausible and we reject the null hypothesis. The data provide statistically significant evidence that the actual average housing expense is less than \$650 per month.
\end{example}


\subsection{Decision errors}

\index{hypothesis testing!decision errors|(}

Hypothesis tests are not flawless, since we can make a wrong decision in statistical hypothesis tests based on the data. For example, in the court system innocent people are sometimes wrongly convicted and the guilty sometimes walk free. However, the difference is that in statistical hypothesis tests, we have the tools necessary to quantify how often we make such errors.

% Hypothesis tests are not flawless. Just think of the court system: innocent people are sometimes wrongly convicted and the guilty sometimes walk free. Similarly, we can make a wrong decision in statistical hypothesis tests. However, the difference is that we have the tools necessary to quantify how often we make such errors.

There are two competing hypotheses: the null and the alternative. In a hypothesis test, we make a statement about which one might be true, but we might choose incorrectly. There are four possible scenarios, which are summarized in Table~\ref{fourHTScenarios}.

\begin{table}[ht]
\centering
\begin{tabular}{l l c c}
& & \multicolumn{2}{c}{\textbf{Test conclusion}} \\
  \cline{3-4}
\vspace{-3.7mm} \\
& & do not reject $H_0$ &  reject $H_0$ in favor of $H_A$ \\
  \cline{2-4}
\vspace{-3.7mm} \\
& $H_0$ true & okay &  Type~1 Error \\
\raisebox{1.5ex}{\textbf{Truth}} & $H_A$ true & Type~2 Error & okay \\
  \cline{2-4}
\end{tabular}
\caption{Four different scenarios for hypothesis tests.}
\label{fourHTScenarios}
\end{table}

A \term{Type~1 Error} is rejecting the null hypothesis when $H_0$ is actually true. A \term{Type~2 Error} is failing to reject the null hypothesis when the alternative is actually true.

\begin{exercise} \label{whatAreTheErrorTypesInUSCourts}
In a US court, the defendant is either innocent ($H_0$) or  guilty ($H_A$). What does a Type~1 Error represent in this context? What does a Type~2 Error represent? Table~\ref{fourHTScenarios} may be useful.\footnote{If the court makes a Type~1 Error, this means the defendant is innocent ($H_0$ true) but wrongly convicted. A Type~2 Error means the court failed to reject $H_0$ (i.e. failed to convict the person) when she was in fact guilty ($H_A$ true).}
\end{exercise}

\begin{exercise} \label{howToReduceType1ErrorsInUSCourts}
How could we reduce the Type~1 Error rate in US courts? What influence would this have on the Type~2 Error rate?\footnote{To lower the Type~1 Error rate, we might raise our standard for conviction from ``beyond a reasonable doubt'' to ``beyond a conceivable doubt'' so fewer people would be wrongly convicted. However, this would also make it more difficult to convict the people who are actually guilty, so we would make more Type~2 Errors.}
\end{exercise}

\begin{exercise} \label{howToReduceType2ErrorsInUSCourts}
How could we reduce the Type~2 Error rate in US courts? What influence would this have on the Type~1 Error rate?\footnote{To lower the Type~2 Error rate, we want to convict more guilty people. We could lower the standards for conviction from ``beyond a reasonable doubt'' to ``beyond a little doubt''. Lowering the bar for guilt will also result in more wrongful convictions, raising the Type~1 Error rate.}
\end{exercise}

\index{hypothesis testing!decision errors|)}

Exercises~\ref{whatAreTheErrorTypesInUSCourts}-\ref{howToReduceType2ErrorsInUSCourts} provide an important lesson: if we reduce how often we make one type of error, we generally make more of the other type.

Hypothesis testing is built around rejecting or failing to reject the null hypothesis. That is, we do not reject $H_0$ unless we have strong evidence. But what precisely does \emph{strong evidence} mean? As a general rule of thumb, for those cases where the null hypothesis is actually true, we do not want to incorrectly reject $H_0$ more than 5\% of the time. This corresponds to a \term{significance level}\index{hypothesis testing!significance level} of 0.05. We often write the significance level using $\alpha$\marginpar[\raggedright\vspace{-4mm}

$\alpha$\\\footnotesize significance\\level of a\\hypothesis test]{\raggedright\vspace{-4mm}

$\alpha$\\\footnotesize significance\\level of a\\hypothesis test} (the Greek letter \emph{alpha}\index{Greek!alpha@alpha ($\alpha$)}): $\alpha = 0.05$. We discuss the appropriateness of different significance levels in Section~\ref{significanceLevel}.

If we use a 95\% confidence interval to evaluate a hypothesis test where the null hypothesis is true, we will make an error whenever the point estimate is at least 1.96 standard errors away from the population parameter. This happens about 5\% of the time (2.5\% in each tail). Similarly, using a 99\% confidence interval to evaluate a hypothesis is equivalent to a significance level of $\alpha = 0.01$.

A confidence interval is, in one sense, simplistic in the world of hypothesis tests. Consider the following two scenarios:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The null value (the parameter value under the null hypothesis) is in the 95\% confidence interval but just barely, so we would not reject $H_0$. However, we might like to somehow say, quantitatively, that it was a close decision.
\item The null value is very far outside of the interval, so we reject $H_0$. However, we want to communicate that, not only did we reject the null hypothesis, but it wasn't even close. Such a case is depicted in Figure~\ref{whyWeWantPValue}.
\end{itemize}
In Section~\ref{pValue}, we introduce a tool called the \emph{p-value} that will be helpful in these cases. The p-value method also extends to hypothesis tests where confidence intervals cannot be easily constructed or applied.

\begin{figure}[hht]
\centering
\includegraphics[width=0.75\textwidth]{ch_inference_foundations/figures/whyWeWantPValue/whyWeWantPValue}
\caption{It would be helpful to quantify the strength of the evidence against the null hypothesis. In this case, the evidence is extremely strong.}
\label{whyWeWantPValue}
\end{figure}


\subsection{Formal testing using p-values}
\label{pValue}

\index{hypothesis testing!p-value|(}

The p-value is a way of quantifying the strength of the evidence against the null hypothesis and in favor of the alternative. Formally the \emph{p-value} is a conditional probability.

\begin{termBox}{\tBoxTitle{p-value}
The \term{p-value}\index{hypothesis testing!p-value|textbf} is the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis is true. We typically use a summary statistic of the data, in this chapter the sample mean, to help compute the p-value and evaluate the hypotheses.}
\end{termBox}

\index{data!school sleep|(}


\begin{exercise} \label{skepticalPerspOfRuralSchoolSleepExercise}
A poll by the National Sleep Foundation found that college students average about 7 hours of sleep per night. Researchers at a rural school are interested in showing that students at their school sleep longer than seven hours on average, and they would like to demonstrate this using a sample of students. What would be an appropriate skeptical position for this research?\footnote{A skeptic would have no reason to believe that sleep patterns at this school are different than the sleep patterns at another school.}
\end{exercise}

We can set up the null hypothesis for this test as a skeptical perspective: the students at this school average 7 hours of sleep per night. The alternative hypothesis takes a new form reflecting the interests of the research: the students average more than 7 hours of sleep. We can write these hypotheses as
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] $\mu = 7$.
\item[$H_A$:] $\mu > 7$.
\end{itemize}
Using $\mu > 7$ as the alternative is an example of a \term{one-sided} hypothesis test. In this investigation, there is no apparent interest in learning whether the mean is less than 7~hours.\footnote{This is entirely based on the interests of the researchers. Had they been only interested in the opposite case -- showing that their students were actually averaging fewer than seven hours of sleep but not interested in showing more than 7 hours -- then our setup would have set the alternative as $\mu < 7$.} Earlier we encountered a \term{two-sided} hypothesis where we looked for any clear difference, greater than or less than the null value.

Always use a two-sided test unless it was made clear prior to data collection that the test should be one-sided. Switching a two-sided test to a one-sided test after observing the data is dangerous because it can inflate the Type~1 Error rate. 

\begin{tipBox}{\tipBoxTitle{One-sided and two-sided tests}
When you are interested in checking for an increase or a decrease, but not both, use a one-sided test. When you are interested in any difference from the null value~--~an increase or decrease~--~then the test should be two-sided.\vspace{0.5mm}}
\end{tipBox}

\begin{tipBox}{\tipBoxTitle{Always write the null hypothesis as an equality}
We will find it most useful if we always list the null hypothesis as an equality (e.g. $\mu = 7$) while the alternative always uses an inequality (e.g. $\mu\neq7$, $\mu>7$, or $\mu<7$).}
\end{tipBox}

The researchers at the rural school conducted a simple random sample of $n=110$ students on campus. They found that these students averaged 7.42 hours of sleep and the standard deviation of the amount of sleep for the students was 1.75 hours. A histogram of the sample is shown in Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}.

\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{ch_inference_foundations/figures/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours/histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\caption{Distribution of a night of sleep for 110 college students. These data are strongly skewed.\index{skew!example: strong}}
\label{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours}
\end{figure}

Before we can use a normal model for the sample mean or compute the standard error of the sample mean, we must verify conditions. (1)~Because this is a simple random sample from less than 10\% of the student body, the observations are independent. (2)~The sample size in the sleep study is sufficiently large since it is greater than 30. (3)~The data show strong skew in Figure~\ref{histOfSleepForCollegeThatWasCheckingForMoreThan7Hours} and the presence of a couple of outliers. This skew and the outliers are acceptable for a sample size of $n=110$. With these conditions verified, the normal model can be safely applied to $\bar{x}$ and we can reasonably calculate the standard error.

\begin{exercise} \label{findSEOfFirstSleepStudyCheckingGreaterThan7Hours}
In the sleep study, the sample standard deviation was 1.75 hours and the sample size is 110. Calculate the standard error of $\bar{x}$.\footnote{The standard error can be estimated from the sample standard deviation and the sample size: $SE_{\bar{x}} = \frac{s_x}{\sqrt{n}} = \frac{1.75}{\sqrt{110}} = 0.17$.}
\end{exercise}

The hypothesis test for the sleep study will be evaluated using a significance level of $\alpha = 0.05$. We want to consider the data under the scenario that the null hypothesis is true. In this case, the sample mean is from a distribution that is nearly normal and has mean 7 and standard deviation of about $SE_{\bar{x}} = 0.17$. Such a distribution is shown in Figure~\ref{pValueOneSidedSleepStudy}.

\begin{figure}[hht]
   \centering
   \includegraphics[width=0.83\textwidth]{ch_inference_foundations/figures/pValueOneSidedSleepStudy/pValueOneSidedSleepStudy}
   \caption{If the null hypothesis is true, then the sample mean $\bar{x}$ came from this nearly normal distribution. The right tail describes the probability of observing such a large sample mean if the null hypothesis is true.}
   \label{pValueOneSidedSleepStudy}
\end{figure}

The shaded tail in Figure~\ref{pValueOneSidedSleepStudy} represents the chance of observing such a large mean, conditional on the null hypothesis being true. That is, the shaded tail represents the \mbox{p-value}. We shade all means larger than our sample mean, $\bar{x} = 7.42$, because they are more favorable to the alternative hypothesis than the observed mean.

We compute the p-value by finding the tail area of this normal distribution, which we learned to do in Section~\ref{normalDist}. First compute the Z-score of the sample mean, $\bar{x} = 7.42$:
\begin{eqnarray*}
Z = \frac{\bar{x} - \text{null value}}{SE_{\bar{x}}} = \frac{7.42 - 7}{0.17} = 2.47
\end{eqnarray*}
Using the normal probability table, the lower unshaded area is found to be 0.993. Thus the shaded area is $1-0.993 = 0.007$. {\em If the null hypothesis is true, the probability of observing a sample mean at least as large as 7.42 hours for a sample of 110 students is only 0.007.}\index{p-value!interpretation example} That is, if the null hypothesis is true, we would not often see such a large mean.

We evaluate the hypotheses by comparing the p-value to the significance level. Because the p-value is less than the significance level (p-value $=0.007 < 0.05=\alpha$), we reject the null hypothesis. What we observed is so unusual with respect to the null hypothesis that it casts serious doubt on $H_0$ and provides strong evidence favoring $H_A$.

\begin{termBox}{\tBoxTitle{p-value as a tool in hypothesis testing}
The smaller the p-value, the stronger the data favor $H_A$ over $H_0$. A small p-value (usually $<0.05$) corresponds to sufficient evidence to reject $H_0$ in favor of $H_A$.}
\index{hypothesis testing!p-value|)}
\end{termBox}

\begin{tipBox}{\tipBoxTitle{It is useful to first draw a picture to find the p-value}
It is useful to draw a picture of the distribution of $\bar{x}$ as though $H_0$ was true (i.e.~$\mu$~equals the null value), and shade the region (or regions) of sample means that are at least as favorable to the alternative hypothesis. These shaded regions represent the p-value.}
\end{tipBox}

The ideas below review the process of evaluating hypothesis tests with p-values:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item The null hypothesis represents a skeptic's position or a position of no difference. We reject this position only if the evidence strongly favors $H_A$.
\item A small p-value means that if the null hypothesis is true, there is a low probability of seeing a point estimate at least as extreme as the one we saw. We interpret this as strong evidence in favor of the alternative.
\item We reject the null hypothesis if the p-value is smaller than the significance level, $\alpha$, which is usually 0.05. Otherwise, we fail to reject $H_0$.
\item We should always state the conclusion of the hypothesis test in plain language so non-statisticians can also understand the results.
\end{itemize}

The p-value is constructed in such a way that we can directly compare it to the significance level ($\alpha$) to determine whether or not to reject $H_0$. This method ensures that the Type~1 Error rate does not exceed the significance level standard. 

\begin{figure}[ht]
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations/figures/pValueOneSidedSleepStudyExplained/pValueOneSidedSleepStudyExplained}
   \caption{To identify the p-value, the distribution of the sample mean is considered as if the null hypothesis was true. Then the p-value is defined and computed as the probability of the observed $\bar{x}$ or an $\bar{x}$ even more favorable to $H_A$ under this distribution.}
   \label{pValueOneSidedSleepStudyExplained}
\end{figure}

\begin{exercise}
If the null hypothesis is true, how often should the p-value be less than 0.05?\footnote{About 5\% of the time. If the null hypothesis is true, then the data only has a 5\% chance of being in the 5\% of data most favorable to $H_A$.}
\index{data!school sleep|)}
\end{exercise}

\begin{exercise}
Suppose we had used a significance level of 0.01 in the sleep study. Would the evidence have been strong enough to reject the null hypothesis? (The p-value was 0.007.) What if the significance level was $\alpha = 0.001$? \footnote{We reject the null hypothesis whenever $p$-$value < \alpha$. Thus, we would still reject the null hypothesis if $\alpha = 0.01$ but not if the significance level had been $\alpha = 0.001$.}
\end{exercise}

\begin{exercise} \label{ebayAmazonOneSidedTestExercise}
\index{data!mario\_kart|(}
Ebay might be interested in showing that buyers on its site tend to pay less than they would for the corresponding new item on Amazon. We'll research this topic for one particular product: a video game called \emph{Mario Kart} for the Nintendo Wii. During early October 2009, Amazon sold this game for \$46.99. Set up an appropriate (one-sided!) hypothesis test to check the claim that Ebay buyers pay less during auctions at this same time.\footnote{The skeptic would say the average is the same on Ebay, and we are interested in showing the average price is lower.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[$H_0$:] The average auction price on Ebay is equal to (or more than) the price on Amazon. We write only the equality in the statistical notation: $\mu_{ebay} = 46.99$.
\item[$H_A$:] The average price on Ebay is less than the price on Amazon, $\mu_{ebay} < 46.99$.
\end{itemize}}
\end{exercise}


\begin{exercise} \label
{exerciseFor52EbayAuctionsToExamineMarioKartLessExpensiveThanAmazonConditions}
During early October 2009, 52 Ebay auctions were recorded for \emph{Mario Kart}.\footnote{These data were collected by OpenIntro staff.} The total prices for the auctions are presented using a histogram in Figure~\ref{ebayMarioKartAuctionPriceHistogramFor3ConditionsExercise}, and we may like to apply the normal model to the sample mean. Check the three conditions required for applying the normal model: (1)~independence, (2)~at~least 30 observations, and (3)~the data are not strongly skewed.\footnote{(1) The independence condition is unclear. \emph{We will make the assumption that the observations are independent, which we should report with any final results.} (2) The sample size is sufficiently large: $n =52 \geq 30$. (3) The data distribution is not strongly skewed; it is approximately symmetric.}
\end{exercise}

\begin{figure}
   \centering
   \includegraphics[width=0.77\textwidth]{ch_inference_foundations/figures/ebayMarioKartAuctionPriceHistogramFor3ConditionsExercise/ebayMarioKartAuctionPriceHistogramFor3ConditionsExercise}
   \caption{A histogram of the total auction prices for 52 Ebay auctions.}
   \label{ebayMarioKartAuctionPriceHistogramFor3ConditionsExercise}
\end{figure}

\begin{example}{The average sale price of the 52 Ebay auctions for \emph{Wii Mario Kart} was \$44.17 with a standard deviation of \$4.15. Does this provide sufficient evidence to reject the null hypothesis in Guided Practice~\ref{ebayAmazonOneSidedTestExercise}? Use a significance level of $\alpha = 0.01$.}
The hypotheses were set up and the conditions were checked in Exercises~\ref{ebayAmazonOneSidedTestExercise} and~\ref{exerciseFor52EbayAuctionsToExamineMarioKartLessExpensiveThanAmazonConditions}. The next step is to find the standard error of the sample mean and produce a sketch to help find the p-value.
\begin{eqnarray*}
SE_{\bar{x}} = s/\sqrt{n} = 4.15/\sqrt{52} = 0.5755
\end{eqnarray*}
\begin{center}
\includegraphics[height=35mm]{ch_inference_foundations/figures/pVForEbayAmazonComparison/pVForEbayAmazonComparison}
\end{center}
Because the alternative hypothesis says we are looking for a smaller mean, we shade the lower tail. We find this shaded area by using the Z-score and normal probability table: $Z = \frac{44.17 - 46.99}{0.5755} = -4.90$, which has area less than 0.0002. The area is so small we cannot really see it on the picture. This lower tail area corresponds to the p-value.

Because the p-value is so small -- specifically, smaller than $\alpha = 0.01$ -- this provides sufficiently strong evidence to reject the null hypothesis in favor of the alternative. The data provide statistically significant evidence that the average price on Ebay is lower than Amazon's asking price.
\index{data!mario\_kart|)}
\end{example}

\begin{termBox}{\tBoxTitle{What's so special about 0.05?}
It's common to use a threshold of 0.05 to determine whether a result is statistically significant, but why is the most common value 0.05? Maybe the standard significance level should be bigger, or maybe it should be smaller. If you're a little puzzled, that probably means you're reading with a critical eye -- good job! We've made a 5-minute task to help clarify \emph{why 0.05}:
\begin{center}
\oiRedirect{textbook-why05}{www.openintro.org/why05}
\end{center}
Sometimes it's also a good idea to deviate from the standard. We'll discuss when to choose a threshold different than 0.05 in Section~\ref{significanceLevel}.\vspace{0.5mm}}
\end{termBox}


\subsection{Two-sided hypothesis testing with p-values}
\label{twoSidedTestsWithPValues}

\index{data!school sleep|(}

We now consider how to compute a p-value for a two-sided test. In one-sided tests, we shade the single tail in the direction of the alternative hypothesis. For example, when the alternative had the form $\mu > 7$, then the p-value was represented by the upper tail (Figure~\ref{pValueOneSidedSleepStudyExplained}). When the alternative was $\mu < 46.99$, the p-value was the lower tail (Guided Practice~\ref{ebayAmazonOneSidedTestExercise}). In a two-sided test, \emph{we shade two tails} since evidence in either direction is favorable to $H_A$.

\begin{exercise} \label{2ndSchSleepHypSetupExercise}
Earlier we talked about a research group investigating whether the students at their school slept longer than 7 hours each night. Let's consider a second group of researchers who want to evaluate whether the students at their college differ from the norm of 7 hours. Write the null and alternative hypotheses for this investigation.\footnote{Because the researchers are interested in any difference, they should use a two-sided setup: $H_0: \mu = 7$, $H_A: \mu \neq 7$.}
\end{exercise}

\begin{example}{The second college randomly samples 122 students and finds a mean of $\bar{x} = 6.83$ hours and a standard deviation of $s=1.8$ hours. Does this provide strong evidence against $H_0$ in Guided Practice~\ref{2ndSchSleepHypSetupExercise}? Use a significance level of $\alpha=0.05$.}
First, we must verify assumptions. (1) A simple random sample of less than 10\% of the student body means the observations are independent. (2) The sample size is 122, which is greater than 30. (3) Based on the earlier distribution and what we already know about college student sleep habits, the sample size will be acceptable.

Next we can compute the standard error ($SE_{\bar{x}} = \frac{s}{\sqrt{n}} = 0.16$) of the estimate and create a picture to represent the p-value, shown in Figure~\ref{2ndSchSleepHTExample}. Both tails are shaded. An estimate of 7.17 or more provides at least as strong of evidence against the null hypothesis and in favor of the alternative as the observed estimate, $\bar{x} = 6.83$.

We can calculate the tail areas by first finding the lower tail corresponding to $\bar{x}$:
\begin{eqnarray*}
Z = \frac{6.83 - 7.00}{0.16} = -1.06 \quad\stackrel{table}{\rightarrow}\quad \text{left tail} = 0.1446
\end{eqnarray*}
Because the normal model is symmetric, the right tail will have the same area as the left tail. The p-value is found as the sum of the two shaded tails:
\begin{eqnarray*}
\text{p-value} = \text{left tail} + \text{right tail} = 2\times(\text{left tail}) = 0.2892
\end{eqnarray*}
This p-value is relatively large (larger than $\alpha=0.05$), so we should not reject $H_0$. That is, if $H_0$ is true, it would not be very unusual to see a sample mean this far from 7 hours simply due to sampling variation. Thus, we do not have sufficient evidence to conclude that the mean is different than 7 hours.

\index{data!school sleep|)}

\begin{figure}
   \centering
   \includegraphics[width=0.9\textwidth]{ch_inference_foundations/figures/2ndSchSleepHTExample/2ndSchSleepHTExample}
   \caption{$H_A$ is two-sided, so \emph{both} tails must be counted for the p-value.}
   \label{2ndSchSleepHTExample}
\end{figure}

\end{example}

\begin{example}{It is never okay to change two-sided tests to one-sided tests after observing the data. In this example we explore the consequences of ignoring this advice. Using $\alpha=0.05$, we show that freely switching from two-sided tests to one-sided tests will cause us to make twice as many Type~1 Errors as intended.} \label{swappingHypAfterDataDoublesType1ErrorRate}
Suppose the sample mean was larger than the null value, $\mu_0$ (e.g. $\mu_0$ would represent~7 if $H_0$:~$\mu = 7$). Then if we can flip to a one-sided test, we would use $H_A$: $\mu > \mu_0$. Now if we obtain any observation with a Z-score greater than 1.65, we would reject $H_0$. If the null hypothesis is true, we incorrectly reject the null hypothesis about 5\% of the time when the sample mean is above the null value, as shown in Figure~\ref{type1ErrorDoublingExampleFigure}.

Suppose the sample mean was smaller than the null value. Then if we change to a one-sided test, we would use $H_A$: $\mu < \mu_0$. If $\bar{x}$ had a Z-score smaller than -1.65, we would reject $H_0$. If the null hypothesis is true, then we would observe such a case about 5\% of the time.

By examining these two scenarios, we can determine that we will make a Type~1 Error $5\%+5\%=10\%$ of the time if we are allowed to swap to the ``best'' one-sided test for the data. This is twice the error rate we prescribed with our significance level: $\alpha=0.05$ (!).

\begin{figure}
   \centering
   \includegraphics[width=0.7\textwidth]{ch_inference_foundations/figures/type1ErrorDoublingExampleFigure/type1ErrorDoublingExampleFigure}
   \caption{The shaded regions represent areas where we would reject $H_0$ under the bad practices considered in Example~\ref{swappingHypAfterDataDoublesType1ErrorRate} when $\alpha = 0.05$.}
   \label{type1ErrorDoublingExampleFigure}
\end{figure}

\end{example}

\begin{caution}{One-sided hypotheses are allowed only \emph{before} seeing data}
{After observing data, it is tempting to turn a two-sided test into a one-sided test. Avoid this temptation. Hypotheses must be set up \emph{before} observing the data. If~they are not, the test should be two-sided.}
\end{caution}


\subsection{Choosing a significance level}
\label{significanceLevel}

\index{hypothesis testing!significance level|(}
\index{significance level|(}

Choosing a significance level for a test is important in many contexts, and the traditional level is 0.05. However, it is often helpful to adjust the significance level based on the application. We may select a level that is smaller or larger than 0.05 depending on the consequences of any conclusions reached from the test.

If making a Type~1 Error is dangerous or especially costly, we should choose a small significance level (e.g. 0.01). Under this scenario we want to be very cautious about rejecting the null hypothesis, so we demand very strong evidence favoring $H_A$ before we would reject $H_0$.

If a Type~2 Error is relatively more dangerous or much more costly than a Type~1 Error, then we should choose a higher significance level (e.g. 0.10). Here we want to be cautious about failing to reject $H_0$ when the null is actually false.

\begin{tipBox}{\tipBoxTitle[]{Significance levels should reflect consequences of errors}
The significance level selected for a test should reflect the consequences associated with Type~1 and Type~2 Errors.}
\end{tipBox}

\begin{example}{A car manufacturer is considering a higher quality but more expensive supplier for window parts in its vehicles. They sample a number of parts from their current supplier and also parts from the new supplier. They decide that if the high quality parts will last more than 12\% longer, it makes financial sense to switch to this more expensive supplier. Is there good reason to modify the significance level in such a hypothesis test?}
The null hypothesis is that the more expensive parts last no more than 12\% longer while the alternative is that they do last more than 12\% longer. This decision is just one of the many regular factors that have a marginal impact on the car and company. A significance level of 0.05 seems reasonable since neither a Type~1 or Type~2 Error should be dangerous or (relatively) much more expensive.
\end{example}

\begin{example}{The same car manufacturer is considering a slightly more expensive supplier for parts related to safety, not windows. If the durability of these safety components is shown to be better than the current supplier, they will switch manufacturers. Is there good reason to modify the significance level in such an evaluation?}
The null hypothesis would be that the suppliers' parts are equally reliable. Because safety is involved, the car company should be eager to switch to the slightly more expensive manufacturer (reject $H_0$) even if the evidence of increased safety is only moderately strong. A slightly larger significance level, such as $\alpha=0.10$, might be appropriate.
\end{example}

\begin{exercise}
A part inside of a machine is very expensive to replace. However, the machine usually functions properly even if this part is broken, so the part is replaced only if we are extremely certain it is broken based on a series of measurements. Identify appropriate hypotheses for this test (in plain language) and suggest an appropriate significance level.\footnote{Here the null hypothesis is that the part is not broken, and the alternative is that it is broken. If we don't have sufficient evidence to reject $H_0$, we would not replace the part. It sounds like failing to fix the part if it is broken ($H_0$ false, $H_A$ true) is not very problematic, and replacing the part is expensive. Thus, we should require very strong evidence against $H_0$ before we replace the part. Choose a small significance level, such as $\alpha=0.01$.}
\end{exercise}\textC{\vspace{-3mm}}

\index{significance level|)}
\index{hypothesis testing!significance level|)}
\index{hypothesis testing|)}




%__________________
\section[Examining the Central Limit Theorem]{Examining the Central Limit Theorem \sectionvideohref{youtube-lsCc_pS3O28&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{cltSection}

\index{Central Limit Theorem|(}

The normal model for the sample mean tends to be very good when the sample consists of at least 30 independent observations and the population data are not strongly skewed. The Central Limit Theorem provides the theory that allows us to make this assumption.

\begin{termBox}{\tBoxTitle{Central Limit Theorem, informal definition}
The distribution of $\bar{x}$ is approximately normal. The approximation can be poor if the sample size is small, but it improves with larger sample sizes.}
\end{termBox}

The Central Limit Theorem states that when the sample size is small, the normal approximation may not be very good. However, as the sample size becomes large, the normal approximation improves. We will investigate three cases to see roughly when the approximation is reasonable.

We consider three data sets: one from a \emph{uniform} distribution, one from an \emph{exponential} distribution, and the other from a \emph{log-normal} distribution. These distributions are shown in the top panels of Figure~\ref{cltSimulations}. The uniform distribution is symmetric, the exponential distribution may be considered as having moderate skew since its right tail is relatively short (few outliers), and the log-normal distribution is strongly skewed and will tend to produce more apparent outliers.\index{skew!example: moderate}\index{skew!example: strong}

\begin{figure}
   \centering
   \includegraphics[width=\textwidth]{ch_inference_foundations/figures/cltSimulations/cltSimulations}
   \caption{Sampling distributions for the mean at different sample sizes and for three different distributions. The dashed red lines show normal distributions.}
   \label{cltSimulations}
\end{figure}

The left panel in the $n=2$ row represents the sampling distribution of $\bar{x}$ if it is the sample mean of two observations from the uniform distribution shown. The dashed line represents the closest approximation of the normal distribution. Similarly, the center and right panels of the $n=2$ row represent the respective distributions of $\bar{x}$ for data from exponential and log-normal distributions.

\begin{exercise}
Examine the distributions in each row of Figure~\ref{cltSimulations}. What do you notice about the normal approximation for each sampling distribution as the sample size becomes larger?\footnote{The normal approximation becomes better as larger samples are used.}
\end{exercise}

\begin{example}{Would the normal approximation be good in all applications where the sample size is at least 30?}
Not necessarily. For example, the normal approximation for the log-normal example is questionable for a sample size of 30. Generally, the more skewed a population distribution or the more common the frequency of outliers, the larger the sample required to guarantee the distribution of the sample mean is nearly normal.
\end{example}

\begin{tipBox}{\tipBoxTitle{With larger $n$, the sampling distribution of $\bar{x}$ becomes more normal}
As the sample size increases, the normal model for $\bar{x}$ becomes more reasonable. We can also relax our condition on skew when the sample size is very large.}
\end{tipBox}

We discussed in Section~\ref{seOfTheMean} that the sample standard deviation, $s$, could be used as a substitute of the population standard deviation, $\sigma$, when computing the standard error. This estimate tends to be reasonable when $n\geq30$. We will encounter alternative distributions for smaller sample sizes in Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData}.

\begin{example}{Figure~\ref{pokerProfitsCanApplyNormalToSampMean} shows a histogram of 50 observations. These represent winnings and losses from 50 consecutive days of a professional poker player. Can the normal approximation be applied to the sample mean, 90.69?}
We should consider each of the required conditions.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item[(1)] These are referred to as \term{time series data}, because the data arrived in a particular sequence. If the player wins on one day, it may influence how she plays the next. To make the assumption of independence we should perform careful checks on such data. While the supporting analysis is not shown, no evidence was found to indicate the observations are not independent.
\item[(2)] The sample size is 50, satisfying the sample size condition.
\item[(3)] There are two outliers, one very extreme, which suggests the data are very strongly skewed or very distant outliers may be common for this type of data. Outliers can play an important role and affect the distribution of the sample mean and the estimate of the standard error.
\end{itemize}
Since we should be skeptical of the independence of observations and the very extreme upper outlier poses a challenge, we should not use the normal model for the sample mean of these 50 observations. If we can obtain a much larger sample, perhaps several hundred observations, then the concerns about skew and outliers would no longer apply.
\end{example}

\begin{figure}[ht]
   \centering
   \includegraphics[height=58mm]{ch_inference_foundations/figures/pokerProfitsCanApplyNormalToSampMean/pokerProfitsCanApplyNormalToSampMean}
   \caption{Sample distribution of poker winnings. These data include some very clear outliers. These are problematic when considering the normality of the sample mean. For example, outliers are often an indicator of very strong skew\index{skew!example: very strong}.}
   \label{pokerProfitsCanApplyNormalToSampMean}
\end{figure}

\begin{caution}
{Examine data structure when considering independence}
{Some data sets are collected in such a way that they have a natural underlying structure between observations, e.g. when observations occur consecutively. Be especially cautious about independence assumptions regarding such data sets.}
\end{caution}

\begin{caution}{Watch out for strong skew and outliers}
{Strong skew is often identified by the presence of clear outliers. If a data set has prominent outliers, or such observations are somewhat common for the type of data under study, then it is useful to collect a sample with many more than 30 observations if the normal model will be used for $\bar{x}$.}
\index{Central Limit Theorem|)}
\end{caution}

You won't be a pro at assessing skew by the end of this book, so just use your best judgement and continue learning. As you develop your statistics skills and encounter tough situations, also consider learning about better ways to analyze skewed data, such as the studentized bootstrap (bootstrap-t), or consult a more experienced statistician.

\index{skew!strongly skewed guideline}


%__________________
\section[Inference for other estimators]{Inference for other estimators \sectionvideohref{youtube-PUMBNtVKr_g&list=PLkIselvEzpM7Pjo94m1e7J5jkIZkbQAl4}}
\label{aFrameworkForInference}

The sample mean is not the only point estimate for which the sampling distribution is nearly normal. For example, the sampling distribution of sample proportions closely resembles the normal distribution when the sample size is sufficiently large. In this section, we introduce a number of examples where the normal approximation is reasonable for the point estimate. Chapters~\ref{inferenceForNumericalData} and~\ref{inferenceForCategoricalData} will revisit each of the point estimates you see in this section along with some other new statistics.

We make another important assumption about each point estimate encountered in this section: the estimate is unbiased. A point estimate is \term{unbiased} if the sampling distribution of the estimate is centered at the parameter it estimates. That is, an unbiased estimate does not naturally over or underestimate the parameter. Rather, it tends to provide a ``good'' estimate. The sample mean is an example of an unbiased point estimate, as are each of the examples we introduce in this section.

Finally, we will discuss the general case where a point estimate may follow some distribution other than the normal distribution. We also provide guidance about how to handle scenarios where the statistical techniques you are familiar with are insufficient for the problem at hand.


\subsection{Confidence intervals for nearly normal point estimates}

\index{confidence interval!using normal model|(}

In Section~\ref{confidenceIntervals}, we used the point estimate $\bar{x}$ with a standard error $SE_{\bar{x}}$ to create a 95\% confidence interval for the population mean:
\begin{align}
\bar{x}\ \pm\ 1.96 \times SE_{\bar{x}}
\label{95PercCIForMeanInGeneralizingSection}
\end{align}
We constructed this interval by noting that the sample mean is within 1.96 standard errors of the actual mean about 95\% of the time. This same logic generalizes to any unbiased point estimate that is nearly normal. We may also generalize the confidence level by using a place-holder $z^{\star}$.

\begin{termBox}{\tBoxTitle{General confidence interval for the normal sampling distribution case}\label{generalConfidenceIntervalTermBox}%
A confidence interval based on an unbiased and nearly normal point estimate is
\begin{eqnarray}
\text{point estimate}\ \pm\ z^{\star}SE
\label{95PercGeneralCIInGeneralizingSection}
\end{eqnarray}
where $z^{\star}$ is selected to correspond to the confidence level, and $SE$ represents the standard error. The value $z^{\star}SE$ is called the \emph{margin of error}\index{margin of error}.}
\end{termBox}

Generally the standard error for a point estimate is estimated from the data and computed using a formula. For example, the standard error for the sample mean is
\begin{eqnarray*}
SE_{\bar{x}} = \frac{s}{\sqrt{n}}
\end{eqnarray*}
In this section, we provide the computed standard error for each example and exercise without detailing where the values came from. In future chapters, you will learn to fill in these and other details for each situation.

\begin{example}{In Guided Practice~\vref{peOfDiffActiveBetweenGender}, we computed a point estimate for the difference in the average days active per week between male and female students: $\bar{x}_{female}-\bar{x}_{male}=1.1$~days. This point estimate is associated with a nearly normal distribution with standard error $SE = 0.5$~days. What is a reasonable 95\% confidence interval for the difference in average days active per week?}
\label{ciForDiffOfPhysActiveBetweenGenders}
The normal approximation is said to be valid, so we apply Equation~\eqref{95PercGeneralCIInGeneralizingSection}:
\begin{eqnarray*}
\text{point estimate}\ \pm\ z^{\star} SE
	\quad\rightarrow\quad 1.1\ \pm\ 1.96\times 0.5
	\quad\rightarrow\quad (0.12, 2.08)
\end{eqnarray*}
We are 95\% confident that the male students, on average, were physically active 0.12 to 2.08 days more than female students in YRBSS each week. That is, the actual average difference is plausibly between 0.12 and 2.08 days per week with 95\% confidence.
\end{example}
%library(openintro); library(xtable); data(yrbss); data(yrbss.samp); (x <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, mean)); diff(x); s <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, sd); n <- by(yrbss.samp$physically_active_7d, yrbss.samp$gender, length); sqrt(sum(s^2/n))


\begin{example}{Does Example~\ref{ciForDiffOfPhysActiveBetweenGenders} guarantee that if a male and female student are selected at random from YRBSS, the male student would be active 0.12 to 2.08 days more than the female student?}
Our confidence interval says absolutely nothing about individual observations. It {only} makes a statement about a plausible range of values for the \emph{average} difference between all male and female students who participated in YRBSS.
\end{example}

\begin{exercise} \label{findZFor99PercConfLevelInFrameworkForInf}
What $z^{\star}$ would be appropriate for a 99\% confidence level? For help, see Figure~\vref{choosingZForCI}.\footnote{We seek $z^{\star}$ such that 99\% of the area under the normal curve will be between the Z-scores -$z^{\star}$ and $z^{\star}$. Because the remaining 1\% is found in the tails, each tail has area 0.5\%, and we can identify -$z^{\star}$ by looking up 0.0050 in the normal probability table: $z^{\star} = 2.58$. See also Figure~\vref{choosingZForCI}.}
\end{exercise}

\textC{\newpage}

\begin{exercise}
The proportion of students who are male in the \data{yrbss\_samp} sample is $\hat{p} = 0.48$. This sample meets certain conditions that ensure $\hat{p}$ will be nearly normal, and the standard error of the estimate is $SE_{\hat{p}} = 0.05$. Create a 90\% confidence interval for the proportion of students in the 2013 YRBSS survey who are male.\footnote{We use $z^{\star}=1.65$ (see Guided Practice~~\vref{find90CIForYrbssAgeExercise}), and apply the general confidence interval formula:
\begin{eqnarray*}
\hat{p}\ \pm\ z^{\star}SE_{\hat{p}}
	\quad\to\quad 0.48\ \pm\ 1.65\times 0.05
	\quad\to\quad (0.3975, 0.5625)
\end{eqnarray*}
Thus, we are 90\% confident that between 40\% and 56\% of the YRBSS students were male.}
\index{confidence interval!using normal model|)}
\end{exercise}


\subsection{Hypothesis testing for nearly normal point estimates}
\index{hypothesis testing!using normal model|(}

Just as the confidence interval method works with many other point estimates, we can generalize our hypothesis testing methods to new point estimates. Here we only consider the p-value approach, introduced in Section~\ref{pValue}, since it is the most commonly used technique and also extends to non-normal cases.

\begin{termBox}{\tBoxTitle[]{Hypothesis testing using the normal model}
\begin{enumerate}
\setlength{\itemsep}{0mm}
\item First write the hypotheses in plain language, then set them up in mathematical notation.
\item Identify an appropriate point estimate of the parameter of interest.
\item Verify conditions to ensure the standard error estimate is reasonable and the point estimate is nearly normal and unbiased.
\item Compute the standard error. Draw a picture depicting the distribution of the estimate under the idea that $H_0$ is true. Shade areas representing the p-value.
\item Using the picture and normal model, compute the \emph{test statistic} (Z-score) and identify the p-value to evaluate the hypotheses. Write a conclusion in plain language.
\end{enumerate}}
\end{termBox}

\begin{exercise} \label{fdaHypSetupForSulph}
A drug called sulphinpyrazone was under consideration for use in reducing the death rate in heart attack patients. To determine whether the drug was effective, a set of 1,475 patients were recruited into an experiment and randomly split into two groups: a control group that received a placebo and a treatment group that received the new drug. What would be an appropriate null hypothesis? And the alternative?\footnote{The skeptic's perspective is that the drug does not work at reducing deaths in heart attack patients ($H_0$), while the alternative is that the drug does work ($H_A$).}
\end{exercise}

\textC{\newpage}

We can formalize the hypotheses from Guided Practice~\ref{fdaHypSetupForSulph} by letting $p_{control}$ and $p_{treatment}$ represent the proportion of patients who died in the control and treatment groups, respectively. Then the hypotheses can be written as
\begin{eqnarray*}
&&H_0: p_{control} = p_{treatment} \quad\text{(the drug doesn't work)} \quad \\
&&H_A: p_{control} > p_{treatment} \quad\text{(the drug works)}
\end{eqnarray*}
or equivalently,
\begin{eqnarray*}
&&H_0: p_{control} - p_{treatment} = 0 \quad\text{(the drug doesn't work)} \quad \\
&&H_A: p_{control} - p_{treatment} > 0 \quad\text{(the drug works)}
\end{eqnarray*}
Strong evidence against the null hypothesis and in favor of the alternative would correspond to an observed difference in death rates,
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment}
\end{eqnarray*}
being larger than we would expect from chance alone. This difference in sample proportions represents a point estimate that is useful in evaluating the hypotheses. 

\begin{example}{We want to evaluate the hypothesis setup from Exericse~\ref{fdaHypSetupForSulph} using data from the actual study.\footnote{Anturane Reinfarction Trial Research Group. 1980. Sulfinpyrazone in the prevention of sudden death after myocardial infarction. New England Journal of Medicine 302(5):250-256.} In the control group, 60 of 742 patients died. In the treatment group, 41 of 733 patients died. The sample difference in death rates can be summarized as
\begin{eqnarray*}
\text{point estimate} = \hat{p}_{control} - \hat{p}_{treatment} = \frac{60}{742} - \frac{41}{733} = 0.025
\end{eqnarray*}
This point estimate is nearly normal and is an unbiased estimate of the actual difference in death rates. The standard error of this sample difference is $SE = 0.013$. Evaluate the hypothesis test at a 5\% significance level: $\alpha=0.05$.}
We would like to identify the p-value to evaluate the hypotheses. If the null hypothesis is true, then the point estimate would have come from a nearly normal distribution, like the one shown in Figure~\ref{sulphStudyFindPValueUsingNormalApprox}. The distribution is centered at zero since $p_{control}-p_{treatment}=0$ under the null hypothesis. Because a large positive difference provides evidence against the null hypothesis and in favor of the alternative, the upper tail has been shaded to represent the p-value. We need not shade the lower tail since this is a one-sided test: an observation in the lower tail does not support the alternative hypothesis.

\begin{figure}[bt]
   \centering
   \includegraphics[width=0.88\textwidth]{ch_inference_foundations/figures/sulphStudyFindPValueUsingNormalApprox/sulphStudyFindPValueUsingNormalApprox}
   \caption{The distribution of the sample difference if the null hypothesis is true.}
   \label{sulphStudyFindPValueUsingNormalApprox}
\end{figure}

The p-value can be computed by using the Z-score of the point estimate and the normal probability table.
\begin{eqnarray}
Z = \frac{\text{point estimate} - \text{null value}}{SE_{\text{point estimate}}}
	= \frac{0.025 - 0}{0.013} = 1.92
\label{zScoreOfPointEstimateForSulphinpyrazoneThisIsFirstTestStatReference}
\end{eqnarray}
Examining Z in the normal probability table, we find that the lower unshaded tail is about 0.973. Thus, the upper shaded tail representing the p-value is
\begin{eqnarray*}
\text{p-value} = 1-0.973 = 0.027
\end{eqnarray*}
Because the p-value is less than the significance level ($\alpha=0.05$), we say the null hypothesis is implausible. That is, we reject the null hypothesis in favor of the alternative and conclude that the drug is effective at reducing deaths in heart attack patie
nts.
\end{example}

The Z-score in Equation~(\ref{zScoreOfPointEstimateForSulphinpyrazoneThisIsFirstTestStatReference}) is called a \term{test statistic}. In most hypothesis tests, a test statistic is a particular data summary that is especially useful for computing the p-value and evaluating the hypothesis test. In the case of point estimates that are nearly normal, the test statistic is the Z-score.

\begin{termBox}{\tBoxTitle{Test statistic}
A \emph{test statistic} is a summary statistic that is particularly useful for evaluating a hypothesis test or identifying the p-value. When a point estimate is nearly normal, we use the Z-score of the point estimate as the test statistic. In later chapters we encounter situations where other test statistics are helpful.}
\index{hypothesis testing!using normal model|)}
\end{termBox}


\subsection{Non-normal point estimates}

We may apply the ideas of confidence intervals and hypothesis testing to cases where the point estimate or test statistic is not necessarily normal. There are many reasons why such a situation may arise:
\begin{itemize}
\setlength{\itemsep}{0mm}
\item the sample size is too small for the normal approximation to be valid;
\item the standard error estimate may be poor; or
\item the point estimate tends towards some distribution that is not the normal distribution.
\end{itemize}
For each case where the normal approximation is not valid, our first task is always to understand and characterize the sampling distribution of the point estimate or test statistic. Next, we can apply the general frameworks for confidence intervals and hypothesis testing to these alternative distributions.


\textC{\newpage}


\subsection{When to retreat}
\label{whenToRetreat}

Statistical tools rely on conditions. When the conditions are not met, these tools are unreliable and drawing conclusions from them is treacherous. The conditions for these tools typically come in two forms.
\begin{itemize}
\setlength{\itemsep}{0mm}
\item \textbf{The individual observations must be independent.} A random sample from less than 10\% of the population ensures the observations are independent. In experiments, we generally require that subjects are randomized into groups. If independence fails, then advanced techniques must be used, and in some such cases, inference may not be possible.
\item \textbf{Other conditions focus on sample size and skew.} For example, if the sample size is too small, the skew too strong, or extreme outliers are present, then the normal model for the sample mean will fail.
\end{itemize}
Verification of conditions for statistical tools is always necessary. Whenever conditions are not satisfied for a statistical technique, there are three options. The first is to learn new methods that are appropriate for the data. The second route is to consult a statistician.\footnote{If you work at a university, then there may be campus consulting services to assist you. Alternatively, there are many private consulting firms that are also available for hire.} The third route is to ignore the failure of conditions. This last option effectively invalidates any analysis and may discredit novel and interesting findings.

Finally, we caution that there may be no inference tools helpful when considering data that include unknown biases, such as convenience samples. For this reason, there are books, courses, and researchers devoted to the techniques of sampling and experimental design. See Sections~\ref{overviewOfDataCollectionPrinciples}-\ref{experimentsSection} for basic principles of data collection.


\subsection{Statistical significance versus practical significance}

When the sample size becomes larger, point estimates become more precise and any real differences in the mean and null value become easier to detect and recognize. Even a very small difference would likely be detected if we took a large enough sample. Sometimes researchers will take such large samples that even the slightest difference is detected. While we still say that difference is \term{statistically significant}, it might not be \term{practically significant}.

Statistically significant differences are sometimes so minor that they are not practically relevant. This is especially important to research: if we conduct a study, we want to focus on finding a meaningful result. We don't want to spend lots of money finding results that hold no practical value.

The role of a statistician in conducting a study often includes planning the size of the study. The statistician might first consult experts or scientific literature to learn what would be the smallest meaningful difference from the null value. She also would obtain some reasonable estimate for the standard deviation. With these important pieces of information, she would choose a sufficiently large sample size so that the power for the meaningful difference is perhaps 80\% or 90\%. While larger sample sizes may still be used, she might advise against using them in some cases, especially in sensitive areas of research.

